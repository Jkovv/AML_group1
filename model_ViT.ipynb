{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c5aecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (2.0.45)\n",
      "Requirement already satisfied: tqdm in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: optuna-integration in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: pytorch_lightning in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: optuna in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna-integration) (4.6.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (2.9.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.10.0)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (4.15.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (0.15.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.13.2)\n",
      "Requirement already satisfied: setuptools in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.6.0)\n",
      "Requirement already satisfied: filelock in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torchmetrics>0.7.0->pytorch_lightning) (2.3.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (2.0.45)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.22.0)\n",
      "Requirement already satisfied: Mako in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "!pip install optuna-integration 'pytorch_lightning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ac745e5-0ac5-4f90-ac57-0543c89e695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback  # optional, not required\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db8fa109-6a5f-45e9-9050-e498d77dd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "TUNE_EPOCHS = 5      # for Bayesian optimisation\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "TUNE_EPOCHS = 5\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58da542",
   "metadata": {},
   "source": [
    "Right now, your functions use global state (model, optimizer, LAMBDA_ATTR, train_loader, val_loader, etc.).\n",
    "We can exploit that and just re-assign the globals inside the Optuna objective to avoid refactoring everything.\n",
    "\n",
    "Idea: for each trial we:\n",
    "* create a new model with its own drop\n",
    "* create a new optimizer (lr, weight_decay)\n",
    "* set LAMBDA_ATTR\n",
    "* train for TUNE_EPOCHS\n",
    "* return best validation accuracy seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d09249b9-254e-44ef-b415-40729cf1048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/val dataset from: processed_bird_data\n",
      "Train size: 3337\n",
      "Val size: 589\n",
      "\n",
      "Loading test dataset from: processed_bird_test_data\n",
      "Test size: 4000\n",
      "\n",
      "Attributes shape: (200, 312)\n",
      "NUM_CLASSES: 200 | NUM_ATTR: 312\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VAL_PATH = \"processed_bird_data\"\n",
    "TEST_PATH = \"processed_bird_test_data\"\n",
    "\n",
    "print(\"Loading train/val dataset from:\", TRAIN_VAL_PATH)\n",
    "full_ds = load_from_disk(TRAIN_VAL_PATH)\n",
    "train_hf = full_ds[\"train\"]\n",
    "val_hf = full_ds[\"validation\"]\n",
    "\n",
    "print(\"Train size:\", len(train_hf))\n",
    "print(\"Val size:\", len(val_hf))\n",
    "\n",
    "print(\"\\nLoading test dataset from:\", TEST_PATH)\n",
    "test_hf = load_from_disk(TEST_PATH)\n",
    "print(\"Test size:\", len(test_hf))\n",
    "\n",
    "# attributes\n",
    "ATTR_PATH = \"data/attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f443e22-bcdc-4d53-b2a8-75260db502da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb6bb57d-7ff9-4497-ab63-386a1f0dd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, attributes, transform=None):\n",
    "        self.ds = hf_dataset\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "        label = int(item[\"label\"]) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        attr_vec = self.attributes[label]\n",
    "        attr_vec = torch.from_numpy(attr_vec)\n",
    "\n",
    "        return img, attr_vec, label\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.ds = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_id = int(item[\"id\"])\n",
    "        return img, img_id\n",
    "\n",
    "\n",
    "train_dataset = BirdTrainDataset(train_hf, attributes, transform=train_transform)\n",
    "val_dataset   = BirdTrainDataset(val_hf,   attributes, transform=eval_transform)\n",
    "test_dataset  = BirdTestDataset(test_hf,   transform=eval_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da9f4495-261b-4151-875a-9b8911a741cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleViTWithAttributes(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderBlock(\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (head_class): Linear(in_features=192, out_features=200, bias=True)\n",
       "  (head_attr): Linear(in_features=192, out_features=312, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size//patch_size\n",
    "        self.num_patches = self.grid_size**2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [B,3,H,W]\n",
    "        x = self.proj(x) #[B,embed_dim,H/P,W/P]\n",
    "        x = x.flatten(2) #[B,embed_dim,num_patches]\n",
    "        x = x.transpose(1, 2) #[B,num_patches,embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x [B,N,D]\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleViTWithAttributes(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 num_classes=200, num_attr=312, embed_dim=192, depth=6,\n",
    "                 num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, drop)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head_class = nn.Linear(embed_dim, num_classes)\n",
    "        # gÅ‚owa atrybutowa\n",
    "        self.head_attr  = nn.Linear(embed_dim, num_attr)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head_class.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head_attr.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:[B,3,224,224]\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x) #[B,N,D]\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1) #[B,1,D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1) #[B,1+N,D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0] #[B,D]\n",
    "\n",
    "        logits = self.head_class(cls)\n",
    "        attr_pred = self.head_attr(cls)\n",
    "        return logits, attr_pred\n",
    "    \n",
    "model = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ca56e-54cd-4681-acae-4a924b24c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6714d61-1bb1-485a-9f19-f3bd241ce9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_cls  += loss_cls.item() * imgs.size(0)\n",
    "        total_attr += loss_attr.item() * imgs.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += imgs.size(0)\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total_cls  += loss_cls.item() * imgs.size(0)\n",
    "            total_attr += loss_attr.item() * imgs.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += imgs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    return avg_loss, avg_cls, avg_attr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "420b944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def create_model_for_trial(drop: float):\n",
    "    # rebuild your ViT with this trial's dropout\n",
    "    model = SimpleViTWithAttributes(\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        num_attr=NUM_ATTR,\n",
    "        embed_dim=192,   # fixed for now\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        mlp_ratio=4.0,\n",
    "        drop=drop\n",
    "    ).to(DEVICE)\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    global model, optimizer, scheduler, LAMBDA_ATTR\n",
    "\n",
    "    # 1. Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    drop = trial.suggest_float(\"drop\", 0.0, 0.3)\n",
    "    lambda_attr = trial.suggest_float(\"lambda_attr\", 0.01, 0.2, log=True)\n",
    "\n",
    "    # 2. Reset random seeds for reproducibility\n",
    "    reset_seed()\n",
    "\n",
    "    # 3. Create model & optimizer for this trial\n",
    "    model = create_model_for_trial(drop)\n",
    "\n",
    "    LAMBDA_ATTR = lambda_attr\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=TUNE_EPOCHS\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # 4. Train for a small number of epochs\n",
    "    for epoch in range(1, TUNE_EPOCHS + 1):\n",
    "        train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "        val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Report to Optuna (for pruning)\n",
    "        trial.report(val_acc, step=epoch)\n",
    "\n",
    "        # Optional: prune bad trials early\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    # We want to maximize validation accuracy\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0c06616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 18:20:42,674] A new study created in memory with name: no-name-9b0d2cc0-70d0-4698-8d9d-f880548e8f7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4260\n",
      "[Epoch 1] Batch 20/105 loss=5.3527\n",
      "[Epoch 1] Batch 40/105 loss=5.2618\n",
      "[Epoch 1] Batch 60/105 loss=5.2006\n",
      "[Epoch 1] Batch 80/105 loss=5.3158\n",
      "[Epoch 1] Batch 100/105 loss=5.1076\n",
      "[Epoch 2] Batch 0/105 loss=5.1222\n",
      "[Epoch 2] Batch 20/105 loss=5.2400\n",
      "[Epoch 2] Batch 40/105 loss=5.1746\n",
      "[Epoch 2] Batch 60/105 loss=5.2009\n",
      "[Epoch 2] Batch 80/105 loss=5.1619\n",
      "[Epoch 2] Batch 100/105 loss=5.1624\n",
      "[Epoch 3] Batch 0/105 loss=5.1895\n",
      "[Epoch 3] Batch 20/105 loss=5.0314\n",
      "[Epoch 3] Batch 40/105 loss=5.0652\n",
      "[Epoch 3] Batch 60/105 loss=5.0949\n",
      "[Epoch 3] Batch 80/105 loss=5.2163\n",
      "[Epoch 3] Batch 100/105 loss=5.0958\n",
      "[Epoch 4] Batch 0/105 loss=5.0459\n",
      "[Epoch 4] Batch 20/105 loss=5.2918\n",
      "[Epoch 4] Batch 40/105 loss=5.0723\n",
      "[Epoch 4] Batch 60/105 loss=4.9639\n",
      "[Epoch 4] Batch 80/105 loss=5.0111\n",
      "[Epoch 4] Batch 100/105 loss=5.0958\n",
      "[Epoch 5] Batch 0/105 loss=4.9436\n",
      "[Epoch 5] Batch 20/105 loss=5.0018\n",
      "[Epoch 5] Batch 40/105 loss=5.0140\n",
      "[Epoch 5] Batch 60/105 loss=5.1614\n",
      "[Epoch 5] Batch 80/105 loss=5.1632\n",
      "[Epoch 5] Batch 100/105 loss=4.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 19:02:49,787] Trial 0 finished with value: 0.013582342954159592 and parameters: {'lr': 4.3284502212938785e-05, 'weight_decay': 0.006351221010640699, 'drop': 0.21959818254342153, 'lambda_attr': 0.060099747183803134}. Best is trial 0 with value: 0.013582342954159592.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4528\n",
      "[Epoch 1] Batch 20/105 loss=5.3027\n",
      "[Epoch 1] Batch 40/105 loss=5.2589\n",
      "[Epoch 1] Batch 60/105 loss=5.2371\n",
      "[Epoch 1] Batch 80/105 loss=5.3567\n",
      "[Epoch 1] Batch 100/105 loss=5.1825\n",
      "[Epoch 2] Batch 0/105 loss=5.1942\n",
      "[Epoch 2] Batch 20/105 loss=5.2541\n",
      "[Epoch 2] Batch 40/105 loss=5.2165\n",
      "[Epoch 2] Batch 60/105 loss=5.2089\n",
      "[Epoch 2] Batch 80/105 loss=5.1774\n",
      "[Epoch 2] Batch 100/105 loss=5.2399\n",
      "[Epoch 3] Batch 0/105 loss=5.1988\n",
      "[Epoch 3] Batch 20/105 loss=5.0773\n",
      "[Epoch 3] Batch 40/105 loss=5.1182\n",
      "[Epoch 3] Batch 60/105 loss=5.1042\n",
      "[Epoch 3] Batch 80/105 loss=5.2409\n",
      "[Epoch 3] Batch 100/105 loss=5.0990\n",
      "[Epoch 4] Batch 0/105 loss=5.0704\n",
      "[Epoch 4] Batch 20/105 loss=5.3023\n",
      "[Epoch 4] Batch 40/105 loss=5.0692\n",
      "[Epoch 4] Batch 60/105 loss=5.0104\n",
      "[Epoch 4] Batch 80/105 loss=5.0952\n",
      "[Epoch 4] Batch 100/105 loss=5.1612\n",
      "[Epoch 5] Batch 0/105 loss=4.9891\n",
      "[Epoch 5] Batch 20/105 loss=5.0519\n",
      "[Epoch 5] Batch 40/105 loss=5.0897\n",
      "[Epoch 5] Batch 60/105 loss=5.2013\n",
      "[Epoch 5] Batch 80/105 loss=5.2107\n",
      "[Epoch 5] Batch 100/105 loss=4.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 19:29:32,060] Trial 1 finished with value: 0.011884550084889643 and parameters: {'lr': 1.8410729205738674e-05, 'weight_decay': 4.207053950287936e-06, 'drop': 0.017425083650459836, 'lambda_attr': 0.13394334706750485}. Best is trial 0 with value: 0.013582342954159592.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4594\n",
      "[Epoch 1] Batch 20/105 loss=5.3370\n",
      "[Epoch 1] Batch 40/105 loss=5.2075\n",
      "[Epoch 1] Batch 60/105 loss=5.1840\n",
      "[Epoch 1] Batch 80/105 loss=5.3240\n",
      "[Epoch 1] Batch 100/105 loss=5.0135\n",
      "[Epoch 2] Batch 0/105 loss=5.0544\n",
      "[Epoch 2] Batch 20/105 loss=5.1223\n",
      "[Epoch 2] Batch 40/105 loss=5.0559\n",
      "[Epoch 2] Batch 60/105 loss=5.1437\n",
      "[Epoch 2] Batch 80/105 loss=5.0775\n",
      "[Epoch 2] Batch 100/105 loss=5.0911\n",
      "[Epoch 3] Batch 0/105 loss=5.1149\n",
      "[Epoch 3] Batch 20/105 loss=4.8674\n",
      "[Epoch 3] Batch 40/105 loss=4.9322\n",
      "[Epoch 3] Batch 60/105 loss=4.9254\n",
      "[Epoch 3] Batch 80/105 loss=5.1680\n",
      "[Epoch 3] Batch 100/105 loss=4.9827\n",
      "[Epoch 4] Batch 0/105 loss=4.8563\n",
      "[Epoch 4] Batch 20/105 loss=5.1926\n",
      "[Epoch 4] Batch 40/105 loss=4.9588\n",
      "[Epoch 4] Batch 60/105 loss=4.8563\n",
      "[Epoch 4] Batch 80/105 loss=4.7585\n",
      "[Epoch 4] Batch 100/105 loss=4.8862\n",
      "[Epoch 5] Batch 0/105 loss=4.7830\n",
      "[Epoch 5] Batch 20/105 loss=4.8867\n",
      "[Epoch 5] Batch 40/105 loss=4.7933\n",
      "[Epoch 5] Batch 60/105 loss=4.9405\n",
      "[Epoch 5] Batch 80/105 loss=5.0820\n",
      "[Epoch 5] Batch 100/105 loss=4.7423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 19:54:41,427] Trial 2 finished with value: 0.01867572156196944 and parameters: {'lr': 0.00010502105436744271, 'weight_decay': 0.0006796578090758161, 'drop': 0.006175348288740734, 'lambda_attr': 0.1827602783178572}. Best is trial 2 with value: 0.01867572156196944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4327\n",
      "[Epoch 1] Batch 20/105 loss=5.2992\n",
      "[Epoch 1] Batch 40/105 loss=5.1957\n",
      "[Epoch 1] Batch 60/105 loss=5.1298\n",
      "[Epoch 1] Batch 80/105 loss=5.2571\n",
      "[Epoch 1] Batch 100/105 loss=4.9224\n",
      "[Epoch 2] Batch 0/105 loss=4.9923\n",
      "[Epoch 2] Batch 20/105 loss=5.0779\n",
      "[Epoch 2] Batch 40/105 loss=4.9025\n",
      "[Epoch 2] Batch 60/105 loss=5.1107\n",
      "[Epoch 2] Batch 80/105 loss=5.0122\n",
      "[Epoch 2] Batch 100/105 loss=5.0659\n",
      "[Epoch 3] Batch 0/105 loss=4.9803\n",
      "[Epoch 3] Batch 20/105 loss=4.7116\n",
      "[Epoch 3] Batch 40/105 loss=4.8328\n",
      "[Epoch 3] Batch 60/105 loss=4.8801\n",
      "[Epoch 3] Batch 80/105 loss=5.0854\n",
      "[Epoch 3] Batch 100/105 loss=4.9809\n",
      "[Epoch 4] Batch 0/105 loss=4.7405\n",
      "[Epoch 4] Batch 20/105 loss=5.1703\n",
      "[Epoch 4] Batch 40/105 loss=4.8663\n",
      "[Epoch 4] Batch 60/105 loss=4.7475\n",
      "[Epoch 4] Batch 80/105 loss=4.5950\n",
      "[Epoch 4] Batch 100/105 loss=4.7174\n",
      "[Epoch 5] Batch 0/105 loss=4.6956\n",
      "[Epoch 5] Batch 20/105 loss=4.8556\n",
      "[Epoch 5] Batch 40/105 loss=4.6713\n",
      "[Epoch 5] Batch 60/105 loss=4.7500\n",
      "[Epoch 5] Batch 80/105 loss=4.9833\n",
      "[Epoch 5] Batch 100/105 loss=4.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 20:21:06,869] Trial 3 finished with value: 0.022071307300509338 and parameters: {'lr': 0.00025959425503112657, 'weight_decay': 7.068974950624607e-06, 'drop': 0.05454749016213018, 'lambda_attr': 0.017322667470546258}. Best is trial 3 with value: 0.022071307300509338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4259\n",
      "[Epoch 1] Batch 20/105 loss=5.3146\n",
      "[Epoch 1] Batch 40/105 loss=5.2339\n",
      "[Epoch 1] Batch 60/105 loss=5.2286\n",
      "[Epoch 1] Batch 80/105 loss=5.3364\n",
      "[Epoch 1] Batch 100/105 loss=5.1218\n",
      "[Epoch 2] Batch 0/105 loss=5.1438\n",
      "[Epoch 2] Batch 20/105 loss=5.2286\n",
      "[Epoch 2] Batch 40/105 loss=5.1965\n",
      "[Epoch 2] Batch 60/105 loss=5.1832\n",
      "[Epoch 2] Batch 80/105 loss=5.1625\n",
      "[Epoch 2] Batch 100/105 loss=5.1759\n",
      "[Epoch 3] Batch 0/105 loss=5.1950\n",
      "[Epoch 3] Batch 20/105 loss=5.0218\n",
      "[Epoch 3] Batch 40/105 loss=5.0694\n",
      "[Epoch 3] Batch 60/105 loss=5.0837\n",
      "[Epoch 3] Batch 80/105 loss=5.2361\n",
      "[Epoch 3] Batch 100/105 loss=5.0759\n",
      "[Epoch 4] Batch 0/105 loss=5.0502\n",
      "[Epoch 4] Batch 20/105 loss=5.2984\n",
      "[Epoch 4] Batch 40/105 loss=5.0698\n",
      "[Epoch 4] Batch 60/105 loss=4.9803\n",
      "[Epoch 4] Batch 80/105 loss=5.0357\n",
      "[Epoch 4] Batch 100/105 loss=5.1196\n",
      "[Epoch 5] Batch 0/105 loss=4.9590\n",
      "[Epoch 5] Batch 20/105 loss=5.0145\n",
      "[Epoch 5] Batch 40/105 loss=5.0245\n",
      "[Epoch 5] Batch 60/105 loss=5.1638\n",
      "[Epoch 5] Batch 80/105 loss=5.1633\n",
      "[Epoch 5] Batch 100/105 loss=4.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 20:46:45,064] Trial 4 finished with value: 0.015280135823429542 and parameters: {'lr': 3.2877474139911175e-05, 'weight_decay': 0.0001256104370001356, 'drop': 0.12958350559263473, 'lambda_attr': 0.023927528765580634}. Best is trial 3 with value: 0.022071307300509338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4372\n",
      "[Epoch 1] Batch 20/105 loss=5.3238\n",
      "[Epoch 1] Batch 40/105 loss=5.2118\n",
      "[Epoch 1] Batch 60/105 loss=5.1567\n",
      "[Epoch 1] Batch 80/105 loss=5.3034\n",
      "[Epoch 1] Batch 100/105 loss=5.0070\n",
      "[Epoch 2] Batch 0/105 loss=5.0464\n",
      "[Epoch 2] Batch 20/105 loss=5.1368\n",
      "[Epoch 2] Batch 40/105 loss=5.0743\n",
      "[Epoch 2] Batch 60/105 loss=5.1290\n",
      "[Epoch 2] Batch 80/105 loss=5.0894\n",
      "[Epoch 2] Batch 100/105 loss=5.0651\n",
      "[Epoch 3] Batch 0/105 loss=5.1162\n",
      "[Epoch 3] Batch 20/105 loss=4.8837\n",
      "[Epoch 3] Batch 40/105 loss=4.9450\n",
      "[Epoch 3] Batch 60/105 loss=4.9538\n",
      "[Epoch 3] Batch 80/105 loss=5.1505\n",
      "[Epoch 3] Batch 100/105 loss=5.0012\n",
      "[Epoch 4] Batch 0/105 loss=4.8735\n",
      "[Epoch 4] Batch 20/105 loss=5.2152\n",
      "[Epoch 4] Batch 40/105 loss=4.9798\n",
      "[Epoch 4] Batch 60/105 loss=4.8718\n",
      "[Epoch 4] Batch 80/105 loss=4.7645\n",
      "[Epoch 4] Batch 100/105 loss=4.8681\n",
      "[Epoch 5] Batch 0/105 loss=4.7906\n",
      "[Epoch 5] Batch 20/105 loss=4.8934\n",
      "[Epoch 5] Batch 40/105 loss=4.8142\n",
      "[Epoch 5] Batch 60/105 loss=4.9449\n",
      "[Epoch 5] Batch 80/105 loss=5.0647\n",
      "[Epoch 5] Batch 100/105 loss=4.7119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 21:20:38,295] Trial 5 finished with value: 0.01867572156196944 and parameters: {'lr': 0.00010952662748632558, 'weight_decay': 3.6138942712165278e-06, 'drop': 0.08764339456056544, 'lambda_attr': 0.029967309097101588}. Best is trial 3 with value: 0.022071307300509338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4367\n",
      "[Epoch 1] Batch 20/105 loss=5.3136\n",
      "[Epoch 1] Batch 40/105 loss=5.2447\n",
      "[Epoch 1] Batch 60/105 loss=5.1892\n",
      "[Epoch 1] Batch 80/105 loss=5.3288\n",
      "[Epoch 1] Batch 100/105 loss=5.0653\n",
      "[Epoch 2] Batch 0/105 loss=5.0910\n",
      "[Epoch 2] Batch 20/105 loss=5.1663\n",
      "[Epoch 2] Batch 40/105 loss=5.1218\n",
      "[Epoch 2] Batch 60/105 loss=5.1667\n",
      "[Epoch 2] Batch 80/105 loss=5.0981\n",
      "[Epoch 2] Batch 100/105 loss=5.1443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 21:37:45,626] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4364\n",
      "[Epoch 1] Batch 20/105 loss=5.3443\n",
      "[Epoch 1] Batch 40/105 loss=5.2219\n",
      "[Epoch 1] Batch 60/105 loss=5.1506\n",
      "[Epoch 1] Batch 80/105 loss=5.3181\n",
      "[Epoch 1] Batch 100/105 loss=5.0083\n",
      "[Epoch 2] Batch 0/105 loss=5.0518\n",
      "[Epoch 2] Batch 20/105 loss=5.1598\n",
      "[Epoch 2] Batch 40/105 loss=5.0937\n",
      "[Epoch 2] Batch 60/105 loss=5.1389\n",
      "[Epoch 2] Batch 80/105 loss=5.0732\n",
      "[Epoch 2] Batch 100/105 loss=5.0752\n",
      "[Epoch 3] Batch 0/105 loss=5.1591\n",
      "[Epoch 3] Batch 20/105 loss=4.9120\n",
      "[Epoch 3] Batch 40/105 loss=4.9500\n",
      "[Epoch 3] Batch 60/105 loss=4.9765\n",
      "[Epoch 3] Batch 80/105 loss=5.1779\n",
      "[Epoch 3] Batch 100/105 loss=5.0238\n",
      "[Epoch 4] Batch 0/105 loss=4.9036\n",
      "[Epoch 4] Batch 20/105 loss=5.2212\n",
      "[Epoch 4] Batch 40/105 loss=4.9930\n",
      "[Epoch 4] Batch 60/105 loss=4.8724\n",
      "[Epoch 4] Batch 80/105 loss=4.8049\n",
      "[Epoch 4] Batch 100/105 loss=4.9475\n",
      "[Epoch 5] Batch 0/105 loss=4.8150\n",
      "[Epoch 5] Batch 20/105 loss=4.9128\n",
      "[Epoch 5] Batch 40/105 loss=4.8472\n",
      "[Epoch 5] Batch 60/105 loss=5.0278\n",
      "[Epoch 5] Batch 80/105 loss=5.0884\n",
      "[Epoch 5] Batch 100/105 loss=4.7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 22:17:26,514] Trial 7 finished with value: 0.01867572156196944 and parameters: {'lr': 0.0001015066704592858, 'weight_decay': 1.5339162591163623e-06, 'drop': 0.1822634555704315, 'lambda_attr': 0.016666983286066417}. Best is trial 3 with value: 0.022071307300509338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4390\n",
      "[Epoch 1] Batch 20/105 loss=5.3386\n",
      "[Epoch 1] Batch 40/105 loss=5.3229\n",
      "[Epoch 1] Batch 60/105 loss=5.2921\n",
      "[Epoch 1] Batch 80/105 loss=5.3105\n",
      "[Epoch 1] Batch 100/105 loss=5.2810\n",
      "[Epoch 2] Batch 0/105 loss=5.2753\n",
      "[Epoch 2] Batch 20/105 loss=5.2989\n",
      "[Epoch 2] Batch 40/105 loss=5.2712\n",
      "[Epoch 2] Batch 60/105 loss=5.2650\n",
      "[Epoch 2] Batch 80/105 loss=5.2711\n",
      "[Epoch 2] Batch 100/105 loss=5.2845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 22:32:03,458] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4158\n",
      "[Epoch 1] Batch 20/105 loss=5.3589\n",
      "[Epoch 1] Batch 40/105 loss=5.2610\n",
      "[Epoch 1] Batch 60/105 loss=5.2153\n",
      "[Epoch 1] Batch 80/105 loss=5.3240\n",
      "[Epoch 1] Batch 100/105 loss=5.1332\n",
      "[Epoch 2] Batch 0/105 loss=5.1446\n",
      "[Epoch 2] Batch 20/105 loss=5.2611\n",
      "[Epoch 2] Batch 40/105 loss=5.1914\n",
      "[Epoch 2] Batch 60/105 loss=5.1965\n",
      "[Epoch 2] Batch 80/105 loss=5.1737\n",
      "[Epoch 2] Batch 100/105 loss=5.1942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 22:47:35,305] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4318\n",
      "[Epoch 1] Batch 20/105 loss=5.3263\n",
      "[Epoch 1] Batch 40/105 loss=5.2086\n",
      "[Epoch 1] Batch 60/105 loss=5.0601\n",
      "[Epoch 1] Batch 80/105 loss=5.2930\n",
      "[Epoch 1] Batch 100/105 loss=4.8588\n",
      "[Epoch 2] Batch 0/105 loss=4.9598\n",
      "[Epoch 2] Batch 20/105 loss=5.0336\n",
      "[Epoch 2] Batch 40/105 loss=4.7960\n",
      "[Epoch 2] Batch 60/105 loss=5.0809\n",
      "[Epoch 2] Batch 80/105 loss=4.9756\n",
      "[Epoch 2] Batch 100/105 loss=5.0964\n",
      "[Epoch 3] Batch 0/105 loss=4.9586\n",
      "[Epoch 3] Batch 20/105 loss=4.6231\n",
      "[Epoch 3] Batch 40/105 loss=4.7724\n",
      "[Epoch 3] Batch 60/105 loss=4.8924\n",
      "[Epoch 3] Batch 80/105 loss=5.0989\n",
      "[Epoch 3] Batch 100/105 loss=4.9115\n",
      "[Epoch 4] Batch 0/105 loss=4.6868\n",
      "[Epoch 4] Batch 20/105 loss=5.1476\n",
      "[Epoch 4] Batch 40/105 loss=4.8164\n",
      "[Epoch 4] Batch 60/105 loss=4.7146\n",
      "[Epoch 4] Batch 80/105 loss=4.5273\n",
      "[Epoch 4] Batch 100/105 loss=4.6886\n",
      "[Epoch 5] Batch 0/105 loss=4.6335\n",
      "[Epoch 5] Batch 20/105 loss=4.8666\n",
      "[Epoch 5] Batch 40/105 loss=4.6510\n",
      "[Epoch 5] Batch 60/105 loss=4.6420\n",
      "[Epoch 5] Batch 80/105 loss=4.9380\n",
      "[Epoch 5] Batch 100/105 loss=4.6393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 23:24:39,567] Trial 10 finished with value: 0.02037351443123939 and parameters: {'lr': 0.00040342112732688975, 'weight_decay': 2.85043206278715e-05, 'drop': 0.1106186318163292, 'lambda_attr': 0.010552829926879392}. Best is trial 3 with value: 0.022071307300509338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4301\n",
      "[Epoch 1] Batch 20/105 loss=5.3295\n",
      "[Epoch 1] Batch 40/105 loss=5.2124\n",
      "[Epoch 1] Batch 60/105 loss=5.0611\n",
      "[Epoch 1] Batch 80/105 loss=5.2928\n",
      "[Epoch 1] Batch 100/105 loss=4.8667\n",
      "[Epoch 2] Batch 0/105 loss=4.9661\n",
      "[Epoch 2] Batch 20/105 loss=5.0341\n",
      "[Epoch 2] Batch 40/105 loss=4.8159\n",
      "[Epoch 2] Batch 60/105 loss=5.0669\n",
      "[Epoch 2] Batch 80/105 loss=5.0208\n",
      "[Epoch 2] Batch 100/105 loss=5.0565\n",
      "[Epoch 3] Batch 0/105 loss=5.0246\n",
      "[Epoch 3] Batch 20/105 loss=4.6948\n",
      "[Epoch 3] Batch 40/105 loss=4.7720\n",
      "[Epoch 3] Batch 60/105 loss=4.9463\n",
      "[Epoch 3] Batch 80/105 loss=5.0897\n",
      "[Epoch 3] Batch 100/105 loss=4.9374\n",
      "[Epoch 4] Batch 0/105 loss=4.6616\n",
      "[Epoch 4] Batch 20/105 loss=5.1644\n",
      "[Epoch 4] Batch 40/105 loss=4.8731\n",
      "[Epoch 4] Batch 60/105 loss=4.7837\n",
      "[Epoch 4] Batch 80/105 loss=4.5331\n",
      "[Epoch 4] Batch 100/105 loss=4.7550\n",
      "[Epoch 5] Batch 0/105 loss=4.6825\n",
      "[Epoch 5] Batch 20/105 loss=4.8361\n",
      "[Epoch 5] Batch 40/105 loss=4.6429\n",
      "[Epoch 5] Batch 60/105 loss=4.6562\n",
      "[Epoch 5] Batch 80/105 loss=4.9582\n",
      "[Epoch 5] Batch 100/105 loss=4.6385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 00:02:47,839] Trial 11 finished with value: 0.023769100169779286 and parameters: {'lr': 0.0004273669312062059, 'weight_decay': 2.3739423955292538e-05, 'drop': 0.10912464381815776, 'lambda_attr': 0.010368427651183114}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4338\n",
      "[Epoch 1] Batch 20/105 loss=5.3244\n",
      "[Epoch 1] Batch 40/105 loss=5.2346\n",
      "[Epoch 1] Batch 60/105 loss=5.0826\n",
      "[Epoch 1] Batch 80/105 loss=5.2832\n",
      "[Epoch 1] Batch 100/105 loss=4.8608\n",
      "[Epoch 2] Batch 0/105 loss=4.9830\n",
      "[Epoch 2] Batch 20/105 loss=5.0437\n",
      "[Epoch 2] Batch 40/105 loss=4.8224\n",
      "[Epoch 2] Batch 60/105 loss=5.1209\n",
      "[Epoch 2] Batch 80/105 loss=5.0114\n",
      "[Epoch 2] Batch 100/105 loss=5.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 00:18:09,985] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4460\n",
      "[Epoch 1] Batch 20/105 loss=5.3148\n",
      "[Epoch 1] Batch 40/105 loss=5.1792\n",
      "[Epoch 1] Batch 60/105 loss=5.1083\n",
      "[Epoch 1] Batch 80/105 loss=5.2954\n",
      "[Epoch 1] Batch 100/105 loss=4.9376\n",
      "[Epoch 2] Batch 0/105 loss=4.9889\n",
      "[Epoch 2] Batch 20/105 loss=5.0862\n",
      "[Epoch 2] Batch 40/105 loss=4.9767\n",
      "[Epoch 2] Batch 60/105 loss=5.1022\n",
      "[Epoch 2] Batch 80/105 loss=4.9954\n",
      "[Epoch 2] Batch 100/105 loss=5.0377\n",
      "[Epoch 3] Batch 0/105 loss=5.0318\n",
      "[Epoch 3] Batch 20/105 loss=4.7897\n",
      "[Epoch 3] Batch 40/105 loss=4.8416\n",
      "[Epoch 3] Batch 60/105 loss=4.8580\n",
      "[Epoch 3] Batch 80/105 loss=5.1047\n",
      "[Epoch 3] Batch 100/105 loss=5.0053\n",
      "[Epoch 4] Batch 0/105 loss=4.7910\n",
      "[Epoch 4] Batch 20/105 loss=5.1592\n",
      "[Epoch 4] Batch 40/105 loss=4.8833\n",
      "[Epoch 4] Batch 60/105 loss=4.8082\n",
      "[Epoch 4] Batch 80/105 loss=4.6330\n",
      "[Epoch 4] Batch 100/105 loss=4.7527\n",
      "[Epoch 5] Batch 0/105 loss=4.7232\n",
      "[Epoch 5] Batch 20/105 loss=4.8327\n",
      "[Epoch 5] Batch 40/105 loss=4.7208\n",
      "[Epoch 5] Batch 60/105 loss=4.8357\n",
      "[Epoch 5] Batch 80/105 loss=5.0075\n",
      "[Epoch 5] Batch 100/105 loss=4.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 00:54:36,681] Trial 13 finished with value: 0.02037351443123939 and parameters: {'lr': 0.00022711179190406156, 'weight_decay': 2.11950469770016e-05, 'drop': 0.15888285342194056, 'lambda_attr': 0.017501117294230414}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4343\n",
      "[Epoch 1] Batch 20/105 loss=5.2967\n",
      "[Epoch 1] Batch 40/105 loss=5.2041\n",
      "[Epoch 1] Batch 60/105 loss=5.1125\n",
      "[Epoch 1] Batch 80/105 loss=5.2896\n",
      "[Epoch 1] Batch 100/105 loss=4.9234\n",
      "[Epoch 2] Batch 0/105 loss=4.9925\n",
      "[Epoch 2] Batch 20/105 loss=5.0985\n",
      "[Epoch 2] Batch 40/105 loss=4.9041\n",
      "[Epoch 2] Batch 60/105 loss=5.1129\n",
      "[Epoch 2] Batch 80/105 loss=5.0043\n",
      "[Epoch 2] Batch 100/105 loss=5.0408\n",
      "[Epoch 3] Batch 0/105 loss=4.9897\n",
      "[Epoch 3] Batch 20/105 loss=4.7166\n",
      "[Epoch 3] Batch 40/105 loss=4.8201\n",
      "[Epoch 3] Batch 60/105 loss=4.8861\n",
      "[Epoch 3] Batch 80/105 loss=5.1219\n",
      "[Epoch 3] Batch 100/105 loss=4.9784\n",
      "[Epoch 4] Batch 0/105 loss=4.7497\n",
      "[Epoch 4] Batch 20/105 loss=5.1778\n",
      "[Epoch 4] Batch 40/105 loss=4.8654\n",
      "[Epoch 4] Batch 60/105 loss=4.7864\n",
      "[Epoch 4] Batch 80/105 loss=4.5880\n",
      "[Epoch 4] Batch 100/105 loss=4.7568\n",
      "[Epoch 5] Batch 0/105 loss=4.6777\n",
      "[Epoch 5] Batch 20/105 loss=4.8417\n",
      "[Epoch 5] Batch 40/105 loss=4.7246\n",
      "[Epoch 5] Batch 60/105 loss=4.7243\n",
      "[Epoch 5] Batch 80/105 loss=4.9426\n",
      "[Epoch 5] Batch 100/105 loss=4.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 01:28:23,850] Trial 14 finished with value: 0.02037351443123939 and parameters: {'lr': 0.0002453634175888639, 'weight_decay': 0.00012799717396110157, 'drop': 0.05912714493419006, 'lambda_attr': 0.016393724345057703}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4323\n",
      "[Epoch 1] Batch 20/105 loss=5.2966\n",
      "[Epoch 1] Batch 40/105 loss=5.1802\n",
      "[Epoch 1] Batch 60/105 loss=5.1193\n",
      "[Epoch 1] Batch 80/105 loss=5.2922\n",
      "[Epoch 1] Batch 100/105 loss=4.9288\n",
      "[Epoch 2] Batch 0/105 loss=4.9944\n",
      "[Epoch 2] Batch 20/105 loss=5.0926\n",
      "[Epoch 2] Batch 40/105 loss=4.9647\n",
      "[Epoch 2] Batch 60/105 loss=5.1091\n",
      "[Epoch 2] Batch 80/105 loss=4.9966\n",
      "[Epoch 2] Batch 100/105 loss=5.0612\n",
      "[Epoch 3] Batch 0/105 loss=5.0281\n",
      "[Epoch 3] Batch 20/105 loss=4.7696\n",
      "[Epoch 3] Batch 40/105 loss=4.8439\n",
      "[Epoch 3] Batch 60/105 loss=4.8593\n",
      "[Epoch 3] Batch 80/105 loss=5.0858\n",
      "[Epoch 3] Batch 100/105 loss=4.9652\n",
      "[Epoch 4] Batch 0/105 loss=4.7892\n",
      "[Epoch 4] Batch 20/105 loss=5.2129\n",
      "[Epoch 4] Batch 40/105 loss=4.8618\n",
      "[Epoch 4] Batch 60/105 loss=4.7597\n",
      "[Epoch 4] Batch 80/105 loss=4.6000\n",
      "[Epoch 4] Batch 100/105 loss=4.7935\n",
      "[Epoch 5] Batch 0/105 loss=4.7176\n",
      "[Epoch 5] Batch 20/105 loss=4.8492\n",
      "[Epoch 5] Batch 40/105 loss=4.7197\n",
      "[Epoch 5] Batch 60/105 loss=4.7280\n",
      "[Epoch 5] Batch 80/105 loss=5.0309\n",
      "[Epoch 5] Batch 100/105 loss=4.6919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 02:07:53,690] Trial 15 finished with value: 0.01867572156196944 and parameters: {'lr': 0.00021768474487790407, 'weight_decay': 9.188227716652774e-06, 'drop': 0.10076176448993387, 'lambda_attr': 0.012952946089363222}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4430\n",
      "[Epoch 1] Batch 20/105 loss=5.3846\n",
      "[Epoch 1] Batch 40/105 loss=5.2248\n",
      "[Epoch 1] Batch 60/105 loss=5.0419\n",
      "[Epoch 1] Batch 80/105 loss=5.2933\n",
      "[Epoch 1] Batch 100/105 loss=4.8484\n",
      "[Epoch 2] Batch 0/105 loss=4.9504\n",
      "[Epoch 2] Batch 20/105 loss=5.0183\n",
      "[Epoch 2] Batch 40/105 loss=4.7340\n",
      "[Epoch 2] Batch 60/105 loss=5.1283\n",
      "[Epoch 2] Batch 80/105 loss=5.0015\n",
      "[Epoch 2] Batch 100/105 loss=5.0624\n",
      "[Epoch 3] Batch 0/105 loss=4.9560\n",
      "[Epoch 3] Batch 20/105 loss=4.6832\n",
      "[Epoch 3] Batch 40/105 loss=4.8000\n",
      "[Epoch 3] Batch 60/105 loss=4.9305\n",
      "[Epoch 3] Batch 80/105 loss=5.1266\n",
      "[Epoch 3] Batch 100/105 loss=4.8951\n",
      "[Epoch 4] Batch 0/105 loss=4.6537\n",
      "[Epoch 4] Batch 20/105 loss=5.1423\n",
      "[Epoch 4] Batch 40/105 loss=4.8147\n",
      "[Epoch 4] Batch 60/105 loss=4.7087\n",
      "[Epoch 4] Batch 80/105 loss=4.5099\n",
      "[Epoch 4] Batch 100/105 loss=4.6726\n",
      "[Epoch 5] Batch 0/105 loss=4.6519\n",
      "[Epoch 5] Batch 20/105 loss=4.8519\n",
      "[Epoch 5] Batch 40/105 loss=4.6161\n",
      "[Epoch 5] Batch 60/105 loss=4.6340\n",
      "[Epoch 5] Batch 80/105 loss=4.9198\n",
      "[Epoch 5] Batch 100/105 loss=4.5505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 02:46:47,199] Trial 16 finished with value: 0.02037351443123939 and parameters: {'lr': 0.00048183492480022197, 'weight_decay': 4.9146810539426464e-05, 'drop': 0.03742862534893712, 'lambda_attr': 0.026060376348876303}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4382\n",
      "[Epoch 1] Batch 20/105 loss=5.3208\n",
      "[Epoch 1] Batch 40/105 loss=5.1920\n",
      "[Epoch 1] Batch 60/105 loss=5.1450\n",
      "[Epoch 1] Batch 80/105 loss=5.3074\n",
      "[Epoch 1] Batch 100/105 loss=4.9787\n",
      "[Epoch 2] Batch 0/105 loss=5.0201\n",
      "[Epoch 2] Batch 20/105 loss=5.1095\n",
      "[Epoch 2] Batch 40/105 loss=5.0336\n",
      "[Epoch 2] Batch 60/105 loss=5.1102\n",
      "[Epoch 2] Batch 80/105 loss=5.0630\n",
      "[Epoch 2] Batch 100/105 loss=5.0691\n",
      "[Epoch 3] Batch 0/105 loss=5.0668\n",
      "[Epoch 3] Batch 20/105 loss=4.8229\n",
      "[Epoch 3] Batch 40/105 loss=4.9086\n",
      "[Epoch 3] Batch 60/105 loss=4.8980\n",
      "[Epoch 3] Batch 80/105 loss=5.1468\n",
      "[Epoch 3] Batch 100/105 loss=4.9954\n",
      "[Epoch 4] Batch 0/105 loss=4.8484\n",
      "[Epoch 4] Batch 20/105 loss=5.2001\n",
      "[Epoch 4] Batch 40/105 loss=4.9467\n",
      "[Epoch 4] Batch 60/105 loss=4.8495\n",
      "[Epoch 4] Batch 80/105 loss=4.7031\n",
      "[Epoch 4] Batch 100/105 loss=4.8059\n",
      "[Epoch 5] Batch 0/105 loss=4.7558\n",
      "[Epoch 5] Batch 20/105 loss=4.8805\n",
      "[Epoch 5] Batch 40/105 loss=4.7796\n",
      "[Epoch 5] Batch 60/105 loss=4.9080\n",
      "[Epoch 5] Batch 80/105 loss=5.0584\n",
      "[Epoch 5] Batch 100/105 loss=4.7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 03:22:08,229] Trial 17 finished with value: 0.01867572156196944 and parameters: {'lr': 0.00015409834605716456, 'weight_decay': 0.000336775626774932, 'drop': 0.14592206217745002, 'lambda_attr': 0.06663296217608104}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4405\n",
      "[Epoch 1] Batch 20/105 loss=5.3188\n",
      "[Epoch 1] Batch 40/105 loss=5.1800\n",
      "[Epoch 1] Batch 60/105 loss=5.1090\n",
      "[Epoch 1] Batch 80/105 loss=5.2826\n",
      "[Epoch 1] Batch 100/105 loss=4.9172\n",
      "[Epoch 2] Batch 0/105 loss=4.9676\n",
      "[Epoch 2] Batch 20/105 loss=5.0590\n",
      "[Epoch 2] Batch 40/105 loss=4.9346\n",
      "[Epoch 2] Batch 60/105 loss=5.0846\n",
      "[Epoch 2] Batch 80/105 loss=4.9687\n",
      "[Epoch 2] Batch 100/105 loss=4.9900\n",
      "[Epoch 3] Batch 0/105 loss=5.0093\n",
      "[Epoch 3] Batch 20/105 loss=4.7442\n",
      "[Epoch 3] Batch 40/105 loss=4.8527\n",
      "[Epoch 3] Batch 60/105 loss=4.8875\n",
      "[Epoch 3] Batch 80/105 loss=5.1489\n",
      "[Epoch 3] Batch 100/105 loss=4.9736\n",
      "[Epoch 4] Batch 0/105 loss=4.7774\n",
      "[Epoch 4] Batch 20/105 loss=5.1488\n",
      "[Epoch 4] Batch 40/105 loss=4.8524\n",
      "[Epoch 4] Batch 60/105 loss=4.7743\n",
      "[Epoch 4] Batch 80/105 loss=4.5636\n",
      "[Epoch 4] Batch 100/105 loss=4.6971\n",
      "[Epoch 5] Batch 0/105 loss=4.7123\n",
      "[Epoch 5] Batch 20/105 loss=4.8214\n",
      "[Epoch 5] Batch 40/105 loss=4.6813\n",
      "[Epoch 5] Batch 60/105 loss=4.7763\n",
      "[Epoch 5] Batch 80/105 loss=4.9972\n",
      "[Epoch 5] Batch 100/105 loss=4.6280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 04:00:46,760] Trial 18 finished with value: 0.022071307300509338 and parameters: {'lr': 0.00029791204863096585, 'weight_decay': 9.2516614469223e-06, 'drop': 0.2531940305133584, 'lambda_attr': 0.02020396268393445}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4432\n",
      "[Epoch 1] Batch 20/105 loss=5.3096\n",
      "[Epoch 1] Batch 40/105 loss=5.1970\n",
      "[Epoch 1] Batch 60/105 loss=5.1428\n",
      "[Epoch 1] Batch 80/105 loss=5.2937\n",
      "[Epoch 1] Batch 100/105 loss=4.9872\n",
      "[Epoch 2] Batch 0/105 loss=5.0290\n",
      "[Epoch 2] Batch 20/105 loss=5.1182\n",
      "[Epoch 2] Batch 40/105 loss=5.0339\n",
      "[Epoch 2] Batch 60/105 loss=5.1145\n",
      "[Epoch 2] Batch 80/105 loss=5.0630\n",
      "[Epoch 2] Batch 100/105 loss=5.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 04:17:13,322] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4254\n",
      "[Epoch 1] Batch 20/105 loss=5.2943\n",
      "[Epoch 1] Batch 40/105 loss=5.1883\n",
      "[Epoch 1] Batch 60/105 loss=5.0816\n",
      "[Epoch 1] Batch 80/105 loss=5.2998\n",
      "[Epoch 1] Batch 100/105 loss=4.8817\n",
      "[Epoch 2] Batch 0/105 loss=4.9821\n",
      "[Epoch 2] Batch 20/105 loss=5.0730\n",
      "[Epoch 2] Batch 40/105 loss=4.8858\n",
      "[Epoch 2] Batch 60/105 loss=5.1068\n",
      "[Epoch 2] Batch 80/105 loss=4.9910\n",
      "[Epoch 2] Batch 100/105 loss=5.0984\n",
      "[Epoch 3] Batch 0/105 loss=5.0215\n",
      "[Epoch 3] Batch 20/105 loss=4.6901\n",
      "[Epoch 3] Batch 40/105 loss=4.8266\n",
      "[Epoch 3] Batch 60/105 loss=4.8878\n",
      "[Epoch 3] Batch 80/105 loss=5.1098\n",
      "[Epoch 3] Batch 100/105 loss=4.9715\n",
      "[Epoch 4] Batch 0/105 loss=4.7235\n",
      "[Epoch 4] Batch 20/105 loss=5.1828\n",
      "[Epoch 4] Batch 40/105 loss=4.8140\n",
      "[Epoch 4] Batch 60/105 loss=4.7038\n",
      "[Epoch 4] Batch 80/105 loss=4.5772\n",
      "[Epoch 4] Batch 100/105 loss=4.7396\n",
      "[Epoch 5] Batch 0/105 loss=4.6841\n",
      "[Epoch 5] Batch 20/105 loss=4.8780\n",
      "[Epoch 5] Batch 40/105 loss=4.7003\n",
      "[Epoch 5] Batch 60/105 loss=4.6569\n",
      "[Epoch 5] Batch 80/105 loss=4.9510\n",
      "[Epoch 5] Batch 100/105 loss=4.6580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 04:57:40,227] Trial 20 finished with value: 0.02037351443123939 and parameters: {'lr': 0.000302777629761442, 'weight_decay': 8.274974937412168e-06, 'drop': 0.12456198490899, 'lambda_attr': 0.012191839002861509}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4383\n",
      "[Epoch 1] Batch 20/105 loss=5.2932\n",
      "[Epoch 1] Batch 40/105 loss=5.2024\n",
      "[Epoch 1] Batch 60/105 loss=5.0780\n",
      "[Epoch 1] Batch 80/105 loss=5.2679\n",
      "[Epoch 1] Batch 100/105 loss=4.8934\n",
      "[Epoch 2] Batch 0/105 loss=4.9608\n",
      "[Epoch 2] Batch 20/105 loss=4.9982\n",
      "[Epoch 2] Batch 40/105 loss=4.8999\n",
      "[Epoch 2] Batch 60/105 loss=5.0837\n",
      "[Epoch 2] Batch 80/105 loss=4.9368\n",
      "[Epoch 2] Batch 100/105 loss=5.0076\n",
      "[Epoch 3] Batch 0/105 loss=4.9914\n",
      "[Epoch 3] Batch 20/105 loss=4.7266\n",
      "[Epoch 3] Batch 40/105 loss=4.8305\n",
      "[Epoch 3] Batch 60/105 loss=4.8549\n",
      "[Epoch 3] Batch 80/105 loss=5.1406\n",
      "[Epoch 3] Batch 100/105 loss=4.9430\n",
      "[Epoch 4] Batch 0/105 loss=4.7407\n",
      "[Epoch 4] Batch 20/105 loss=5.1223\n",
      "[Epoch 4] Batch 40/105 loss=4.8575\n",
      "[Epoch 4] Batch 60/105 loss=4.7849\n",
      "[Epoch 4] Batch 80/105 loss=4.5368\n",
      "[Epoch 4] Batch 100/105 loss=4.7202\n",
      "[Epoch 5] Batch 0/105 loss=4.6655\n",
      "[Epoch 5] Batch 20/105 loss=4.8540\n",
      "[Epoch 5] Batch 40/105 loss=4.6655\n",
      "[Epoch 5] Batch 60/105 loss=4.7143\n",
      "[Epoch 5] Batch 80/105 loss=4.9810\n",
      "[Epoch 5] Batch 100/105 loss=4.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 05:31:13,692] Trial 21 finished with value: 0.02037351443123939 and parameters: {'lr': 0.00035546031788461787, 'weight_decay': 8.756805280546209e-06, 'drop': 0.28325069170998785, 'lambda_attr': 0.02364721757248912}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4250\n",
      "[Epoch 1] Batch 20/105 loss=5.3050\n",
      "[Epoch 1] Batch 40/105 loss=5.1933\n",
      "[Epoch 1] Batch 60/105 loss=5.1411\n",
      "[Epoch 1] Batch 80/105 loss=5.2800\n",
      "[Epoch 1] Batch 100/105 loss=4.9655\n",
      "[Epoch 2] Batch 0/105 loss=5.0061\n",
      "[Epoch 2] Batch 20/105 loss=5.1062\n",
      "[Epoch 2] Batch 40/105 loss=5.0190\n",
      "[Epoch 2] Batch 60/105 loss=5.1073\n",
      "[Epoch 2] Batch 80/105 loss=5.0247\n",
      "[Epoch 2] Batch 100/105 loss=5.0295\n",
      "[Epoch 3] Batch 0/105 loss=5.0580\n",
      "[Epoch 3] Batch 20/105 loss=4.8290\n",
      "[Epoch 3] Batch 40/105 loss=4.9135\n",
      "[Epoch 3] Batch 60/105 loss=4.9167\n",
      "[Epoch 3] Batch 80/105 loss=5.1531\n",
      "[Epoch 3] Batch 100/105 loss=5.0081\n",
      "[Epoch 4] Batch 0/105 loss=4.8434\n",
      "[Epoch 4] Batch 20/105 loss=5.1893\n",
      "[Epoch 4] Batch 40/105 loss=4.9142\n",
      "[Epoch 4] Batch 60/105 loss=4.8358\n",
      "[Epoch 4] Batch 80/105 loss=4.6633\n",
      "[Epoch 4] Batch 100/105 loss=4.8176\n",
      "[Epoch 5] Batch 0/105 loss=4.7503\n",
      "[Epoch 5] Batch 20/105 loss=4.8525\n",
      "[Epoch 5] Batch 40/105 loss=4.7862\n",
      "[Epoch 5] Batch 60/105 loss=4.8735\n",
      "[Epoch 5] Batch 80/105 loss=5.0495\n",
      "[Epoch 5] Batch 100/105 loss=4.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 06:05:43,180] Trial 22 finished with value: 0.01867572156196944 and parameters: {'lr': 0.00018675839994556782, 'weight_decay': 9.65561126905972e-06, 'drop': 0.21858131828053876, 'lambda_attr': 0.020131623578867264}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4457\n",
      "[Epoch 1] Batch 20/105 loss=5.3135\n",
      "[Epoch 1] Batch 40/105 loss=5.1845\n",
      "[Epoch 1] Batch 60/105 loss=5.1059\n",
      "[Epoch 1] Batch 80/105 loss=5.2724\n",
      "[Epoch 1] Batch 100/105 loss=4.9158\n",
      "[Epoch 2] Batch 0/105 loss=4.9597\n",
      "[Epoch 2] Batch 20/105 loss=5.0515\n",
      "[Epoch 2] Batch 40/105 loss=4.9289\n",
      "[Epoch 2] Batch 60/105 loss=5.0932\n",
      "[Epoch 2] Batch 80/105 loss=4.9823\n",
      "[Epoch 2] Batch 100/105 loss=4.9983\n",
      "[Epoch 3] Batch 0/105 loss=4.9885\n",
      "[Epoch 3] Batch 20/105 loss=4.7443\n",
      "[Epoch 3] Batch 40/105 loss=4.8461\n",
      "[Epoch 3] Batch 60/105 loss=4.8727\n",
      "[Epoch 3] Batch 80/105 loss=5.1413\n",
      "[Epoch 3] Batch 100/105 loss=4.9635\n",
      "[Epoch 4] Batch 0/105 loss=4.7503\n",
      "[Epoch 4] Batch 20/105 loss=5.1217\n",
      "[Epoch 4] Batch 40/105 loss=4.8487\n",
      "[Epoch 4] Batch 60/105 loss=4.7717\n",
      "[Epoch 4] Batch 80/105 loss=4.5665\n",
      "[Epoch 4] Batch 100/105 loss=4.7145\n",
      "[Epoch 5] Batch 0/105 loss=4.7232\n",
      "[Epoch 5] Batch 20/105 loss=4.8542\n",
      "[Epoch 5] Batch 40/105 loss=4.6776\n",
      "[Epoch 5] Batch 60/105 loss=4.7830\n",
      "[Epoch 5] Batch 80/105 loss=4.9994\n",
      "[Epoch 5] Batch 100/105 loss=4.6296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 06:43:18,154] Trial 23 finished with value: 0.022071307300509338 and parameters: {'lr': 0.00030777398303933327, 'weight_decay': 1.5558205341022404e-06, 'drop': 0.25716261354321895, 'lambda_attr': 0.03179408669133864}. Best is trial 11 with value: 0.023769100169779286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Batch 0/105 loss=5.4390\n",
      "[Epoch 1] Batch 20/105 loss=5.3236\n",
      "[Epoch 1] Batch 40/105 loss=5.1836\n",
      "[Epoch 1] Batch 60/105 loss=5.0769\n",
      "[Epoch 1] Batch 80/105 loss=5.2655\n",
      "[Epoch 1] Batch 100/105 loss=4.9200\n",
      "[Epoch 2] Batch 0/105 loss=4.9859\n",
      "[Epoch 2] Batch 20/105 loss=5.0699\n",
      "[Epoch 2] Batch 40/105 loss=4.9367\n",
      "[Epoch 2] Batch 60/105 loss=5.0837\n",
      "[Epoch 2] Batch 80/105 loss=4.9637\n",
      "[Epoch 2] Batch 100/105 loss=4.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 06:57:08,023] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value: 0.023769100169779286\n",
      "Best params: {'lr': 0.0004273669312062059, 'weight_decay': 2.3739423955292538e-05, 'drop': 0.10912464381815776, 'lambda_attr': 0.010368427651183114}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=2),\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print(\"Best value:\", study.best_trial.value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccb9ed",
   "metadata": {},
   "source": [
    "{'lr': 4.3284502212938785e-05, 'weight_decay': 0.006351221010640699, 'drop': 0.21959818254342153, 'lambda_attr': 0.060099747183803134}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5803f8f-ede0-479c-ac77-27c98afe70f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINAL TRAIN] Epoch 1/25\n",
      "[Epoch 1] Batch 0/105 loss=5.4301\n",
      "[Epoch 1] Batch 20/105 loss=5.3295\n",
      "[Epoch 1] Batch 40/105 loss=5.2124\n",
      "[Epoch 1] Batch 60/105 loss=5.0611\n",
      "[Epoch 1] Batch 80/105 loss=5.2928\n",
      "[Epoch 1] Batch 100/105 loss=4.8667\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 2/25\n",
      "[Epoch 2] Batch 0/105 loss=4.9661\n",
      "[Epoch 2] Batch 20/105 loss=5.0323\n",
      "[Epoch 2] Batch 40/105 loss=4.7495\n",
      "[Epoch 2] Batch 60/105 loss=5.0976\n",
      "[Epoch 2] Batch 80/105 loss=4.9452\n",
      "[Epoch 2] Batch 100/105 loss=5.0395\n",
      "\n",
      "[FINAL TRAIN] Epoch 3/25\n",
      "[Epoch 3] Batch 0/105 loss=4.9850\n",
      "[Epoch 3] Batch 20/105 loss=4.6469\n",
      "[Epoch 3] Batch 40/105 loss=4.7907\n",
      "[Epoch 3] Batch 60/105 loss=4.9544\n",
      "[Epoch 3] Batch 80/105 loss=5.1524\n",
      "[Epoch 3] Batch 100/105 loss=4.9507\n",
      "\n",
      "[FINAL TRAIN] Epoch 4/25\n",
      "[Epoch 4] Batch 0/105 loss=4.6722\n",
      "[Epoch 4] Batch 20/105 loss=5.3330\n",
      "[Epoch 4] Batch 40/105 loss=4.8650\n",
      "[Epoch 4] Batch 60/105 loss=4.7634\n",
      "[Epoch 4] Batch 80/105 loss=4.5779\n",
      "[Epoch 4] Batch 100/105 loss=4.8708\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 5/25\n",
      "[Epoch 5] Batch 0/105 loss=4.5535\n",
      "[Epoch 5] Batch 20/105 loss=4.8694\n",
      "[Epoch 5] Batch 40/105 loss=4.7009\n",
      "[Epoch 5] Batch 60/105 loss=4.6794\n",
      "[Epoch 5] Batch 80/105 loss=5.1446\n",
      "[Epoch 5] Batch 100/105 loss=4.5903\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 6/25\n",
      "[Epoch 6] Batch 0/105 loss=4.5441\n",
      "[Epoch 6] Batch 20/105 loss=4.7476\n",
      "[Epoch 6] Batch 40/105 loss=4.5472\n",
      "[Epoch 6] Batch 60/105 loss=5.2036\n",
      "[Epoch 6] Batch 80/105 loss=4.6167\n",
      "[Epoch 6] Batch 100/105 loss=4.6173\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 7/25\n",
      "[Epoch 7] Batch 0/105 loss=3.9937\n",
      "[Epoch 7] Batch 20/105 loss=4.5454\n",
      "[Epoch 7] Batch 40/105 loss=4.5746\n",
      "[Epoch 7] Batch 60/105 loss=4.7816\n",
      "[Epoch 7] Batch 80/105 loss=4.5648\n",
      "[Epoch 7] Batch 100/105 loss=4.2570\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 8/25\n",
      "[Epoch 8] Batch 0/105 loss=4.3373\n",
      "[Epoch 8] Batch 20/105 loss=4.3302\n",
      "[Epoch 8] Batch 40/105 loss=4.3857\n",
      "[Epoch 8] Batch 60/105 loss=4.7221\n",
      "[Epoch 8] Batch 80/105 loss=4.2749\n",
      "[Epoch 8] Batch 100/105 loss=4.3518\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 9/25\n",
      "[Epoch 9] Batch 0/105 loss=4.2873\n",
      "[Epoch 9] Batch 20/105 loss=4.5872\n",
      "[Epoch 9] Batch 40/105 loss=4.6474\n",
      "[Epoch 9] Batch 60/105 loss=4.1287\n",
      "[Epoch 9] Batch 80/105 loss=4.5226\n",
      "[Epoch 9] Batch 100/105 loss=4.2458\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 10/25\n",
      "[Epoch 10] Batch 0/105 loss=4.2440\n",
      "[Epoch 10] Batch 20/105 loss=4.2021\n",
      "[Epoch 10] Batch 40/105 loss=4.3615\n",
      "[Epoch 10] Batch 60/105 loss=3.9380\n",
      "[Epoch 10] Batch 80/105 loss=4.1779\n",
      "[Epoch 10] Batch 100/105 loss=4.1076\n",
      "\n",
      "[FINAL TRAIN] Epoch 11/25\n",
      "[Epoch 11] Batch 0/105 loss=3.9684\n",
      "[Epoch 11] Batch 20/105 loss=3.7871\n",
      "[Epoch 11] Batch 40/105 loss=3.9131\n",
      "[Epoch 11] Batch 60/105 loss=4.0408\n",
      "[Epoch 11] Batch 80/105 loss=4.5178\n",
      "[Epoch 11] Batch 100/105 loss=3.7031\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 12/25\n",
      "[Epoch 12] Batch 0/105 loss=4.2484\n",
      "[Epoch 12] Batch 20/105 loss=4.0845\n",
      "[Epoch 12] Batch 40/105 loss=4.1372\n",
      "[Epoch 12] Batch 60/105 loss=3.9341\n",
      "[Epoch 12] Batch 80/105 loss=3.9484\n",
      "[Epoch 12] Batch 100/105 loss=3.6889\n",
      "\n",
      "[FINAL TRAIN] Epoch 13/25\n",
      "[Epoch 13] Batch 0/105 loss=3.8090\n",
      "[Epoch 13] Batch 20/105 loss=4.0865\n",
      "[Epoch 13] Batch 40/105 loss=3.7134\n",
      "[Epoch 13] Batch 60/105 loss=4.0614\n",
      "[Epoch 13] Batch 80/105 loss=4.0154\n",
      "[Epoch 13] Batch 100/105 loss=3.7485\n",
      "\n",
      "[FINAL TRAIN] Epoch 14/25\n",
      "[Epoch 14] Batch 0/105 loss=3.8577\n",
      "[Epoch 14] Batch 20/105 loss=3.9041\n",
      "[Epoch 14] Batch 40/105 loss=3.6776\n",
      "[Epoch 14] Batch 60/105 loss=3.9940\n",
      "[Epoch 14] Batch 80/105 loss=3.6733\n",
      "[Epoch 14] Batch 100/105 loss=3.9793\n",
      "\n",
      "[FINAL TRAIN] Epoch 15/25\n",
      "[Epoch 15] Batch 0/105 loss=3.8557\n",
      "[Epoch 15] Batch 20/105 loss=4.0971\n",
      "[Epoch 15] Batch 40/105 loss=3.7939\n",
      "[Epoch 15] Batch 60/105 loss=3.7078\n",
      "[Epoch 15] Batch 80/105 loss=4.0137\n",
      "[Epoch 15] Batch 100/105 loss=3.9286\n",
      "\n",
      "[FINAL TRAIN] Epoch 16/25\n",
      "[Epoch 16] Batch 0/105 loss=3.7629\n",
      "[Epoch 16] Batch 20/105 loss=3.7160\n",
      "[Epoch 16] Batch 40/105 loss=3.8788\n",
      "[Epoch 16] Batch 60/105 loss=3.4818\n",
      "[Epoch 16] Batch 80/105 loss=3.7257\n",
      "[Epoch 16] Batch 100/105 loss=3.8966\n",
      "\n",
      "[FINAL TRAIN] Epoch 17/25\n",
      "[Epoch 17] Batch 0/105 loss=3.4311\n",
      "[Epoch 17] Batch 20/105 loss=3.8416\n",
      "[Epoch 17] Batch 40/105 loss=4.0712\n",
      "[Epoch 17] Batch 60/105 loss=3.7418\n",
      "[Epoch 17] Batch 80/105 loss=3.7426\n",
      "[Epoch 17] Batch 100/105 loss=3.4666\n",
      "\n",
      "[FINAL TRAIN] Epoch 18/25\n",
      "[Epoch 18] Batch 0/105 loss=3.4778\n",
      "[Epoch 18] Batch 20/105 loss=3.2924\n",
      "[Epoch 18] Batch 40/105 loss=3.6091\n",
      "[Epoch 18] Batch 60/105 loss=3.4944\n",
      "[Epoch 18] Batch 80/105 loss=3.4937\n",
      "[Epoch 18] Batch 100/105 loss=3.5459\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 19/25\n",
      "[Epoch 19] Batch 0/105 loss=3.5228\n",
      "[Epoch 19] Batch 20/105 loss=3.5961\n",
      "[Epoch 19] Batch 40/105 loss=3.3990\n",
      "[Epoch 19] Batch 60/105 loss=3.7933\n",
      "[Epoch 19] Batch 80/105 loss=3.2107\n",
      "[Epoch 19] Batch 100/105 loss=3.3978\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 20/25\n",
      "[Epoch 20] Batch 0/105 loss=3.1809\n",
      "[Epoch 20] Batch 20/105 loss=3.3558\n",
      "[Epoch 20] Batch 40/105 loss=3.9049\n",
      "[Epoch 20] Batch 60/105 loss=3.3786\n",
      "[Epoch 20] Batch 80/105 loss=3.2743\n",
      "[Epoch 20] Batch 100/105 loss=3.4489\n",
      "\n",
      "[FINAL TRAIN] Epoch 21/25\n",
      "[Epoch 21] Batch 0/105 loss=3.2974\n",
      "[Epoch 21] Batch 20/105 loss=3.0297\n",
      "[Epoch 21] Batch 40/105 loss=3.4101\n",
      "[Epoch 21] Batch 60/105 loss=3.4326\n",
      "[Epoch 21] Batch 80/105 loss=3.4341\n",
      "[Epoch 21] Batch 100/105 loss=3.3902\n",
      "\n",
      "[FINAL TRAIN] Epoch 22/25\n",
      "[Epoch 22] Batch 0/105 loss=3.6997\n",
      "[Epoch 22] Batch 20/105 loss=3.4466\n",
      "[Epoch 22] Batch 40/105 loss=3.5074\n",
      "[Epoch 22] Batch 60/105 loss=3.5982\n",
      "[Epoch 22] Batch 80/105 loss=3.1067\n",
      "[Epoch 22] Batch 100/105 loss=3.3687\n",
      "Saved new best optimized model\n",
      "\n",
      "[FINAL TRAIN] Epoch 23/25\n",
      "[Epoch 23] Batch 0/105 loss=3.3497\n",
      "[Epoch 23] Batch 20/105 loss=3.2773\n",
      "[Epoch 23] Batch 40/105 loss=3.2498\n",
      "[Epoch 23] Batch 60/105 loss=3.5952\n",
      "[Epoch 23] Batch 80/105 loss=3.5549\n",
      "[Epoch 23] Batch 100/105 loss=3.2454\n",
      "\n",
      "[FINAL TRAIN] Epoch 24/25\n",
      "[Epoch 24] Batch 0/105 loss=3.4436\n",
      "[Epoch 24] Batch 20/105 loss=3.4702\n",
      "[Epoch 24] Batch 40/105 loss=3.5718\n",
      "[Epoch 24] Batch 60/105 loss=3.6762\n",
      "[Epoch 24] Batch 80/105 loss=3.0041\n",
      "[Epoch 24] Batch 100/105 loss=3.3514\n",
      "\n",
      "[FINAL TRAIN] Epoch 25/25\n",
      "[Epoch 25] Batch 0/105 loss=3.3549\n",
      "[Epoch 25] Batch 20/105 loss=3.3706\n",
      "[Epoch 25] Batch 40/105 loss=3.1984\n",
      "[Epoch 25] Batch 60/105 loss=2.9711\n",
      "[Epoch 25] Batch 80/105 loss=3.5903\n",
      "[Epoch 25] Batch 100/105 loss=2.8915\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "best_lr           = best_params[\"lr\"]\n",
    "best_weight_decay = best_params[\"weight_decay\"]\n",
    "best_drop         = best_params[\"drop\"]\n",
    "best_lambda_attr  = best_params[\"lambda_attr\"]\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "model = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=best_drop,\n",
    ").to(DEVICE)\n",
    "\n",
    "LAMBDA_ATTR = best_lambda_attr\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[FINAL TRAIN] Epoch {epoch}/{EPOCHS}\")\n",
    "    train_one_epoch(epoch)\n",
    "    _, _, _, val_acc = evaluate()\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"vit_best_model_optimized.pth\")\n",
    "        print(\"Saved new best optimized model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0ff533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 25\n",
      "Best trial value (val_acc): 0.023769100169779286\n",
      "Best hyperparameters:\n",
      "  lr: 0.0004273669312062059\n",
      "  weight_decay: 2.3739423955292538e-05\n",
      "  drop: 0.10912464381815776\n",
      "  lambda_attr: 0.010368427651183114\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial value (val_acc):\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4305c16-e782-4882-bc9b-6df3eaea2342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label\n",
      "0   1     33\n",
      "1   2     55\n",
      "2   3     28\n",
      "3   4     34\n",
      "4   5     16\n",
      "\n",
      "Saved vit_submission.csv\n"
     ]
    }
   ],
   "source": [
    "best_vit = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "best_vit.load_state_dict(torch.load(\"vit_best_model.pth\", map_location=DEVICE))\n",
    "best_vit.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits, attr_pred = best_vit(imgs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids.numpy().tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission_vit = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(submission_vit.head())\n",
    "\n",
    "submission_vit.to_csv(\"vit_submission.csv\", index=False)\n",
    "print(\"\\nSaved vit_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
