{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6a1dc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b7fab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Config & Seeds\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16   # 224 / 16 = 14 patches per side -> 196 patches total\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Random seed set to:\", RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "116c5f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV shape: (3926, 2)\n",
      "Test  CSV shape: (4000, 3)\n",
      "            image_path  label\n",
      "0  /train_images/1.jpg      1\n",
      "1  /train_images/2.jpg      1\n",
      "2  /train_images/3.jpg      1\n",
      "3  /train_images/4.jpg      1\n",
      "4  /train_images/5.jpg      1\n",
      "\n",
      "Attributes shape: (200, 312)\n",
      "NUM_CLASSES: 200 | NUM_ATTR: 312\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")   # folder you showed in the screenshot\n",
    "\n",
    "# --- CSVs with paths + labels ---\n",
    "train_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n",
    "test_df  = pd.read_csv(DATA_DIR / \"test_images_path.csv\")\n",
    "\n",
    "print(\"Train CSV shape:\", train_df.shape)\n",
    "print(\"Test  CSV shape:\", test_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# --- Attributes ---\n",
    "ATTR_PATH = DATA_DIR / \"attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR    = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)\n",
    "\n",
    "# image root folders\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train_images\"\n",
    "TEST_IMG_DIR  = DATA_DIR / \"test_images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e558804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e02ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "209f5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 99\n",
      "Val batches: 25\n",
      "Test batches: 125\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Datasets & Dataloaders\n",
    "\n",
    "# Cell 5: Datasets & Dataloaders\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, df, attributes, img_root, img_col=\"image_path\", label_col=\"label\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # 1. FIX PATH: Handle absolute vs relative path issue\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # 2. FIX INDEX ERROR: Subtract 1 to make labels 0-indexed (0 to 199)\n",
    "        # Assuming your CSV labels are 1-200.\n",
    "        label = int(row[self.label_col]) - 1\n",
    "\n",
    "        attr_vec = self.attributes[label]  # Now accesses indices 0..199\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (\n",
    "            img,\n",
    "            torch.tensor(attr_vec, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, df, img_root, img_col=\"image_path\", id_col=\"id\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.id_col = id_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # 1. FIX PATH here too\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        img_id = int(row[self.id_col])\n",
    "        return img, img_id\n",
    "\n",
    "# --- Setup Datasets & Loaders ---\n",
    "\n",
    "# Re-initialize datasets with the new classes\n",
    "full_train_dataset = BirdTrainDataset(\n",
    "    df=train_df,\n",
    "    attributes=attributes,\n",
    "    img_root=TRAIN_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    label_col=\"label\",\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# 80/20 split into train / val\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size   = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED),\n",
    ")\n",
    "\n",
    "test_dataset = BirdTestDataset(\n",
    "    df=test_df,\n",
    "    img_root=TEST_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    id_col=\"id\",\n",
    "    transform=eval_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7517fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ViT Building Blocks (PatchEmbed & TransformerEncoderBlock)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        x = self.proj(x)               # [B, embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2)               # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)          # [B, N, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=drop,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ac4b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDA_ATTR = 0.05\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Loss Functions (classification + attribute regression)\n",
    "\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05   # weight for attribute regression loss\n",
    "\n",
    "print(\"LAMBDA_ATTR =\", LAMBDA_ATTR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "031754f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: MAEViT (\"Nuclear Option\": Mask 75% and reconstruct)\n",
    "\n",
    "class MAEViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoencoder with ViT backbone.\n",
    "    Encoder = patch embedding + transformer encoder\n",
    "    Decoder = tiny transformer that reconstructs masked patches.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_chans=3,\n",
    "        embed_dim=192,\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        decoder_embed_dim=128,\n",
    "        decoder_depth=4,\n",
    "        decoder_num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, decoder_embed_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=decoder_embed_dim,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=decoder_depth)\n",
    "\n",
    "        patch_dim = patch_size * patch_size * in_chans\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.num_patches = num_patches\n",
    "        self.norm_pix_loss = True\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    # ---- Patchify / Unpatchify ----\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: [B, 3, H, W] -> [B, L, patch_dim]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = imgs.shape\n",
    "        assert H == self.img_size and W == self.img_size\n",
    "\n",
    "        h = H // p\n",
    "        w = W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)      # [B, h, w, p, p, C]\n",
    "        x = x.reshape(B, h * w, p * p * C)   # [B, L, patch_dim]\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, L, patch_dim] -> [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, L, patch_dim = x.shape\n",
    "        C = self.in_chans\n",
    "        h = w = int(L ** 0.5)\n",
    "        assert h * w == L\n",
    "\n",
    "        x = x.reshape(B, h, w, p, p, C)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        imgs = x.reshape(B, C, h * p, w * p)\n",
    "        return imgs\n",
    "\n",
    "    # ---- Random Masking ----\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        x: [B, L, D]\n",
    "        Returns: x_masked, mask, ids_restore\n",
    "        mask: 1 = masked, 0 = visible\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(B, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(\n",
    "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "        )\n",
    "\n",
    "        mask = torch.ones(B, L, device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    # ---- Encoder & Decoder ----\n",
    "\n",
    "    def forward_encoder(self, imgs):\n",
    "        x = self.patch_embed(imgs)          # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x_masked, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "        x_encoded = self.encoder(x_masked)\n",
    "        return x_encoded, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x_encoded, ids_restore):\n",
    "        B, L_vis, D = x_encoded.shape\n",
    "        x = self.decoder_embed(x_encoded)   # [B, L_vis, D_dec]\n",
    "\n",
    "        L = self.num_patches\n",
    "        L_mask = L - L_vis\n",
    "        mask_tokens = self.mask_token.repeat(B, L_mask, 1)\n",
    "\n",
    "        x_ = torch.cat([x, mask_tokens], dim=1)        # [B, L, D_dec]\n",
    "        index = ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n",
    "        x_full = torch.gather(x_, dim=1, index=index)  # [B, L, D_dec]\n",
    "\n",
    "        x_full = x_full + self.decoder_pos_embed\n",
    "        x_full = self.decoder(x_full)\n",
    "        pred = self.decoder_pred(x_full)               # [B, L, patch_dim]\n",
    "        return pred\n",
    "\n",
    "    def loss(self, imgs, pred, mask):\n",
    "        target = self.patchify(imgs)\n",
    "\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1e-6).sqrt()\n",
    "\n",
    "        loss_per_patch = (pred - target) ** 2\n",
    "        loss_per_patch = loss_per_patch.mean(dim=-1)\n",
    "        loss = (loss_per_patch * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x_encoded, mask, ids_restore = self.forward_encoder(imgs)\n",
    "        pred = self.forward_decoder(x_encoded, ids_restore)\n",
    "        loss = self.loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "370bdfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_path', 'label'], dtype='object')\n",
      "            image_path  label\n",
      "0  /train_images/1.jpg      1\n",
      "1  /train_images/2.jpg      1\n",
      "2  /train_images/3.jpg      1\n",
      "3  /train_images/4.jpg      1\n",
      "4  /train_images/5.jpg      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")  # adjust if needed\n",
    "train_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n",
    "\n",
    "print(train_df.columns)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151df5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MAE Pretraining for 20 epochs...\n",
      "Metric to watch: 'Reconstruction Loss' (Lower is better)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad31b114ba2d4be3af4abccb8eb9ee10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9: Self-supervised pretraining of MAE on train images\n",
    "from tqdm.auto import tqdm  # specific import for progress bars in notebooks\n",
    "\n",
    "def pretrain_mae(mae_model, loader, epochs=20, lr=1e-4, weight_decay=0.05):\n",
    "    optimizer = optim.AdamW(mae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    mae_model.train()\n",
    "\n",
    "    print(f\"Starting MAE Pretraining for {epochs} epochs...\")\n",
    "    print(\"Metric to watch: 'Reconstruction Loss' (Lower is better)\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        samples = 0\n",
    "        \n",
    "        # Create a progress bar for the dataloader\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "        \n",
    "        for imgs, _, _ in pbar:   # ignore attributes + labels (we don't use them yet!)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, _, _ = mae_model(imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            samples += bs\n",
    "            \n",
    "            # Update the progress bar with the running average loss\n",
    "            current_avg_loss = total_loss / samples\n",
    "            pbar.set_postfix({\"Recon Loss\": f\"{current_avg_loss:.4f}\"})\n",
    "\n",
    "        # Final print for the epoch log\n",
    "        print(f\"[MAE] Epoch {epoch}/{epochs} completed. Avg Loss: {total_loss / samples:.4f}\")\n",
    "\n",
    "    return mae_model\n",
    "\n",
    "\n",
    "mae = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,   # 75% masked: \"nuclear option\"\n",
    ").to(DEVICE)\n",
    "\n",
    "mae = pretrain_mae(mae, train_loader, epochs=20, lr=1e-4, weight_decay=0.05)\n",
    "torch.save(mae.state_dict(), \"mae_pretrained_nuclear.pth\")\n",
    "print(\"Saved MAE nuclear weights -> mae_pretrained_nuclear.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Classifier + attribute head reusing MAE encoder\n",
    "\n",
    "class MAEViTClassifierWithAttributes(nn.Module):\n",
    "    \"\"\"\n",
    "    Reuses MAE encoder as backbone, adds:\n",
    "      - class head (NUM_CLASSES)\n",
    "      - attribute regression head (NUM_ATTR)\n",
    "    \"\"\"\n",
    "    def __init__(self, mae_model: MAEViT, num_classes: int, num_attr: int, drop: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = mae_model.patch_embed\n",
    "        self.pos_embed   = mae_model.pos_embed\n",
    "        self.encoder     = mae_model.encoder\n",
    "        self.embed_dim   = mae_model.pos_embed.shape[-1]\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        self.head_class = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.head_attr  = nn.Linear(self.embed_dim, num_attr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)          # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x)              # [B, L, D]\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)                # global average pooling over patches\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.head_class(x)\n",
    "        attr_pred = self.head_attr(x)\n",
    "        return logits, attr_pred\n",
    "\n",
    "\n",
    "# Reload MAE weights into fresh encoder\n",
    "mae_for_ft = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,\n",
    ").to(DEVICE)\n",
    "\n",
    "mae_for_ft.load_state_dict(torch.load(\"mae_pretrained_nuclear.pth\", map_location=DEVICE))\n",
    "\n",
    "# Option: freeze encoder (backbone acts like fixed \"pretrained\")\n",
    "for p in mae_for_ft.patch_embed.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in mae_for_ft.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "mae_for_ft.pos_embed.requires_grad = False\n",
    "\n",
    "model = MAEViTClassifierWithAttributes(\n",
    "    mae_model=mae_for_ft,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "LAMBDA_ATTR = 0.05\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(\"MAE classifier ready. Trainable params:\",\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training & evaluation loops (no Bayesian optimisation)\n",
    "\n",
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_cls  += loss_cls.item() * bs\n",
    "        total_attr += loss_attr.item() * bs\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += bs\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            batch_acc = (preds == labels).float().mean().item()\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"- Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Epoch {epoch_idx}] Train Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Train Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_cls  += loss_cls.item() * bs\n",
    "            total_attr += loss_attr.item() * bs\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += bs\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Validation] Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Val Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feaa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Train MAE-backed classifier (\"nuclear option\", no Bayes)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[NUCLEAR MAE] Epoch {epoch}/{EPOCHS}\")\n",
    "    train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"  -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"mae_nuclear_classifier.pth\")\n",
    "        print(\"Saved new best nuclear MAE classifier -> mae_nuclear_classifier.pth\")\n",
    "\n",
    "print(\"Best validation accuracy (MAE nuclear):\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Final validation eval with best-saved MAE classifier\n",
    "\n",
    "# Reload a fresh encoder & classifier to ensure we're using the saved best model\n",
    "mae_eval = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,\n",
    ").to(DEVICE)\n",
    "\n",
    "mae_eval.load_state_dict(torch.load(\"mae_pretrained_nuclear.pth\", map_location=DEVICE))\n",
    "\n",
    "best_mae = MAEViTClassifierWithAttributes(\n",
    "    mae_model=mae_eval,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "best_mae.load_state_dict(torch.load(\"mae_nuclear_classifier.pth\", map_location=DEVICE))\n",
    "best_mae.eval()\n",
    "\n",
    "val_correct = 0\n",
    "val_samples = 0\n",
    "val_total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, attr_targets, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        logits, attr_pred = best_mae(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_samples += imgs.size(0)\n",
    "        val_total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "val_acc = val_correct / val_samples\n",
    "val_loss_avg = val_total_loss / val_samples\n",
    "\n",
    "print(\"\\n=== Final Validation Evaluation (MAE Nuclear) ===\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss_avg:.4f}\")\n",
    "print(f\"Correct Predictions: {val_correct}/{val_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Test predictions + submission using MAE nuclear model\n",
    "\n",
    "# Reuse best_mae from previous cell; if notebook restarted, rebuild as below:\n",
    "\n",
    "# mae_eval = MAEViT(\n",
    "#     img_size=IMG_SIZE,\n",
    "#     patch_size=PATCH_SIZE,\n",
    "#     in_chans=3,\n",
    "#     embed_dim=192,\n",
    "#     depth=6,\n",
    "#     num_heads=3,\n",
    "#     decoder_embed_dim=128,\n",
    "#     decoder_depth=4,\n",
    "#     decoder_num_heads=4,\n",
    "#     mask_ratio=0.75,\n",
    "# ).to(DEVICE)\n",
    "# mae_eval.load_state_dict(torch.load(\"mae_pretrained_nuclear.pth\", map_location=DEVICE))\n",
    "# best_mae = MAEViTClassifierWithAttributes(\n",
    "#     mae_model=mae_eval,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     num_attr=NUM_ATTR,\n",
    "#     drop=0.1,\n",
    "# ).to(DEVICE)\n",
    "# best_mae.load_state_dict(torch.load(\"mae_nuclear_classifier.pth\", map_location=DEVICE))\n",
    "# best_mae.eval()\n",
    "\n",
    "# Test predictions + submission using MAE nuclear model\n",
    "best_mae.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "test_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits, attr_pred = best_mae(imgs)\n",
    "\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids)\n",
    "        all_preds.extend(preds)\n",
    "        test_samples += len(img_ids)\n",
    "\n",
    "print(f\"Total test samples processed: {test_samples}\")\n",
    "print(f\"Predictions made:             {len(all_preds)}\")\n",
    "\n",
    "# Show predicted class distribution\n",
    "all_preds_np = np.array(all_preds)\n",
    "unique, counts = np.unique(all_preds_np, return_counts=True)\n",
    "print(\"\\nPredicted class distribution on test set (0-based):\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  class {u}: {c} samples\")\n",
    "\n",
    "# If competition expects labels starting from 1 instead of 0:\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels,\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head())\n",
    "\n",
    "submission.to_csv(\"mae_nuclear_submission.csv\", index=False)\n",
    "print(\"\\nâœ“ Saved mae_nuclear_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
