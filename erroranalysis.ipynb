{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4933187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/anaconda3/lib/python3.13/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75dda1",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "1. Computational Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c20379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import os\n",
    "import shutil\n",
    "from transformers import AutoModelForImageClassification\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_from_disk\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48947dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileNetV2ForImageClassification were not initialized from the model checkpoint at google/mobilenet_v2_1.0_224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1001]) in the checkpoint and torch.Size([200]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1001, 1280]) in the checkpoint and torch.Size([200, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTATIONAL COMPLEXITY ANALYSIS\n",
      "\n",
      "--- Simple CNN ---\n",
      "Parameters: 508,104\n",
      "FLOPs (Approx): 817,184,136\n",
      "Size (MB): 0.51\n",
      "--- Baseline (MobileNetV2) ---\n",
      "Parameters: 2,480,072\n",
      "FLOPs (Approx): 299,784,584\n",
      "Size (MB): 2.48\n",
      "--- CoAtNet (Hybrid/Aug/Pre-tune) ---\n",
      "Parameters: 26,820,362\n",
      "FLOPs (Approx): 4,213,652,296\n",
      "Size (MB): 26.82\n",
      "--- ResNet (CNN) ---\n",
      "Parameters: 2,511,944\n",
      "FLOPs (Approx): 214,535,816\n",
      "Size (MB): 2.51\n",
      "--- CoNeXt (Hybrid) ---\n",
      "Parameters: 27,973,928\n",
      "FLOPs (Approx): 321,756,392\n",
      "Size (MB): 27.97\n",
      "--- ViT MAE (Transformer) ---\n",
      "Parameters: 2,954,048\n",
      "FLOPs (Approx): 30,818,048\n",
      "Size (MB): 2.95\n",
      "\n",
      "--- COMPARISON ---\n",
      "CoAtNet vs CNN Params: 52.8x larger\n",
      "CoAtNet vs Baseline Params: 10.8x larger\n"
     ]
    }
   ],
   "source": [
    "# We dont need the trained models, just look at the architectures\n",
    "# COPIED from the cnn scratch model:\n",
    "class BirdCNN(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ResNet model\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample: identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetScratch(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "        )\n",
    "        self.layer1 = BasicBlock(32, 64, stride=2)\n",
    "        self.layer2 = BasicBlock(64, 128, stride=2)\n",
    "        self.layer3 = BasicBlock(128, 256, stride=2)\n",
    "        self.layer4 = BasicBlock(256, 256, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ViT MAE model\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size//patch_size\n",
    "        self.num_patches = self.grid_size**2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim), nn.GELU(), nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim), nn.Dropout(drop),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleViTWithAttributes(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=200, num_attr=312, embed_dim=192, depth=6, num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, drop) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head_class = nn.Linear(embed_dim, num_classes)\n",
    "        self.head_attr  = nn.Linear(embed_dim, num_attr)\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0]\n",
    "        return self.head_class(cls), self.head_attr(cls)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Simple CNN\n",
    "cnn_model = BirdCNN(num_classes=200)\n",
    "\n",
    "# Our hugging face baseline:\n",
    "baseline_model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/mobilenet_v2_1.0_224\", num_labels=200, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Our main model (coatnet, same for pre/post training and augmentation versions)\n",
    "coatnet_model = timm.create_model(\"coatnet_0_rw_224\", pretrained=False, num_classes=200)\n",
    "\n",
    "# ResNet\n",
    "resnet_model = ResNetScratch(num_classes=200)\n",
    "\n",
    "# ConvNeXt\n",
    "convnext_model = timm.create_model(\"convnext_tiny\", pretrained=False, num_classes=200)\n",
    "\n",
    "# ViT MAE\n",
    "vit_model = SimpleViTWithAttributes(img_size=224, patch_size=16, in_chans=3, num_classes=200, num_attr=312, embed_dim=192, depth=6, num_heads=3)\n",
    "\n",
    "def get_stats(model, model_name):\n",
    "    stats = summary(model, input_size=(1, 3, 224, 224), verbose=0)\n",
    "    params = stats.total_params\n",
    "    # FLOPPPPP\n",
    "    flops = stats.total_mult_adds\n",
    "    \n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    print(f\"FLOPs (Approx): {flops:,}\")\n",
    "    print(f\"Size (MB): {stats.to_megabytes(params):.2f}\")\n",
    "    return params, flops\n",
    "\n",
    "print(\"COMPUTATIONAL COMPLEXITY ANALYSIS\\n\")\n",
    "cnn_p, cnn_f = get_stats(cnn_model, \"Simple CNN\")\n",
    "base_p, base_f = get_stats(baseline_model, \"Baseline (MobileNetV2)\")\n",
    "coat_p, coat_f = get_stats(coatnet_model, \"CoAtNet (Hybrid/Aug/Pre-tune)\")\n",
    "res_p, res_f = get_stats(resnet_model, \"ResNet (CNN)\")\n",
    "conv_p, conv_f = get_stats(convnext_model, \"CoNeXt (Hybrid)\")\n",
    "vit_p, vit_f = get_stats(vit_model, \"ViT MAE (Transformer)\")\n",
    "\n",
    "print(\"\\n--- COMPARISON ---\")\n",
    "print(f\"CoAtNet vs CNN Params: {coat_p / cnn_p:.1f}x larger\")\n",
    "print(f\"CoAtNet vs Baseline Params: {coat_p / base_p:.1f}x larger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9956e2",
   "metadata": {},
   "source": [
    "2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329cfb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Inference for Error Analysis.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"processed_bird_data\"\n",
    "MODEL_PATH = \"final_new_model/model.safetensors\"\n",
    "\n",
    "# Load data\n",
    "dataset = load_from_disk(DATA_PATH)\n",
    "val_ds = dataset[\"validation\"]\n",
    "\n",
    "# Transforms\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "val_transforms = Compose([Resize(256), CenterCrop(224), ToTensor(), normalize])\n",
    "\n",
    "def transform_fn(batch):\n",
    "    batch[\"pixel_values\"] = [val_transforms(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "val_ds.set_transform(transform_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, collate_fn=lambda x: {\n",
    "    \"pixel_values\": torch.stack([i[\"pixel_values\"] for i in x]), \n",
    "    \"labels\": torch.tensor([i[\"label\"] for i in x])\n",
    "})\n",
    "\n",
    "# Load Model\n",
    "model = timm.create_model(\"coatnet_0_rw_224\", pretrained=False, num_classes=200)\n",
    "from safetensors.torch import load_file\n",
    "model.load_state_dict(load_file(MODEL_PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "confidences = []\n",
    "images_for_plot = []\n",
    "\n",
    "print(\"Running Inference for Error Analysis.\")\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get max prob and predicted class\n",
    "        max_probs, preds = torch.max(probs, dim=1)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "        confidences.extend(max_probs.cpu().numpy())\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd6275",
   "metadata": {},
   "source": [
    "First right and wrong predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b24c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Failures (Use 2 for Poster):\n",
      "     True  Pred  Confidence  Index\n",
      "94     68    36    0.999438     94\n",
      "284    43     9    0.998038    284\n",
      "14     26    28    0.997386     14\n",
      "\n",
      "Top 3 Successes (Use 2 for Poster):\n",
      "     True  Pred  Confidence  Index\n",
      "335    11    11    0.999415    335\n",
      "126    33    33    0.999210    126\n",
      "58      6     6    0.999071     58\n",
      "\n",
      "Saving images to folder: poster_images...\n",
      "Saved: poster_images/fail_1_true68_pred36.png (Conf: 0.9994)\n",
      "Saved: poster_images/fail_2_true43_pred9.png (Conf: 0.9980)\n",
      "Saved: poster_images/fail_3_true26_pred28.png (Conf: 0.9974)\n",
      "Saved: poster_images/success_1_true11_pred11.png (Conf: 0.9994)\n",
      "Saved: poster_images/success_2_true33_pred33.png (Conf: 0.9992)\n",
      "Saved: poster_images/success_3_true6_pred6.png (Conf: 0.9991)\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = load_from_disk(DATA_PATH)[\"validation\"]\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    \"True\": true_labels,\n",
    "    \"Pred\": pred_labels,\n",
    "    \"Confidence\": confidences,\n",
    "    \"Index\": range(len(true_labels))\n",
    "})\n",
    "\n",
    "# flops\n",
    "wrong_preds = df_results[df_results[\"True\"] != df_results[\"Pred\"]]\n",
    "top_wrong = wrong_preds.sort_values(by=\"Confidence\", ascending=False).head(3)\n",
    "\n",
    "# successes\n",
    "correct_preds = df_results[df_results[\"True\"] == df_results[\"Pred\"]]\n",
    "top_correct = correct_preds.sort_values(by=\"Confidence\", ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 Failures (Use 2 for Poster):\")\n",
    "print(top_wrong)\n",
    "print(\"\\nTop 3 Successes (Use 2 for Poster):\")\n",
    "print(top_correct)\n",
    "\n",
    "output_dir = \"poster_images\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "print(f\"\\nSaving images to folder: {output_dir}...\")\n",
    "\n",
    "def save_crops(df, prefix):\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        idx = int(row[\"Index\"])\n",
    "        true_cls = int(row[\"True\"])\n",
    "        pred_cls = int(row[\"Pred\"])\n",
    "        conf = row[\"Confidence\"]\n",
    "        \n",
    "        img = raw_val_ds[idx][\"image\"]\n",
    "        fname = f\"{output_dir}/{prefix}_{i+1}_true{true_cls}_pred{pred_cls}.png\"\n",
    "        img.save(fname)\n",
    "        print(f\"Saved: {fname} (Conf: {conf:.4f})\")\n",
    "\n",
    "save_crops(top_wrong, \"fail\")\n",
    "save_crops(top_correct, \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ce3ec",
   "metadata": {},
   "source": [
    "Now actual confusion matrix (200x200 is hard to had to get creative), so its better to show which pairs of birds are confused most often:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2199ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Most Confused Pairs (Model predicts B when it is actually A):\n",
      "True Class 28 -> Predicted as Class 8 (Count: 2)\n",
      "True Class 64 -> Predicted as Class 61 (Count: 2)\n",
      "True Class 78 -> Predicted as Class 92 (Count: 2)\n",
      "True Class 97 -> Predicted as Class 11 (Count: 2)\n",
      "True Class 55 -> Predicted as Class 34 (Count: 3)\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "np.fill_diagonal(cm, 0)\n",
    "\n",
    "# Find max confusion\n",
    "flat_indices = np.argsort(cm.flatten())[-5:] # Top 5 confused pairs\n",
    "rows, cols = np.unravel_index(flat_indices, cm.shape)\n",
    "\n",
    "print(\"\\nTop 5 Most Confused Pairs (Model predicts B when it is actually A):\")\n",
    "for r, c in zip(rows, cols):\n",
    "    print(f\"True Class {r} -> Predicted as Class {c} (Count: {cm[r, c]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
