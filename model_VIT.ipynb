{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac745e5-0ac5-4f90-ac57-0543c89e695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8fa109-6a5f-45e9-9050-e498d77dd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09249b9-254e-44ef-b415-40729cf1048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/val dataset from: processed_bird_data\n",
      "Train size: 3337\n",
      "Val size: 589\n",
      "\n",
      "Loading test dataset from: processed_bird_test_data\n",
      "Test size: 4000\n",
      "\n",
      "Attributes shape: (200, 312)\n",
      "NUM_CLASSES: 200 | NUM_ATTR: 312\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VAL_PATH = \"processed_bird_data\"\n",
    "TEST_PATH = \"processed_bird_test_data\"\n",
    "\n",
    "print(\"Loading train/val dataset from:\", TRAIN_VAL_PATH)\n",
    "full_ds = load_from_disk(TRAIN_VAL_PATH)\n",
    "train_hf = full_ds[\"train\"]\n",
    "val_hf = full_ds[\"validation\"]\n",
    "\n",
    "print(\"Train size:\", len(train_hf))\n",
    "print(\"Val size:\", len(val_hf))\n",
    "\n",
    "print(\"\\nLoading test dataset from:\", TEST_PATH)\n",
    "test_hf = load_from_disk(TEST_PATH)\n",
    "print(\"Test size:\", len(test_hf))\n",
    "\n",
    "# attributes\n",
    "ATTR_PATH = \"data/attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f443e22-bcdc-4d53-b2a8-75260db502da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb6bb57d-7ff9-4497-ab63-386a1f0dd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, attributes, transform=None):\n",
    "        self.ds = hf_dataset\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "        label = int(item[\"label\"]) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        attr_vec = self.attributes[label]\n",
    "        attr_vec = torch.from_numpy(attr_vec)\n",
    "\n",
    "        return img, attr_vec, label\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.ds = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_id = int(item[\"id\"])\n",
    "        return img, img_id\n",
    "\n",
    "\n",
    "train_dataset = BirdTrainDataset(train_hf, attributes, transform=train_transform)\n",
    "val_dataset   = BirdTrainDataset(val_hf,   attributes, transform=eval_transform)\n",
    "test_dataset  = BirdTestDataset(test_hf,   transform=eval_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9f4495-261b-4151-875a-9b8911a741cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleViTWithAttributes(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderBlock(\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (head_class): Linear(in_features=192, out_features=200, bias=True)\n",
       "  (head_attr): Linear(in_features=192, out_features=312, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size//patch_size\n",
    "        self.num_patches = self.grid_size**2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [B,3,H,W]\n",
    "        x = self.proj(x) #[B,embed_dim,H/P,W/P]\n",
    "        x = x.flatten(2) #[B,embed_dim,num_patches]\n",
    "        x = x.transpose(1, 2) #[B,num_patches,embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x [B,N,D]\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleViTWithAttributes(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 num_classes=200, num_attr=312, embed_dim=192, depth=6,\n",
    "                 num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, drop)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head_class = nn.Linear(embed_dim, num_classes)\n",
    "        # gÅ‚owa atrybutowa\n",
    "        self.head_attr  = nn.Linear(embed_dim, num_attr)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head_class.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head_attr.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:[B,3,224,224]\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x) #[B,N,D]\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1) #[B,1,D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1) #[B,1+N,D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0] #[B,D]\n",
    "\n",
    "        logits = self.head_class(cls)\n",
    "        attr_pred = self.head_attr(cls)\n",
    "        return logits, attr_pred\n",
    "\n",
    "\n",
    "model = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13ca56e-54cd-4681-acae-4a924b24c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6714d61-1bb1-485a-9f19-f3bd241ce9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_cls  += loss_cls.item() * imgs.size(0)\n",
    "        total_attr += loss_attr.item() * imgs.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += imgs.size(0)\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total_cls  += loss_cls.item() * imgs.size(0)\n",
    "            total_attr += loss_attr.item() * imgs.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += imgs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    return avg_loss, avg_cls, avg_attr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5803f8f-ede0-479c-ac77-27c98afe70f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n",
      "[Epoch 1] Batch 0/105 loss=5.4359\n",
      "[Epoch 1] Batch 20/105 loss=5.3092\n",
      "[Epoch 1] Batch 40/105 loss=5.1971\n",
      "[Epoch 1] Batch 60/105 loss=5.0976\n",
      "[Epoch 1] Batch 80/105 loss=5.2808\n",
      "[Epoch 1] Batch 100/105 loss=4.8651\n",
      "Val:   loss=5.1342 (cls=5.1340, attr=0.0044), acc=0.0153\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 2/25\n",
      "[Epoch 2] Batch 0/105 loss=4.9859\n",
      "[Epoch 2] Batch 20/105 loss=5.0588\n",
      "[Epoch 2] Batch 40/105 loss=4.8415\n",
      "[Epoch 2] Batch 60/105 loss=5.0961\n",
      "[Epoch 2] Batch 80/105 loss=4.9912\n",
      "[Epoch 2] Batch 100/105 loss=5.0554\n",
      "Val:   loss=5.0907 (cls=5.0906, attr=0.0034), acc=0.0170\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 3/25\n",
      "[Epoch 3] Batch 0/105 loss=4.9734\n",
      "[Epoch 3] Batch 20/105 loss=4.6914\n",
      "[Epoch 3] Batch 40/105 loss=4.7898\n",
      "[Epoch 3] Batch 60/105 loss=4.9327\n",
      "[Epoch 3] Batch 80/105 loss=5.1013\n",
      "[Epoch 3] Batch 100/105 loss=4.9826\n",
      "Val:   loss=5.0147 (cls=5.0146, attr=0.0029), acc=0.0136\n",
      "\n",
      "Epoch 4/25\n",
      "[Epoch 4] Batch 0/105 loss=4.7168\n",
      "[Epoch 4] Batch 20/105 loss=5.1933\n",
      "[Epoch 4] Batch 40/105 loss=4.8781\n",
      "[Epoch 4] Batch 60/105 loss=4.7267\n",
      "[Epoch 4] Batch 80/105 loss=4.5974\n",
      "[Epoch 4] Batch 100/105 loss=4.8700\n",
      "Val:   loss=4.9651 (cls=4.9649, attr=0.0029), acc=0.0255\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 5/25\n",
      "[Epoch 5] Batch 0/105 loss=4.6161\n",
      "[Epoch 5] Batch 20/105 loss=4.8325\n",
      "[Epoch 5] Batch 40/105 loss=4.7066\n",
      "[Epoch 5] Batch 60/105 loss=4.6543\n",
      "[Epoch 5] Batch 80/105 loss=5.1138\n",
      "[Epoch 5] Batch 100/105 loss=4.6497\n",
      "Val:   loss=4.8634 (cls=4.8633, attr=0.0030), acc=0.0306\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 6/25\n",
      "[Epoch 6] Batch 0/105 loss=4.4699\n",
      "[Epoch 6] Batch 20/105 loss=4.7106\n",
      "[Epoch 6] Batch 40/105 loss=4.5364\n",
      "[Epoch 6] Batch 60/105 loss=5.1580\n",
      "[Epoch 6] Batch 80/105 loss=4.6217\n",
      "[Epoch 6] Batch 100/105 loss=4.6387\n",
      "Val:   loss=4.8018 (cls=4.8017, attr=0.0027), acc=0.0289\n",
      "\n",
      "Epoch 7/25\n",
      "[Epoch 7] Batch 0/105 loss=4.0409\n",
      "[Epoch 7] Batch 20/105 loss=4.4406\n",
      "[Epoch 7] Batch 40/105 loss=4.6004\n",
      "[Epoch 7] Batch 60/105 loss=4.6602\n",
      "[Epoch 7] Batch 80/105 loss=4.4288\n",
      "[Epoch 7] Batch 100/105 loss=4.2956\n",
      "Val:   loss=4.6888 (cls=4.6886, attr=0.0027), acc=0.0306\n",
      "\n",
      "Epoch 8/25\n",
      "[Epoch 8] Batch 0/105 loss=4.5410\n",
      "[Epoch 8] Batch 20/105 loss=4.1116\n",
      "[Epoch 8] Batch 40/105 loss=4.2763\n",
      "[Epoch 8] Batch 60/105 loss=4.6267\n",
      "[Epoch 8] Batch 80/105 loss=4.2691\n",
      "[Epoch 8] Batch 100/105 loss=4.4719\n",
      "Val:   loss=4.6822 (cls=4.6820, attr=0.0027), acc=0.0340\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 9/25\n",
      "[Epoch 9] Batch 0/105 loss=4.1714\n",
      "[Epoch 9] Batch 20/105 loss=4.6331\n",
      "[Epoch 9] Batch 40/105 loss=4.6509\n",
      "[Epoch 9] Batch 60/105 loss=4.2010\n",
      "[Epoch 9] Batch 80/105 loss=4.6123\n",
      "[Epoch 9] Batch 100/105 loss=4.3887\n",
      "Val:   loss=4.5935 (cls=4.5933, attr=0.0025), acc=0.0492\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 10/25\n",
      "[Epoch 10] Batch 0/105 loss=4.3860\n",
      "[Epoch 10] Batch 20/105 loss=4.1712\n",
      "[Epoch 10] Batch 40/105 loss=4.3814\n",
      "[Epoch 10] Batch 60/105 loss=4.0901\n",
      "[Epoch 10] Batch 80/105 loss=4.1097\n",
      "[Epoch 10] Batch 100/105 loss=4.1558\n",
      "Val:   loss=4.5653 (cls=4.5652, attr=0.0024), acc=0.0441\n",
      "\n",
      "Epoch 11/25\n",
      "[Epoch 11] Batch 0/105 loss=3.9891\n",
      "[Epoch 11] Batch 20/105 loss=3.8704\n",
      "[Epoch 11] Batch 40/105 loss=3.9021\n",
      "[Epoch 11] Batch 60/105 loss=3.9826\n",
      "[Epoch 11] Batch 80/105 loss=4.3887\n",
      "[Epoch 11] Batch 100/105 loss=3.7233\n",
      "Val:   loss=4.5101 (cls=4.5100, attr=0.0023), acc=0.0577\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 12/25\n",
      "[Epoch 12] Batch 0/105 loss=4.0928\n",
      "[Epoch 12] Batch 20/105 loss=4.1494\n",
      "[Epoch 12] Batch 40/105 loss=4.0691\n",
      "[Epoch 12] Batch 60/105 loss=3.8419\n",
      "[Epoch 12] Batch 80/105 loss=4.0279\n",
      "[Epoch 12] Batch 100/105 loss=3.7107\n",
      "Val:   loss=4.4790 (cls=4.4789, attr=0.0023), acc=0.0526\n",
      "\n",
      "Epoch 13/25\n",
      "[Epoch 13] Batch 0/105 loss=4.0348\n",
      "[Epoch 13] Batch 20/105 loss=4.0442\n",
      "[Epoch 13] Batch 40/105 loss=3.8035\n",
      "[Epoch 13] Batch 60/105 loss=3.9901\n",
      "[Epoch 13] Batch 80/105 loss=4.1171\n",
      "[Epoch 13] Batch 100/105 loss=3.8017\n",
      "Val:   loss=4.4284 (cls=4.4283, attr=0.0022), acc=0.0526\n",
      "\n",
      "Epoch 14/25\n",
      "[Epoch 14] Batch 0/105 loss=3.8686\n",
      "[Epoch 14] Batch 20/105 loss=3.7913\n",
      "[Epoch 14] Batch 40/105 loss=3.7526\n",
      "[Epoch 14] Batch 60/105 loss=4.2585\n",
      "[Epoch 14] Batch 80/105 loss=3.5727\n",
      "[Epoch 14] Batch 100/105 loss=3.9917\n",
      "Val:   loss=4.4484 (cls=4.4483, attr=0.0023), acc=0.0543\n",
      "\n",
      "Epoch 15/25\n",
      "[Epoch 15] Batch 0/105 loss=4.1044\n",
      "[Epoch 15] Batch 20/105 loss=3.9261\n",
      "[Epoch 15] Batch 40/105 loss=3.7417\n",
      "[Epoch 15] Batch 60/105 loss=3.6597\n",
      "[Epoch 15] Batch 80/105 loss=4.0910\n",
      "[Epoch 15] Batch 100/105 loss=3.9420\n",
      "Val:   loss=4.4066 (cls=4.4065, attr=0.0022), acc=0.0475\n",
      "\n",
      "Epoch 16/25\n",
      "[Epoch 16] Batch 0/105 loss=3.6623\n",
      "[Epoch 16] Batch 20/105 loss=3.7673\n",
      "[Epoch 16] Batch 40/105 loss=3.9246\n",
      "[Epoch 16] Batch 60/105 loss=3.5212\n",
      "[Epoch 16] Batch 80/105 loss=3.4559\n",
      "[Epoch 16] Batch 100/105 loss=3.7186\n",
      "Val:   loss=4.3880 (cls=4.3879, attr=0.0022), acc=0.0611\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 17/25\n",
      "[Epoch 17] Batch 0/105 loss=3.4288\n",
      "[Epoch 17] Batch 20/105 loss=3.6712\n",
      "[Epoch 17] Batch 40/105 loss=4.1690\n",
      "[Epoch 17] Batch 60/105 loss=3.5773\n",
      "[Epoch 17] Batch 80/105 loss=3.8602\n",
      "[Epoch 17] Batch 100/105 loss=3.5331\n",
      "Val:   loss=4.3464 (cls=4.3463, attr=0.0022), acc=0.0594\n",
      "\n",
      "Epoch 18/25\n",
      "[Epoch 18] Batch 0/105 loss=3.5266\n",
      "[Epoch 18] Batch 20/105 loss=3.3420\n",
      "[Epoch 18] Batch 40/105 loss=3.5312\n",
      "[Epoch 18] Batch 60/105 loss=3.6006\n",
      "[Epoch 18] Batch 80/105 loss=3.4079\n",
      "[Epoch 18] Batch 100/105 loss=3.5835\n",
      "Val:   loss=4.3427 (cls=4.3426, attr=0.0021), acc=0.0662\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 19/25\n",
      "[Epoch 19] Batch 0/105 loss=3.5592\n",
      "[Epoch 19] Batch 20/105 loss=3.6606\n",
      "[Epoch 19] Batch 40/105 loss=3.4272\n",
      "[Epoch 19] Batch 60/105 loss=3.7897\n",
      "[Epoch 19] Batch 80/105 loss=3.2078\n",
      "[Epoch 19] Batch 100/105 loss=3.4143\n",
      "Val:   loss=4.3141 (cls=4.3140, attr=0.0021), acc=0.0662\n",
      "\n",
      "Epoch 20/25\n",
      "[Epoch 20] Batch 0/105 loss=3.3861\n",
      "[Epoch 20] Batch 20/105 loss=3.4571\n",
      "[Epoch 20] Batch 40/105 loss=3.8276\n",
      "[Epoch 20] Batch 60/105 loss=3.4240\n",
      "[Epoch 20] Batch 80/105 loss=3.4528\n",
      "[Epoch 20] Batch 100/105 loss=3.5568\n",
      "Val:   loss=4.3128 (cls=4.3127, attr=0.0021), acc=0.0730\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 21/25\n",
      "[Epoch 21] Batch 0/105 loss=3.4677\n",
      "[Epoch 21] Batch 20/105 loss=3.2269\n",
      "[Epoch 21] Batch 40/105 loss=3.4106\n",
      "[Epoch 21] Batch 60/105 loss=3.5609\n",
      "[Epoch 21] Batch 80/105 loss=3.4298\n",
      "[Epoch 21] Batch 100/105 loss=3.4289\n",
      "Val:   loss=4.3087 (cls=4.3086, attr=0.0021), acc=0.0662\n",
      "\n",
      "Epoch 22/25\n",
      "[Epoch 22] Batch 0/105 loss=3.7383\n",
      "[Epoch 22] Batch 20/105 loss=3.5324\n",
      "[Epoch 22] Batch 40/105 loss=3.4771\n",
      "[Epoch 22] Batch 60/105 loss=3.5035\n",
      "[Epoch 22] Batch 80/105 loss=3.2137\n",
      "[Epoch 22] Batch 100/105 loss=3.5158\n",
      "Val:   loss=4.2978 (cls=4.2977, attr=0.0021), acc=0.0815\n",
      "Best VIT model saved\n",
      "\n",
      "Epoch 23/25\n",
      "[Epoch 23] Batch 0/105 loss=3.3780\n",
      "[Epoch 23] Batch 20/105 loss=3.3914\n",
      "[Epoch 23] Batch 40/105 loss=3.2701\n",
      "[Epoch 23] Batch 60/105 loss=3.6853\n",
      "[Epoch 23] Batch 80/105 loss=3.6045\n",
      "[Epoch 23] Batch 100/105 loss=3.3583\n",
      "Val:   loss=4.3036 (cls=4.3035, attr=0.0021), acc=0.0764\n",
      "\n",
      "Epoch 24/25\n",
      "[Epoch 24] Batch 0/105 loss=3.6091\n",
      "[Epoch 24] Batch 20/105 loss=3.4968\n",
      "[Epoch 24] Batch 40/105 loss=3.5812\n",
      "[Epoch 24] Batch 60/105 loss=3.8698\n",
      "[Epoch 24] Batch 80/105 loss=3.0998\n",
      "[Epoch 24] Batch 100/105 loss=3.2856\n",
      "Val:   loss=4.3012 (cls=4.3011, attr=0.0021), acc=0.0747\n",
      "\n",
      "Epoch 25/25\n",
      "[Epoch 25] Batch 0/105 loss=3.5351\n",
      "[Epoch 25] Batch 20/105 loss=3.4104\n",
      "[Epoch 25] Batch 40/105 loss=3.1185\n",
      "[Epoch 25] Batch 60/105 loss=3.0743\n",
      "[Epoch 25] Batch 80/105 loss=3.5168\n",
      "[Epoch 25] Batch 100/105 loss=2.9355\n",
      "Val:   loss=4.3017 (cls=4.3016, attr=0.0021), acc=0.0798\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Val:   loss={val_loss:.4f} (cls={val_cls:.4f}, attr={val_attr:.4f}), acc={val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"vit_best_model.pth\")\n",
    "        print(\"Best VIT model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4305c16-e782-4882-bc9b-6df3eaea2342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label\n",
      "0   1     35\n",
      "1   2     99\n",
      "2   3      7\n",
      "3   4     12\n",
      "4   5     74\n",
      "\n",
      "Saved vit_submission.csv\n"
     ]
    }
   ],
   "source": [
    "best_vit = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "best_vit.load_state_dict(torch.load(\"vit_best_model.pth\", map_location=DEVICE))\n",
    "best_vit.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits, attr_pred = best_vit(imgs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids.numpy().tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission_vit = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(submission_vit.head())\n",
    "\n",
    "submission_vit.to_csv(\"vit_submission.csv\", index=False)\n",
    "print(\"\\nSaved vit_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1baa4-a872-4759-8e67-39f24d545af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
