{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e521e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (1.0.22)\n",
      "Requirement already satisfied: torch in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from timm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from timm) (0.24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from timm) (0.34.4)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch->timm) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torchvision->timm) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.12 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: optuna in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (1.16.4)\n",
      "Requirement already satisfied: colorlog in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: kaleido in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: choreographer>=1.1.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from kaleido) (1.2.1)\n",
      "Requirement already satisfied: logistro>=1.0.8 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from kaleido) (2.0.1)\n",
      "Requirement already satisfied: orjson>=3.10.15 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from kaleido) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from kaleido) (24.2)\n",
      "Requirement already satisfied: pytest-timeout>=2.4.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from kaleido) (2.4.0)\n",
      "Requirement already satisfied: simplejson>=3.19.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from choreographer>=1.1.1->kaleido) (3.20.2)\n",
      "Requirement already satisfied: pytest>=7.0.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pytest-timeout>=2.4.0->kaleido) (8.3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (0.4.6)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install ipywidgets\n",
    "!pip install optuna plotly\n",
    "!pip install kaleido\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506a5cf",
   "metadata": {},
   "source": [
    "# new model - coatnet\n",
    "hybrid architecture - Convolution + Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8f6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import evaluate\n",
    "import sys\n",
    "import shutil\n",
    "import safetensors.torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import safetensors.torch\n",
    "from torch.utils.data import DataLoader\n",
    "from optuna.visualization.matplotlib import plot_optimization_history, plot_param_importances\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from datasets import load_from_disk\n",
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, ToTensor, Normalize, \n",
    "    RandomHorizontalFlip, RandomResizedCrop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e284eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading and transforming data.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"processed_bird_data\"\n",
    "OUTPUT_DIR = \"new_model_checkpoints\"\n",
    "MODEL_NAME = \"coatnet_0_rw_224\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading and transforming data.\")\n",
    "dataset = load_from_disk(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adfe7b",
   "metadata": {},
   "source": [
    "Applying Aggressive Data Augmentation to prevent overfitting:\n",
    "\n",
    "1. Does random resize/zoom (scale 0.8â€“1.0). Forces the model to recognize a bird by looking at its specific feature (e.g. the head, wing, etc.) instead of the background (e.g. tress).\n",
    "2. Incorporates horizontal flipping to double the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07295e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready.\n"
     ]
    }
   ],
   "source": [
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "_train_transforms = Compose([\n",
    "    RandomResizedCrop(224, scale=(0.8, 1.0)), \n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "_val_transforms = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def train_transforms(batch):\n",
    "    batch[\"pixel_values\"] = [_train_transforms(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "def val_transforms(batch):\n",
    "    batch[\"pixel_values\"] = [_val_transforms(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "# fix nonetype error\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    train_transforms, batched=True, remove_columns=[\"image\"]\n",
    ")\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(\n",
    "    val_transforms, batched=True, remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "print(\"Data ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66360a0",
   "metadata": {},
   "source": [
    "# Training\n",
    "Initializing CoAtNet model with random weights, no pretraining. \n",
    "\n",
    "It uses standard Convolutional layers in the early stages to extract low-level features (edges, textures), and then uses Transformer layers in the final stages to understand the global shape of the bird. Using Bayesian optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d851fc-42bc-4e07-9072-11e57cb0ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "class TimmTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.epoch_train_loss = 0.0\n",
    "        self.epoch_train_acc = 0.0\n",
    "        self.epoch_steps = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        pixel_values = inputs.get(\"pixel_values\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        logits = model(pixel_values)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, logits) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        # Move inputs to the correct device (GPU/MPS) - CRITICAL FIX\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if \"pixel_values\" in inputs:\n",
    "                outputs = model(inputs[\"pixel_values\"])\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            loss = None\n",
    "            labels = inputs.get(\"labels\")\n",
    "            \n",
    "        return (loss, outputs, labels)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c68fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-14 14:56:15,964] A new study created in memory with name: no-name-0b66506a-07e3-431f-8583-50be5c7d8312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkowa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Bayesian optimization with Optuna\n",
    "def objective(trial):\n",
    "    # hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-1, log=True)\n",
    "    \n",
    "    model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=200)\n",
    "    model.to(device)\n",
    "    \n",
    "    run_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/trial_{trial.number}\", \n",
    "        per_device_train_batch_size=32, \n",
    "        num_train_epochs=5,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=0.1,\n",
    "        disable_tqdm=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = TimmTrainer(\n",
    "        model=model,\n",
    "        args=run_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    metrics = trainer.evaluate()\n",
    "    accuracy = metrics[\"eval_accuracy\"]\n",
    "    \n",
    "    trial.report(accuracy, step=5)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "print(\"starting optimization.\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"Best accuracy: {study.best_value*100:.2f}%\")\n",
    "print(f\"Best learning rate: {study.best_params['lr']:.6f}\")\n",
    "print(f\"Best weight decay: {study.best_params['weight_decay']:.6f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a80576-44a1-4953-a5f1-d1503c9413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.figure()\n",
    "    plot_optimization_history(study)\n",
    "    plt.title(\"Optimization History\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"optuna_history.png\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Skipping history plot: {e}\")\n",
    "\n",
    "try:\n",
    "    plt.figure()\n",
    "    plot_param_importances(study)\n",
    "    plt.title(\"Hyperparameter Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"optuna_importance.png\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Skipping importance plot: {e}\")\n",
    "\n",
    "print(\"Plots saved successfully (or skipped if invalid).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cafe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "model = timm.create_model(\n",
    "    MODEL_NAME, \n",
    "    pretrained=False, \n",
    "    num_classes=200 \n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "class TimmTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.epoch_train_loss = 0.0\n",
    "        self.epoch_train_acc = 0.0\n",
    "        self.epoch_steps = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        pixel_values = inputs.get(\"pixel_values\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        logits = model(pixel_values)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        if model.training:\n",
    "            with torch.no_grad():\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                acc = (preds == labels).float().mean().item()\n",
    "                \n",
    "                self.epoch_train_loss += loss.item()\n",
    "                self.epoch_train_acc += acc\n",
    "                self.epoch_steps += 1\n",
    "                \n",
    "                if self.epoch_steps % 20 == 0:\n",
    "                    current_epoch_float = self.state.epoch if self.state.epoch is not None else 0\n",
    "                    print(f\" >> Epoch: {current_epoch_float:.2f} | Batch: {self.epoch_steps} | Curr Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return (loss, logits) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if \"pixel_values\" in inputs:\n",
    "                outputs = model(inputs[\"pixel_values\"])\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            loss = None\n",
    "            labels = inputs.get(\"labels\")\n",
    "            \n",
    "        return (loss, outputs, labels)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        avg_train_loss = self.epoch_train_loss / self.epoch_steps if self.epoch_steps > 0 else 0\n",
    "        avg_train_acc = self.epoch_train_acc / self.epoch_steps if self.epoch_steps > 0 else 0\n",
    "        \n",
    "        val_loss = metrics.get(f\"{metric_key_prefix}_loss\", 0.0)\n",
    "        val_acc = metrics.get(f\"{metric_key_prefix}_accuracy\", 0.0)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\" Training Loss:   {avg_train_loss:.4f} | Training Acc:   {avg_train_acc*100:.2f}%\")\n",
    "        print(f\" Validation Loss: {val_loss:.4f}       | Validation Acc: {val_acc*100:.2f}%\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Reset counters\n",
    "        self.epoch_train_loss = 0.0\n",
    "        self.epoch_train_acc = 0.0\n",
    "        self.epoch_steps = 0\n",
    "        return metrics\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32, \n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5.9e-4,\n",
    "    weight_decay=0.065,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    load_best_model_at_end=True,     \n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    \n",
    "    disable_tqdm=True, \n",
    "    logging_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = TimmTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"final_new_model\")\n",
    "print(\"Best model (Seed 42) saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433eb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "class TimmTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.epoch_train_loss = 0.0\n",
    "        self.epoch_train_acc = 0.0\n",
    "        self.epoch_steps = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        pixel_values = inputs.get(\"pixel_values\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        logits = model(pixel_values)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            if \"pixel_values\" in inputs:\n",
    "                outputs = model(inputs[\"pixel_values\"])\n",
    "            else:\n",
    "                raise ValueError(f\"Batch is empty! Keys found: {inputs.keys()}\")\n",
    "            \n",
    "            loss = None\n",
    "            labels = inputs.get(\"labels\")\n",
    "            \n",
    "        return (loss, outputs, labels)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        self.epoch_train_loss = 0.0\n",
    "        self.epoch_train_acc = 0.0\n",
    "        self.epoch_steps = 0\n",
    "        return metrics\n",
    "\n",
    "# Baseline (seed 42)\n",
    "print(\"\\n=== Loading Seed 42 Model to Establish Baseline ===\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=200)\n",
    "\n",
    "try:\n",
    "    state_dict = safetensors.torch.load_file(\"final_new_model/model.safetensors\", device=device)\n",
    "except FileNotFoundError:\n",
    "    state_dict = safetensors.torch.load_file(f\"{OUTPUT_DIR}/final_new_model/model.safetensors\", device=device)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate Baseline\n",
    "eval_trainer = TimmTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"temp_eval\", \n",
    "        report_to=\"none\", \n",
    "        per_device_eval_batch_size=32,\n",
    "        remove_unused_columns=False \n",
    "    ),\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "base_metrics = eval_trainer.evaluate()\n",
    "global_best_acc = base_metrics[\"eval_accuracy\"]\n",
    "print(f\"Current Best Accuracy (Seed 42): {global_best_acc*100:.2f}%\")\n",
    "\n",
    "# Exoeriment:\n",
    "other_seeds = [1, 100]\n",
    "\n",
    "for seed in other_seeds:\n",
    "    print(f\"\\n{'='*20} Running Seed {seed} {'='*20}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Re-init fresh model\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=200)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Args for this seed\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/seed_{seed}\",\n",
    "        per_device_train_batch_size=32, \n",
    "        num_train_epochs=30,\n",
    "        learning_rate=5.9e-4,     \n",
    "        weight_decay=0.065,       \n",
    "        warmup_ratio=0.1,\n",
    "        load_best_model_at_end=True,     \n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        save_total_limit=1,\n",
    "        seed=seed,\n",
    "        disable_tqdm=True,\n",
    "        logging_strategy=\"epoch\", \n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = TimmTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    # Check result\n",
    "    metrics = trainer.evaluate()\n",
    "    current_acc = metrics[\"eval_accuracy\"]\n",
    "    print(f\"Seed {seed} Accuracy: {current_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save if better\n",
    "    if current_acc > global_best_acc:\n",
    "        print(f\" >>> New best found! ({current_acc*100:.2f}% > {global_best_acc*100:.2f}%) Overwriting final model.\")\n",
    "        global_best_acc = current_acc\n",
    "        trainer.save_model(\"final_new_model\")\n",
    "    else:\n",
    "        print(f\" >>> Did not beat current best ({global_best_acc*100:.2f}%). Keeping previous model.\")\n",
    "\n",
    "print(f\"\\nFinal Experiment Finished. Absolute Best Accuracy: {global_best_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f9c12",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"processed_bird_test_data\"\n",
    "MODEL_PATH = \"final_new_model\"\n",
    "MODEL_NAME = \"coatnet_0_rw_224\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "test_transforms = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def apply_test_transforms(batch):\n",
    "    batch[\"pixel_values\"] = [test_transforms(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "print(f\"Loading test data from '{TEST_DATA_PATH}'.\")\n",
    "try:\n",
    "    test_dataset = load_from_disk(TEST_DATA_PATH)\n",
    "    if \"test\" in test_dataset:\n",
    "        test_dataset = test_dataset[\"test\"]\n",
    "        \n",
    "    print(f\"Applying transforms to {len(test_dataset)} images.\")\n",
    "    test_dataset = test_dataset.map(apply_test_transforms, batched=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"id\"])\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Data ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    test_loader = None\n",
    "\n",
    "if test_loader:\n",
    "    print(f\"Loading best model weights from '{MODEL_PATH}'.\")\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=200)\n",
    "    \n",
    "    try:\n",
    "        state_dict = safetensors.torch.load_file(f\"{MODEL_PATH}/model.safetensors\", device=device)\n",
    "        print(\"Loaded SafeTensors.\")\n",
    "    except FileNotFoundError:\n",
    "        state_dict = torch.load(f\"{MODEL_PATH}/pytorch_model.bin\", map_location=device)\n",
    "        print(\"Loaded PyTorch Bin.\")\n",
    "        \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "\n",
    "    print(\"Generating predictions.\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            ids = batch[\"id\"]\n",
    "            \n",
    "            outputs = model(pixel_values)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_ids.extend(ids.numpy())\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(f\"Processing batch {i}/{len(test_loader)}.\")\n",
    "\n",
    "    print(\"\\nCreating CSV.\")\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"id\": all_ids,\n",
    "        \"label\": all_preds\n",
    "    })\n",
    "\n",
    "    submission_df[\"label\"] = submission_df[\"label\"] + 1\n",
    "    \n",
    "    submission_df = submission_df.sort_values(by=\"id\")\n",
    "\n",
    "    csv_filename = \"coatnet_submission.csv\"\n",
    "    submission_df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"Saved '{csv_filename}' successfully!\")\n",
    "    print(\"\\nFirst 5 rows (Sanity Check):\")\n",
    "    print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a321377-8f0b-4c47-9d97-b5829d262786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset analysis - added later (without rerunning the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8fa6100-ae61-423c-93f7-382513a718ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model weights from seedcheckpoints/seed_100/checkpoint-2835/model.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkowa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GRANULAR CLASS ANALYSIS (4 Groups)\n",
      "============================================================\n",
      "Group A (50): 35.94%\n",
      "Group B (50): 30.34%\n",
      "Group C (50): 19.05%\n",
      "Group D (50): 11.76%\n",
      "\n",
      "Max Accuracy Spread: 24.18 percentage points\n",
      "\n",
      "============================================================\n",
      "STABILITY ANALYSIS (5 Random Splits)\n",
      "============================================================\n",
      "Random Run 1: 28.57%\n",
      "Random Run 2: 26.19%\n",
      "Random Run 3: 29.25%\n",
      "Random Run 4: 30.27%\n",
      "Random Run 5: 29.25%\n",
      "\n",
      "Mean Accuracy: 28.71%\n",
      "Standard Deviation: 1.37%\n",
      "CONCLUSION: Model is STABLE across data selection.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import safetensors.torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# config\n",
    "checkpoint_path = \"seedcheckpoints/seed_100/checkpoint-2835/model.safetensors\"\n",
    "MODEL_NAME = \"coatnet_0_rw_224\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# loading data\n",
    "try:\n",
    "    dataset\n",
    "except NameError:\n",
    "    print(\"Loading dataset from disk...\")\n",
    "    dataset = load_from_disk(\"processed_bird_data\")\n",
    "\n",
    "# transformations \n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "val_transforms_func = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def val_transforms(batch):\n",
    "    batch[\"pixel_values\"] = [val_transforms_func(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "val_dataset = dataset[\"validation\"]\n",
    "val_dataset = val_dataset.map(val_transforms, batched=True)\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "\n",
    "# loading the model from a checkpoint (so we dont need to rerun the code)\n",
    "print(f\"Loading model weights from {checkpoint_path}...\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=200)\n",
    "state_dict = safetensors.torch.load_file(checkpoint_path, device=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# trainer \n",
    "class TimmTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        pixel_values = inputs.get(\"pixel_values\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        logits = model(pixel_values)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            if \"pixel_values\" in inputs:\n",
    "                outputs = model(inputs[\"pixel_values\"])\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            loss = None\n",
    "            labels = inputs.get(\"labels\")\n",
    "            \n",
    "        return (loss, outputs, labels)\n",
    "\n",
    "eval_trainer = TimmTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"temp_eval\", report_to=\"none\", remove_unused_columns=False),\n",
    ")\n",
    "\n",
    "# predictions\n",
    "predictions = eval_trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# extended subset analysis\n",
    "df_results = pd.DataFrame({\"label\": labels, \"pred\": preds})\n",
    "df_results[\"correct\"] = df_results[\"label\"] == df_results[\"pred\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRANULAR CLASS ANALYSIS (4 Groups)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "subsets = [\n",
    "    (0, 50, \"Group A (50)\"),\n",
    "    (50, 100, \"Group B (50)\"),\n",
    "    (100, 150, \"Group C (50)\"),\n",
    "    (150, 200, \"Group D (50)\")\n",
    "]\n",
    "\n",
    "accuracies = []\n",
    "for start, end, name in subsets:\n",
    "    subset = df_results[(df_results[\"label\"] >= start) & (df_results[\"label\"] < end)]\n",
    "    acc = subset[\"correct\"].mean()\n",
    "    accuracies.append(acc)\n",
    "    print(f\"{name}: {acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nMax Accuracy Spread: {(max(accuracies) - min(accuracies))*100:.2f} percentage points\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STABILITY ANALYSIS (5 Random Splits)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "random_accs = []\n",
    "for i in range(1, 6):\n",
    "    random_subset = df_results.sample(frac=0.5, random_state=i*42)\n",
    "    acc = random_subset[\"correct\"].mean()\n",
    "    random_accs.append(acc)\n",
    "    print(f\"Random Run {i}: {acc*100:.2f}%\")\n",
    "\n",
    "mean_rnd = np.mean(random_accs)\n",
    "std_rnd = np.std(random_accs)\n",
    "\n",
    "print(f\"\\nMean Accuracy: {mean_rnd*100:.2f}%\")\n",
    "print(f\"Standard Deviation: {std_rnd*100:.2f}%\")\n",
    "\n",
    "if std_rnd < 0.015:\n",
    "    print(\"CONCLUSION: Model is STABLE across data selection.\")\n",
    "else:\n",
    "    print(\"CONCLUSION: Variance detected across data selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3093b4-27db-4b8a-bb80-dc6325da3664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
