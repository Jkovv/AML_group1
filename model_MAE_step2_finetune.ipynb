{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214de954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d9fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16  \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Random seed set to:\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f244b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV shape: (3926, 2)\n",
      "Test  CSV shape: (4000, 3)\n",
      "            image_path  label\n",
      "0  /train_images/1.jpg      1\n",
      "1  /train_images/2.jpg      1\n",
      "2  /train_images/3.jpg      1\n",
      "3  /train_images/4.jpg      1\n",
      "4  /train_images/5.jpg      1\n",
      "\n",
      "Attributes shape: (200, 312)\n",
      "NUM_CLASSES: 200 | NUM_ATTR: 312\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")   \n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n",
    "test_df  = pd.read_csv(DATA_DIR / \"test_images_path.csv\")\n",
    "\n",
    "print(\"Train CSV shape:\", train_df.shape)\n",
    "print(\"Test  CSV shape:\", test_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# attributes \n",
    "ATTR_PATH = DATA_DIR / \"attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR    = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)\n",
    "\n",
    "# image root folders\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train_images\"\n",
    "TEST_IMG_DIR  = DATA_DIR / \"test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2328b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2914d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, df, attributes, img_root, img_col=\"image_path\", label_col=\"label\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        label = int(row[self.label_col]) - 1\n",
    "\n",
    "        attr_vec = self.attributes[label]  \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (\n",
    "            img,\n",
    "            torch.tensor(attr_vec, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, df, img_root, img_col=\"image_path\", id_col=\"id\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.id_col = id_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        img_id = int(row[self.id_col])\n",
    "        return img, img_id\n",
    "\n",
    "\n",
    "full_train_dataset = BirdTrainDataset(\n",
    "    df=train_df,\n",
    "    attributes=attributes,\n",
    "    img_root=TRAIN_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    label_col=\"label\",\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# 80/20 split into train / val\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size   = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED),\n",
    ")\n",
    "\n",
    "test_dataset = BirdTestDataset(\n",
    "    df=test_df,\n",
    "    img_root=TEST_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    id_col=\"id\",\n",
    "    transform=eval_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12430ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Building Blocks (PatchEmbed & TransformerEncoderBlock)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x:[B,3,H,W]\n",
    "        x = self.proj(x) # [B, embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2) # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2) # [B, N, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=drop,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b781eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDA_ATTR = 0.05\n"
     ]
    }
   ],
   "source": [
    "# loss functions \n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05 # weight for attribute regression loss\n",
    "\n",
    "print(\"LAMBDA_ATTR =\", LAMBDA_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e5dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAEViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoencoder with ViT backbone.\n",
    "    Encoder = patch embedding + transformer encoder\n",
    "    Decoder = tiny transformer that reconstructs masked patches.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_chans=3,\n",
    "        embed_dim=192,\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        decoder_embed_dim=128,\n",
    "        decoder_depth=4,\n",
    "        decoder_num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, decoder_embed_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=decoder_embed_dim,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=decoder_depth)\n",
    "\n",
    "        patch_dim = patch_size * patch_size * in_chans\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.num_patches = num_patches\n",
    "        self.norm_pix_loss = True\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    # patchify / Unpatchify \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: [B, 3, H, W] -> [B, L, patch_dim]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = imgs.shape\n",
    "        assert H == self.img_size and W == self.img_size\n",
    "\n",
    "        h = H // p\n",
    "        w = W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)  # [B, h, w, p, p, C]\n",
    "        x = x.reshape(B, h * w, p * p * C) # [B, L, patch_dim]\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, L, patch_dim] -> [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, L, patch_dim = x.shape\n",
    "        C = self.in_chans\n",
    "        h = w = int(L ** 0.5)\n",
    "        assert h * w == L\n",
    "\n",
    "        x = x.reshape(B, h, w, p, p, C)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        imgs = x.reshape(B, C, h * p, w * p)\n",
    "        return imgs\n",
    "\n",
    "    # Random Masking \n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        x: [B, L, D]\n",
    "        Returns: x_masked, mask, ids_restore\n",
    "        mask: 1 = masked, 0 = visible\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(B, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(\n",
    "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "        )\n",
    "\n",
    "        mask = torch.ones(B, L, device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    # encoder, decoder\n",
    "    def forward_encoder(self, imgs):\n",
    "        x = self.patch_embed(imgs) # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x_masked, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "        x_encoded = self.encoder(x_masked)\n",
    "        return x_encoded, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x_encoded, ids_restore):\n",
    "        B, L_vis, D = x_encoded.shape\n",
    "        x = self.decoder_embed(x_encoded) # [B,L_vis,D_dec]\n",
    "\n",
    "        L = self.num_patches\n",
    "        L_mask = L - L_vis\n",
    "        mask_tokens = self.mask_token.repeat(B, L_mask, 1)\n",
    "\n",
    "        x_ = torch.cat([x, mask_tokens], dim=1) # [B, L, D_dec]\n",
    "        index = ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n",
    "        x_full = torch.gather(x_, dim=1, index=index) # [B, L, D_dec]\n",
    "\n",
    "        x_full = x_full + self.decoder_pos_embed\n",
    "        x_full = self.decoder(x_full)\n",
    "        pred = self.decoder_pred(x_full) #[B,L,patch_dim]\n",
    "        return pred\n",
    "\n",
    "    def loss(self, imgs, pred, mask):\n",
    "        target = self.patchify(imgs)\n",
    "\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1e-6).sqrt()\n",
    "\n",
    "        loss_per_patch = (pred - target) ** 2\n",
    "        loss_per_patch = loss_per_patch.mean(dim=-1)\n",
    "        loss = (loss_per_patch * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x_encoded, mask, ids_restore = self.forward_encoder(imgs)\n",
    "        pred = self.forward_decoder(x_encoded, ids_restore)\n",
    "        loss = self.loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d8f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkowa\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    ").encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MAE Pretraining for 20 epochs...\n",
      "Metrics: Recon Loss (lower better), Masked MSE (lower better)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fb5ea8cf8e4c0ba553055abd6d4caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def pretrain_mae(mae_model, loader, epochs=20, lr=1e-4, weight_decay=0.05):\n",
    "    optimizer = optim.AdamW(mae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    mae_model.train()\n",
    "\n",
    "    print(f\"Starting MAE Pretraining for {epochs} epochs...\")\n",
    "    print(\"Metrics: Recon Loss (lower better), Masked MSE (lower better)\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_masked_mse = 0.0\n",
    "        n_batches = 0\n",
    "        samples = 0\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "\n",
    "        for imgs, _, _ in pbar:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, pred_patches, mask = mae_model(imgs)  # pred_patches: [B,N,D], mask: [B,N]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            samples += bs\n",
    "            total_loss += loss.item() * bs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mask = mask.bool()  # [B,N]\n",
    "                per_patch_mse = (pred_patches ** 2).mean(dim=-1)  # [B,N]\n",
    "                masked_mse = per_patch_mse[mask].mean().item()\n",
    "\n",
    "            total_masked_mse += masked_mse\n",
    "            n_batches += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Recon Loss\": f\"{(total_loss / samples):.4f}\",\n",
    "                \"Masked MSE\": f\"{(total_masked_mse / n_batches):.4f}\"\n",
    "            })\n",
    "\n",
    "        print(f\"[MAE] Epoch {epoch}/{epochs} done | Avg Loss: {total_loss / samples:.4f} | Avg Masked MSE: {total_masked_mse / n_batches:.4f}\")\n",
    "\n",
    "    return mae_model\n",
    "\n",
    "mae = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,   # 75% masked: \"nuclear option\"\n",
    ").to(DEVICE)\n",
    "\n",
    "mae = pretrain_mae(mae, train_loader, epochs=20, lr=1e-4, weight_decay=0.05)\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    mae.encoder.state_dict(),\n",
    "    \"checkpoints/mae_encoder_pretrained.pt\"\n",
    ")\n",
    "print(\"MAE encoder saved correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a759313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier + attribute head reusing MAE encoder\n",
    "LR = 3e-4\n",
    "class MAEViTClassifierWithAttributes(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses pretrained MAE encoder as backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_embed, pos_embed, encoder, num_classes, num_attr, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = patch_embed\n",
    "        self.pos_embed   = pos_embed\n",
    "        self.encoder     = encoder\n",
    "\n",
    "        self.embed_dim = pos_embed.shape[-1]\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        self.head_class = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.head_attr  = nn.Linear(self.embed_dim, num_attr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x) # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x) # [B, L, D]\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.head_class(x), self.head_attr(x)\n",
    "\n",
    "# Reload MAE weights into fresh encoder\n",
    "mae_for_ft = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75, \n",
    ").to(DEVICE)\n",
    "\n",
    "mae_for_ft.encoder.load_state_dict(\n",
    "    torch.load(\n",
    "        \"checkpoints/mae_encoder_pretrained.pt\",\n",
    "        map_location=DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"MAE encoder weights loaded (decoder ignored)\")\n",
    "\n",
    "for p in mae_for_ft.patch_embed.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in mae_for_ft.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "mae_for_ft.pos_embed.requires_grad = False\n",
    "model = MAEViTClassifierWithAttributes(\n",
    "    patch_embed=mae_for_ft.patch_embed,\n",
    "    pos_embed=mae_for_ft.pos_embed,\n",
    "    encoder=mae_for_ft.encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "print(\"MAE classifier ready. Trainable params:\",\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = next(iter(mae_for_ft.encoder.parameters()))\n",
    "print(\"Mean abs encoder weight:\", torch.mean(torch.abs(w)).item())\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"checkpoints/mae_encoder_pretrained.pt\"))\n",
    "encoder.to(DEVICE)\n",
    "print(\"Pretrained MAE encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_cls  += loss_cls.item() * bs\n",
    "        total_attr += loss_attr.item() * bs\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += bs\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            batch_acc = (preds == labels).float().mean().item()\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"- Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Epoch {epoch_idx}] Train Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Train Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_cls  += loss_cls.item() * bs\n",
    "            total_attr += loss_attr.item() * bs\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += bs\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Validation] Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Val Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MAE-backed classifier + checkpoints\n",
    "\n",
    "import os\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "CHECKPOINT_EVERY = 5   # save every epoch \n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[CLS + MAE] Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # Train + Validate\n",
    "    train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "\n",
    "    print(f\"→ Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"mae_nuclear_classifier.pt\")\n",
    "        print(\"Saved new best MAE classifier\")\n",
    "\n",
    "    # Save checkpoint (resume-safe)\n",
    "    if epoch % CHECKPOINT_EVERY == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "        }\n",
    "\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            f\"checkpoints/checkpoint_epoch_{epoch}.pt\"\n",
    ")\n",
    "\n",
    "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "print(\"Best validation accuracy (MAE-backed):\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_attr = nn.MSELoss()\n",
    "\n",
    "# Rebuild MAE skeleton\n",
    "mae_eval = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,\n",
    ")\n",
    "\n",
    "# encoder weights ON CPU\n",
    "mae_eval.encoder.load_state_dict(\n",
    "    torch.load(\n",
    "        \"checkpoints/mae_encoder_pretrained.pt\",\n",
    "        map_location=\"cpu\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# MAE backbone to GPU AFTER loading\n",
    "mae_eval = mae_eval.to(DEVICE)\n",
    "\n",
    "print(\"MAE encoder loaded for evaluation\")\n",
    "\n",
    "best_mae = MAEViTClassifierWithAttributes(\n",
    "    patch_embed=mae_eval.patch_embed,\n",
    "    pos_embed=mae_eval.pos_embed,\n",
    "    encoder=mae_eval.encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ")\n",
    "\n",
    "# classifier weights ON CPU\n",
    "state_dict = torch.load(\n",
    "    \"mae_nuclear_classifier.pt\",\n",
    "    map_location=\"cpu\"\n",
    ")\n",
    "\n",
    "best_mae.load_state_dict(state_dict)\n",
    "\n",
    "# moving the model to GPU AFTER loading\n",
    "best_mae = best_mae.to(DEVICE)\n",
    "best_mae.eval()\n",
    "\n",
    "print(\"Best MAE classifier loaded\")\n",
    "\n",
    "# Final validation loop\n",
    "val_correct = 0\n",
    "val_samples = 0\n",
    "val_total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, attr_targets, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        logits, attr_pred = best_mae(imgs)\n",
    "\n",
    "        loss_cls = criterion_cls(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_samples += imgs.size(0)\n",
    "        val_total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "val_acc = val_correct / val_samples\n",
    "val_loss_avg = val_total_loss / val_samples\n",
    "\n",
    "print(\"\\n Final Validation Evaluation (MAE-pretrained ViT) \")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss_avg:.4f}\")\n",
    "print(f\"Correct Predictions: {val_correct}/{val_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running test inference: 100%|██████████| 125/125 [01:55<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total test samples processed: 4000\n",
      "\n",
      "Predicted class distribution (0-based):\n",
      "  class 0: 462 samples\n",
      "  class 1: 3 samples\n",
      "  class 2: 5 samples\n",
      "  class 3: 67 samples\n",
      "  class 6: 155 samples\n",
      "  class 7: 33 samples\n",
      "  class 9: 165 samples\n",
      "  class 12: 499 samples\n",
      "  class 13: 344 samples\n",
      "  class 15: 75 samples\n",
      "  class 16: 606 samples\n",
      "  class 17: 3 samples\n",
      "  class 19: 70 samples\n",
      "  class 20: 12 samples\n",
      "  class 21: 1 samples\n",
      "  class 25: 71 samples\n",
      "  class 26: 9 samples\n",
      "  class 29: 3 samples\n",
      "  class 30: 357 samples\n",
      "  class 33: 299 samples\n",
      "  class 36: 9 samples\n",
      "  class 39: 52 samples\n",
      "  class 43: 9 samples\n",
      "  class 44: 45 samples\n",
      "  class 49: 39 samples\n",
      "  class 57: 1 samples\n",
      "  class 63: 1 samples\n",
      "  class 68: 1 samples\n",
      "  class 70: 261 samples\n",
      "  class 71: 228 samples\n",
      "  class 84: 34 samples\n",
      "  class 87: 2 samples\n",
      "  class 130: 79 samples\n",
      "\n",
      "First few predictions:\n",
      "   id  label\n",
      "0   1     34\n",
      "1   2     17\n",
      "2   3      1\n",
      "3   4     10\n",
      "4   5     17\n",
      "\n",
      "✓ Saved mae_nuclear_submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_mae.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in tqdm(test_loader, desc=\"Running test inference\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        logits, _ = best_mae(imgs)   # attr_pred not needed\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "print(f\"\\nTotal test samples processed: {len(all_preds)}\")\n",
    "\n",
    "# prediction distribution\n",
    "all_preds_np = np.array(all_preds)\n",
    "unique, counts = np.unique(all_preds_np, return_counts=True)\n",
    "\n",
    "print(\"\\nPredicted class distribution (0-based):\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  class {u}: {c} samples\")\n",
    "\n",
    "# submission (1-based labels if required)\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels,\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head())\n",
    "\n",
    "submission.to_csv(\"mae_nuclear_submission.csv\", index=False)\n",
    "print(\"\\n Saved mae_nuclear_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
