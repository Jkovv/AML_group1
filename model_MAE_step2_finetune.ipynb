{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214de954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d9fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Config & Seeds\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16   # 224 / 16 = 14 patches per side -> 196 patches total\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Random seed set to:\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f244b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV shape: (3926, 2)\n",
      "Test  CSV shape: (4000, 3)\n",
      "            image_path  label\n",
      "0  /train_images/1.jpg      1\n",
      "1  /train_images/2.jpg      1\n",
      "2  /train_images/3.jpg      1\n",
      "3  /train_images/4.jpg      1\n",
      "4  /train_images/5.jpg      1\n",
      "\n",
      "Attributes shape: (200, 312)\n",
      "NUM_CLASSES: 200 | NUM_ATTR: 312\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Splitting\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")   # folder you showed in the screenshot\n",
    "\n",
    "# --- CSVs with paths + labels ---\n",
    "train_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n",
    "test_df  = pd.read_csv(DATA_DIR / \"test_images_path.csv\")\n",
    "\n",
    "print(\"Train CSV shape:\", train_df.shape)\n",
    "print(\"Test  CSV shape:\", test_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# --- Attributes ---\n",
    "ATTR_PATH = DATA_DIR / \"attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR    = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)\n",
    "\n",
    "# image root folders\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train_images\"\n",
    "TEST_IMG_DIR  = DATA_DIR / \"test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2328b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2914d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 99\n",
      "Val batches: 25\n",
      "Test batches: 125\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Datasets & Dataloaders\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, df, attributes, img_root, img_col=\"image_path\", label_col=\"label\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # 1. FIX PATH: Handle absolute vs relative path issue\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # 2. FIX INDEX ERROR: Subtract 1 to make labels 0-indexed (0 to 199)\n",
    "        # Assuming your CSV labels are 1-200.\n",
    "        label = int(row[self.label_col]) - 1\n",
    "\n",
    "        attr_vec = self.attributes[label]  # Now accesses indices 0..199\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (\n",
    "            img,\n",
    "            torch.tensor(attr_vec, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, df, img_root, img_col=\"image_path\", id_col=\"id\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.id_col = id_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        img_id = int(row[self.id_col])\n",
    "        return img, img_id\n",
    "\n",
    "# --- Setup Datasets & Loaders ---\n",
    "\n",
    "# Re-initialize datasets with the new classes\n",
    "full_train_dataset = BirdTrainDataset(\n",
    "    df=train_df,\n",
    "    attributes=attributes,\n",
    "    img_root=TRAIN_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    label_col=\"label\",\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# 80/20 split into train / val\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size   = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED),\n",
    ")\n",
    "\n",
    "test_dataset = BirdTestDataset(\n",
    "    df=test_df,\n",
    "    img_root=TEST_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    id_col=\"id\",\n",
    "    transform=eval_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12430ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ViT Building Blocks (PatchEmbed & TransformerEncoderBlock)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        x = self.proj(x)               # [B, embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2)               # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)          # [B, N, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=drop,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b781eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDA_ATTR = 0.05\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Loss Functions (classification + attribute regression)\n",
    "\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05   # weight for attribute regression loss\n",
    "\n",
    "print(\"LAMBDA_ATTR =\", LAMBDA_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e5dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: MAEViT (\"Nuclear Option\": Mask 75% and reconstruct)\n",
    "\n",
    "class MAEViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoencoder with ViT backbone.\n",
    "    Encoder = patch embedding + transformer encoder\n",
    "    Decoder = tiny transformer that reconstructs masked patches.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_chans=3,\n",
    "        embed_dim=192,\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        decoder_embed_dim=128,\n",
    "        decoder_depth=4,\n",
    "        decoder_num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, decoder_embed_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=decoder_embed_dim,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=decoder_depth)\n",
    "\n",
    "        patch_dim = patch_size * patch_size * in_chans\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.num_patches = num_patches\n",
    "        self.norm_pix_loss = True\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    # ---- Patchify / Unpatchify ----\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: [B, 3, H, W] -> [B, L, patch_dim]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = imgs.shape\n",
    "        assert H == self.img_size and W == self.img_size\n",
    "\n",
    "        h = H // p\n",
    "        w = W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)      # [B, h, w, p, p, C]\n",
    "        x = x.reshape(B, h * w, p * p * C)   # [B, L, patch_dim]\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, L, patch_dim] -> [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, L, patch_dim = x.shape\n",
    "        C = self.in_chans\n",
    "        h = w = int(L ** 0.5)\n",
    "        assert h * w == L\n",
    "\n",
    "        x = x.reshape(B, h, w, p, p, C)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        imgs = x.reshape(B, C, h * p, w * p)\n",
    "        return imgs\n",
    "\n",
    "    # ---- Random Masking ----\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        x: [B, L, D]\n",
    "        Returns: x_masked, mask, ids_restore\n",
    "        mask: 1 = masked, 0 = visible\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(B, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(\n",
    "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "        )\n",
    "\n",
    "        mask = torch.ones(B, L, device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    # ---- Encoder & Decoder ----\n",
    "\n",
    "    def forward_encoder(self, imgs):\n",
    "        x = self.patch_embed(imgs)          # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x_masked, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "        x_encoded = self.encoder(x_masked)\n",
    "        return x_encoded, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x_encoded, ids_restore):\n",
    "        B, L_vis, D = x_encoded.shape\n",
    "        x = self.decoder_embed(x_encoded)   # [B, L_vis, D_dec]\n",
    "\n",
    "        L = self.num_patches\n",
    "        L_mask = L - L_vis\n",
    "        mask_tokens = self.mask_token.repeat(B, L_mask, 1)\n",
    "\n",
    "        x_ = torch.cat([x, mask_tokens], dim=1)        # [B, L, D_dec]\n",
    "        index = ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n",
    "        x_full = torch.gather(x_, dim=1, index=index)  # [B, L, D_dec]\n",
    "\n",
    "        x_full = x_full + self.decoder_pos_embed\n",
    "        x_full = self.decoder(x_full)\n",
    "        pred = self.decoder_pred(x_full)               # [B, L, patch_dim]\n",
    "        return pred\n",
    "\n",
    "    def loss(self, imgs, pred, mask):\n",
    "        target = self.patchify(imgs)\n",
    "\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1e-6).sqrt()\n",
    "\n",
    "        loss_per_patch = (pred - target) ** 2\n",
    "        loss_per_patch = loss_per_patch.mean(dim=-1)\n",
    "        loss = (loss_per_patch * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x_encoded, mask, ids_restore = self.forward_encoder(imgs)\n",
    "        pred = self.forward_decoder(x_encoded, ids_restore)\n",
    "        loss = self.loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d8f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/VanshitaS/miniforge3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    ").encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b7771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MAE Pretraining for 20 epochs...\n",
      "Metrics: Recon Loss (lower better), Masked MSE (lower better)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af3a8026b0e46da9759bc4a3fdd3362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Initialize and pretrain MAE model\u001b[39;00m\n\u001b[32m     53\u001b[39m mae = MAEViT(\n\u001b[32m     54\u001b[39m     img_size=IMG_SIZE,\n\u001b[32m     55\u001b[39m     patch_size=PATCH_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m     mask_ratio=\u001b[32m0.75\u001b[39m,   \u001b[38;5;66;03m# 75% masked: \"nuclear option\"\u001b[39;00m\n\u001b[32m     64\u001b[39m ).to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m mae = \u001b[43mpretrain_mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Save only the encoder\u001b[39;00m\n\u001b[32m     69\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mpretrain_mae\u001b[39m\u001b[34m(mae_model, loader, epochs, lr, weight_decay)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ✅ pbar is defined here\u001b[39;00m\n\u001b[32m     20\u001b[39m pbar = tqdm(loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mBirdTrainDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     24\u001b[39m filename = Path(row[\u001b[38;5;28mself\u001b[39m.img_col]).name\n\u001b[32m     25\u001b[39m img_path = \u001b[38;5;28mself\u001b[39m.img_root / filename\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 2. FIX INDEX ERROR: Subtract 1 to make labels 0-indexed (0 to 199)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Assuming your CSV labels are 1-200.\u001b[39;00m\n\u001b[32m     31\u001b[39m label = \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;28mself\u001b[39m.label_col]) - \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/PIL/Image.py:3504\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3501\u001b[39m     fp = io.BytesIO(fp.read())\n\u001b[32m   3502\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3504\u001b[39m prefix = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3506\u001b[39m preinit()\n\u001b[32m   3508\u001b[39m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 9: Self-supervised pretraining of MAE on train images\n",
    "from tqdm.auto import tqdm  # specific import for progress bars in notebooks\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def pretrain_mae(mae_model, loader, epochs=20, lr=1e-4, weight_decay=0.05):\n",
    "    optimizer = optim.AdamW(mae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    mae_model.train()\n",
    "\n",
    "    print(f\"Starting MAE Pretraining for {epochs} epochs...\")\n",
    "    print(\"Metrics: Recon Loss (lower better), Masked MSE (lower better)\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_masked_mse = 0.0\n",
    "        n_batches = 0\n",
    "        samples = 0\n",
    "\n",
    "        # ✅ pbar is defined here\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "\n",
    "        for imgs, _, _ in pbar:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, pred_patches, mask = mae_model(imgs)  # <-- pred_patches: [B,N,D], mask: [B,N]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            samples += bs\n",
    "            total_loss += loss.item() * bs\n",
    "\n",
    "            # ---- Masked MSE metric (patch-level) ----\n",
    "            with torch.no_grad():\n",
    "                mask = mask.bool()  # [B, N]\n",
    "                per_patch_mse = (pred_patches ** 2).mean(dim=-1)  # [B, N]\n",
    "                masked_mse = per_patch_mse[mask].mean().item()\n",
    "\n",
    "            total_masked_mse += masked_mse\n",
    "            n_batches += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Recon Loss\": f\"{(total_loss / samples):.4f}\",\n",
    "                \"Masked MSE\": f\"{(total_masked_mse / n_batches):.4f}\"\n",
    "            })\n",
    "\n",
    "        print(f\"[MAE] Epoch {epoch}/{epochs} done | Avg Loss: {total_loss / samples:.4f} | Avg Masked MSE: {total_masked_mse / n_batches:.4f}\")\n",
    "\n",
    "    return mae_model\n",
    "\n",
    "# Initialize and pretrain MAE model\n",
    "mae = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,   # 75% masked: \"nuclear option\"\n",
    ").to(DEVICE)\n",
    "\n",
    "mae = pretrain_mae(mae, train_loader, epochs=20, lr=1e-4, weight_decay=0.05)\n",
    "\n",
    "# Save only the encoder\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    mae.encoder.state_dict(),\n",
    "    \"checkpoints/mae_encoder_pretrained.pt\"\n",
    ")\n",
    "\n",
    "print(\"✓ MAE encoder saved correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a759313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MAE encoder weights loaded (decoder ignored)\n",
      "MAE classifier ready. Trainable params: 99200\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Classifier + attribute head reusing MAE encoder\n",
    "LR = 3e-4\n",
    "class MAEViTClassifierWithAttributes(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses pretrained MAE encoder as backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_embed, pos_embed, encoder, num_classes, num_attr, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = patch_embed\n",
    "        self.pos_embed   = pos_embed\n",
    "        self.encoder     = encoder\n",
    "\n",
    "        self.embed_dim = pos_embed.shape[-1]\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        self.head_class = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.head_attr  = nn.Linear(self.embed_dim, num_attr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)      # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x)          # [B, L, D]\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.head_class(x), self.head_attr(x)\n",
    "\n",
    "# Reload MAE weights into fresh encoder\n",
    "mae_for_ft = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75, \n",
    ").to(DEVICE)\n",
    "\n",
    "# Load ONLY encoder weights\n",
    "mae_for_ft.encoder.load_state_dict(\n",
    "    torch.load(\n",
    "        \"checkpoints/mae_encoder_pretrained.pt\",\n",
    "        map_location=DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ MAE encoder weights loaded (decoder ignored)\")\n",
    "\n",
    "for p in mae_for_ft.patch_embed.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in mae_for_ft.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "mae_for_ft.pos_embed.requires_grad = False\n",
    "model = MAEViTClassifierWithAttributes(\n",
    "    patch_embed=mae_for_ft.patch_embed,\n",
    "    pos_embed=mae_for_ft.pos_embed,\n",
    "    encoder=mae_for_ft.encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(\"MAE classifier ready. Trainable params:\",\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08e4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs encoder weight: 0.04385969415307045\n",
      "✓ Pretrained MAE encoder loaded\n"
     ]
    }
   ],
   "source": [
    "w = next(iter(mae_for_ft.encoder.parameters()))\n",
    "print(\"Mean abs encoder weight:\", torch.mean(torch.abs(w)).item())\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"checkpoints/mae_encoder_pretrained.pt\"))\n",
    "encoder.to(DEVICE)\n",
    "print(\"✓ Pretrained MAE encoder loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "056f828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training & evaluation loops (no Bayesian optimisation)\n",
    "\n",
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_cls  += loss_cls.item() * bs\n",
    "        total_attr += loss_attr.item() * bs\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += bs\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            batch_acc = (preds == labels).float().mean().item()\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"- Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Epoch {epoch_idx}] Train Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Train Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_cls  += loss_cls.item() * bs\n",
    "            total_attr += loss_attr.item() * bs\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += bs\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Validation] Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Val Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5de3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLS + MAE] Epoch 1/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[CLS + MAE] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Train + Validate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_loss, train_cls, train_attr, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m val_loss, val_cls, val_attr, val_acc = evaluate()\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m→ Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch_idx)\u001b[39m\n\u001b[32m      8\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m      9\u001b[39m samples = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mBirdTrainDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     33\u001b[39m attr_vec = \u001b[38;5;28mself\u001b[39m.attributes[label]  \u001b[38;5;66;03m# Now accesses indices 0..199\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     39\u001b[39m     img,\n\u001b[32m     40\u001b[39m     torch.tensor(attr_vec, dtype=torch.float32),\n\u001b[32m     41\u001b[39m     torch.tensor(label, dtype=torch.long),\n\u001b[32m     42\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:973\u001b[39m, in \u001b[36mRandomResizedCrop.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    967\u001b[39m \u001b[33;03m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m \u001b[33;03m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    972\u001b[39m i, j, h, w = \u001b[38;5;28mself\u001b[39m.get_params(img, \u001b[38;5;28mself\u001b[39m.scale, \u001b[38;5;28mself\u001b[39m.ratio)\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresized_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchvision/transforms/functional.py:650\u001b[39m, in \u001b[36mresized_crop\u001b[39m\u001b[34m(img, top, left, height, width, size, interpolation, antialias)\u001b[39m\n\u001b[32m    648\u001b[39m     _log_api_usage_once(resized_crop)\n\u001b[32m    649\u001b[39m img = crop(img, top, left, height, width)\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m img = \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:253\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/PIL/Image.py:2304\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2292\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2294\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2295\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2296\u001b[39m         )\n\u001b[32m   2297\u001b[39m         box = (\n\u001b[32m   2298\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2299\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2300\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2301\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2302\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 12: Train MAE-backed classifier + checkpoints\n",
    "\n",
    "import os\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "CHECKPOINT_EVERY = 5   # save every epoch (change to 5 if you want)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[CLS + MAE] Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # Train + Validate\n",
    "    train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "\n",
    "    print(f\"→ Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save BEST model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"mae_nuclear_classifier.pt\")\n",
    "        print(\"✓ Saved new best MAE classifier\")\n",
    "\n",
    "    # Save checkpoint (resume-safe)\n",
    "    if epoch % CHECKPOINT_EVERY == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "        }\n",
    "\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            f\"checkpoints/checkpoint_epoch_{epoch}.pt\"\n",
    ")\n",
    "\n",
    "        print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "print(\"Best validation accuracy (MAE-backed):\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/VanshitaS/miniforge3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MAE encoder loaded for evaluation\n",
      "✓ Best MAE classifier loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Final validation eval with best-saved MAE classifier\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_attr = nn.MSELoss()\n",
    "\n",
    "# 1. Rebuild MAE skeleton\n",
    "mae_eval = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,\n",
    ")\n",
    "\n",
    "# 2. Load encoder weights ON CPU\n",
    "mae_eval.encoder.load_state_dict(\n",
    "    torch.load(\n",
    "        \"checkpoints/mae_encoder_pretrained.pt\",\n",
    "        map_location=\"cpu\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Move MAE backbone to GPU AFTER loading\n",
    "mae_eval = mae_eval.to(DEVICE)\n",
    "\n",
    "print(\"✓ MAE encoder loaded for evaluation\")\n",
    "\n",
    "# 3. Rebuild classifier\n",
    "best_mae = MAEViTClassifierWithAttributes(\n",
    "    patch_embed=mae_eval.patch_embed,\n",
    "    pos_embed=mae_eval.pos_embed,\n",
    "    encoder=mae_eval.encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Load classifier weights ON CPU\n",
    "state_dict = torch.load(\n",
    "    \"mae_nuclear_classifier.pt\",\n",
    "    map_location=\"cpu\"\n",
    ")\n",
    "\n",
    "best_mae.load_state_dict(state_dict)\n",
    "\n",
    "# Move model to GPU AFTER loading\n",
    "best_mae = best_mae.to(DEVICE)\n",
    "best_mae.eval()\n",
    "\n",
    "print(\"✓ Best MAE classifier loaded\")\n",
    "\n",
    "# Final validation loop\n",
    "val_correct = 0\n",
    "val_samples = 0\n",
    "val_total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, attr_targets, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        logits, attr_pred = best_mae(imgs)\n",
    "\n",
    "        loss_cls = criterion_cls(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_samples += imgs.size(0)\n",
    "        val_total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "val_acc = val_correct / val_samples\n",
    "val_loss_avg = val_total_loss / val_samples\n",
    "\n",
    "print(\"\\n=== Final Validation Evaluation (MAE-pretrained ViT) ===\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss_avg:.4f}\")\n",
    "print(f\"Correct Predictions: {val_correct}/{val_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b5d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running test inference: 100%|██████████| 125/125 [01:55<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total test samples processed: 4000\n",
      "\n",
      "Predicted class distribution (0-based):\n",
      "  class 0: 462 samples\n",
      "  class 1: 3 samples\n",
      "  class 2: 5 samples\n",
      "  class 3: 67 samples\n",
      "  class 6: 155 samples\n",
      "  class 7: 33 samples\n",
      "  class 9: 165 samples\n",
      "  class 12: 499 samples\n",
      "  class 13: 344 samples\n",
      "  class 15: 75 samples\n",
      "  class 16: 606 samples\n",
      "  class 17: 3 samples\n",
      "  class 19: 70 samples\n",
      "  class 20: 12 samples\n",
      "  class 21: 1 samples\n",
      "  class 25: 71 samples\n",
      "  class 26: 9 samples\n",
      "  class 29: 3 samples\n",
      "  class 30: 357 samples\n",
      "  class 33: 299 samples\n",
      "  class 36: 9 samples\n",
      "  class 39: 52 samples\n",
      "  class 43: 9 samples\n",
      "  class 44: 45 samples\n",
      "  class 49: 39 samples\n",
      "  class 57: 1 samples\n",
      "  class 63: 1 samples\n",
      "  class 68: 1 samples\n",
      "  class 70: 261 samples\n",
      "  class 71: 228 samples\n",
      "  class 84: 34 samples\n",
      "  class 87: 2 samples\n",
      "  class 130: 79 samples\n",
      "\n",
      "First few predictions:\n",
      "   id  label\n",
      "0   1     34\n",
      "1   2     17\n",
      "2   3      1\n",
      "3   4     10\n",
      "4   5     17\n",
      "\n",
      "✓ Saved mae_nuclear_submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Test predictions + submission \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "best_mae.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in tqdm(test_loader, desc=\"Running test inference\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        logits, _ = best_mae(imgs)   # attr_pred not needed\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "print(f\"\\nTotal test samples processed: {len(all_preds)}\")\n",
    "\n",
    "# Inspect prediction distribution\n",
    "all_preds_np = np.array(all_preds)\n",
    "unique, counts = np.unique(all_preds_np, return_counts=True)\n",
    "\n",
    "print(\"\\nPredicted class distribution (0-based):\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  class {u}: {c} samples\")\n",
    "\n",
    "# Create submission (1-based labels if required)\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels,\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head())\n",
    "\n",
    "submission.to_csv(\"mae_nuclear_submission.csv\", index=False)\n",
    "print(\"\\n✓ Saved mae_nuclear_submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
