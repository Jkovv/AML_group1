{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214de954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config & Seeds\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16   # 224 / 16 = 14 patches per side -> 196 patches total\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Random seed set to:\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Splitting\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")   # folder you showed in the screenshot\n",
    "\n",
    "# --- CSVs with paths + labels ---\n",
    "train_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n",
    "test_df  = pd.read_csv(DATA_DIR / \"test_images_path.csv\")\n",
    "\n",
    "print(\"Train CSV shape:\", train_df.shape)\n",
    "print(\"Test  CSV shape:\", test_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# --- Attributes ---\n",
    "ATTR_PATH = DATA_DIR / \"attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR    = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)\n",
    "\n",
    "# image root folders\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train_images\"\n",
    "TEST_IMG_DIR  = DATA_DIR / \"test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2328b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2914d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Datasets & Dataloaders\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, df, attributes, img_root, img_col=\"image_path\", label_col=\"label\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # 1. FIX PATH: Handle absolute vs relative path issue\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # 2. FIX INDEX ERROR: Subtract 1 to make labels 0-indexed (0 to 199)\n",
    "        # Assuming your CSV labels are 1-200.\n",
    "        label = int(row[self.label_col]) - 1\n",
    "\n",
    "        attr_vec = self.attributes[label]  # Now accesses indices 0..199\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (\n",
    "            img,\n",
    "            torch.tensor(attr_vec, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, df, img_root, img_col=\"image_path\", id_col=\"id\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_col = img_col\n",
    "        self.id_col = id_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        filename = Path(row[self.img_col]).name\n",
    "        img_path = self.img_root / filename\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        img_id = int(row[self.id_col])\n",
    "        return img, img_id\n",
    "\n",
    "# --- Setup Datasets & Loaders ---\n",
    "\n",
    "# Re-initialize datasets with the new classes\n",
    "full_train_dataset = BirdTrainDataset(\n",
    "    df=train_df,\n",
    "    attributes=attributes,\n",
    "    img_root=TRAIN_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    label_col=\"label\",\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# 80/20 split into train / val\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size   = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED),\n",
    ")\n",
    "\n",
    "test_dataset = BirdTestDataset(\n",
    "    df=test_df,\n",
    "    img_root=TEST_IMG_DIR,\n",
    "    img_col=\"image_path\",\n",
    "    id_col=\"id\",\n",
    "    transform=eval_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12430ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ViT Building Blocks (PatchEmbed & TransformerEncoderBlock)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        x = self.proj(x)               # [B, embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2)               # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)          # [B, N, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=drop,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Loss Functions (classification + attribute regression)\n",
    "\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05   # weight for attribute regression loss\n",
    "\n",
    "print(\"LAMBDA_ATTR =\", LAMBDA_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: MAEViT (\"Nuclear Option\": Mask 75% and reconstruct)\n",
    "\n",
    "class MAEViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoencoder with ViT backbone.\n",
    "    Encoder = patch embedding + transformer encoder\n",
    "    Decoder = tiny transformer that reconstructs masked patches.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_chans=3,\n",
    "        embed_dim=192,\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        decoder_embed_dim=128,\n",
    "        decoder_depth=4,\n",
    "        decoder_num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, decoder_embed_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=decoder_embed_dim,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=decoder_depth)\n",
    "\n",
    "        patch_dim = patch_size * patch_size * in_chans\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.num_patches = num_patches\n",
    "        self.norm_pix_loss = True\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    # ---- Patchify / Unpatchify ----\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: [B, 3, H, W] -> [B, L, patch_dim]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = imgs.shape\n",
    "        assert H == self.img_size and W == self.img_size\n",
    "\n",
    "        h = H // p\n",
    "        w = W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)      # [B, h, w, p, p, C]\n",
    "        x = x.reshape(B, h * w, p * p * C)   # [B, L, patch_dim]\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, L, patch_dim] -> [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, L, patch_dim = x.shape\n",
    "        C = self.in_chans\n",
    "        h = w = int(L ** 0.5)\n",
    "        assert h * w == L\n",
    "\n",
    "        x = x.reshape(B, h, w, p, p, C)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        imgs = x.reshape(B, C, h * p, w * p)\n",
    "        return imgs\n",
    "\n",
    "    # ---- Random Masking ----\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        x: [B, L, D]\n",
    "        Returns: x_masked, mask, ids_restore\n",
    "        mask: 1 = masked, 0 = visible\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(B, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(\n",
    "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "        )\n",
    "\n",
    "        mask = torch.ones(B, L, device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    # ---- Encoder & Decoder ----\n",
    "\n",
    "    def forward_encoder(self, imgs):\n",
    "        x = self.patch_embed(imgs)          # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x_masked, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "        x_encoded = self.encoder(x_masked)\n",
    "        return x_encoded, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x_encoded, ids_restore):\n",
    "        B, L_vis, D = x_encoded.shape\n",
    "        x = self.decoder_embed(x_encoded)   # [B, L_vis, D_dec]\n",
    "\n",
    "        L = self.num_patches\n",
    "        L_mask = L - L_vis\n",
    "        mask_tokens = self.mask_token.repeat(B, L_mask, 1)\n",
    "\n",
    "        x_ = torch.cat([x, mask_tokens], dim=1)        # [B, L, D_dec]\n",
    "        index = ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n",
    "        x_full = torch.gather(x_, dim=1, index=index)  # [B, L, D_dec]\n",
    "\n",
    "        x_full = x_full + self.decoder_pos_embed\n",
    "        x_full = self.decoder(x_full)\n",
    "        pred = self.decoder_pred(x_full)               # [B, L, patch_dim]\n",
    "        return pred\n",
    "\n",
    "    def loss(self, imgs, pred, mask):\n",
    "        target = self.patchify(imgs)\n",
    "\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1e-6).sqrt()\n",
    "\n",
    "        loss_per_patch = (pred - target) ** 2\n",
    "        loss_per_patch = loss_per_patch.mean(dim=-1)\n",
    "        loss = (loss_per_patch * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x_encoded, mask, ids_restore = self.forward_encoder(imgs)\n",
    "        pred = self.forward_decoder(x_encoded, ids_restore)\n",
    "        loss = self.loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    ").encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Self-supervised pretraining of MAE on train images\n",
    "from tqdm.auto import tqdm  # specific import for progress bars in notebooks\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def pretrain_mae(mae_model, loader, epochs=20, lr=1e-4, weight_decay=0.05):\n",
    "    optimizer = optim.AdamW(mae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    mae_model.train()\n",
    "\n",
    "    print(f\"Starting MAE Pretraining for {epochs} epochs...\")\n",
    "    print(\"Metrics: Recon Loss (lower better), Masked MSE (lower better)\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_masked_mse = 0.0\n",
    "        n_batches = 0\n",
    "        samples = 0\n",
    "\n",
    "        # ✅ pbar is defined here\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "\n",
    "        for imgs, _, _ in pbar:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, pred_patches, mask = mae_model(imgs)  # <-- pred_patches: [B,N,D], mask: [B,N]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            samples += bs\n",
    "            total_loss += loss.item() * bs\n",
    "\n",
    "            # ---- Masked MSE metric (patch-level) ----\n",
    "            with torch.no_grad():\n",
    "                mask = mask.bool()  # [B, N]\n",
    "                per_patch_mse = (pred_patches ** 2).mean(dim=-1)  # [B, N]\n",
    "                masked_mse = per_patch_mse[mask].mean().item()\n",
    "\n",
    "            total_masked_mse += masked_mse\n",
    "            n_batches += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Recon Loss\": f\"{(total_loss / samples):.4f}\",\n",
    "                \"Masked MSE\": f\"{(total_masked_mse / n_batches):.4f}\"\n",
    "            })\n",
    "\n",
    "        print(f\"[MAE] Epoch {epoch}/{epochs} done | Avg Loss: {total_loss / samples:.4f} | Avg Masked MSE: {total_masked_mse / n_batches:.4f}\")\n",
    "\n",
    "    return mae_model\n",
    "\n",
    "# Initialize and pretrain MAE model\n",
    "mae = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,   # 75% masked: \"nuclear option\"\n",
    ").to(DEVICE)\n",
    "\n",
    "mae = pretrain_mae(mae, train_loader, epochs=20, lr=1e-4, weight_decay=0.05)\n",
    "\n",
    "# Save only the encoder\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    mae.encoder.state_dict(),\n",
    "    \"checkpoints/mae_encoder_pretrained.pt\"\n",
    ")\n",
    "\n",
    "print(\"✓ MAE encoder saved correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a759313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Classifier + attribute head reusing MAE encoder\n",
    "LR = 3e-4\n",
    "class MAEViTClassifierWithAttributes(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses pretrained MAE encoder as backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_embed, pos_embed, encoder, num_classes, num_attr, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = patch_embed\n",
    "        self.pos_embed   = pos_embed\n",
    "        self.encoder     = encoder\n",
    "\n",
    "        self.embed_dim = pos_embed.shape[-1]\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        self.head_class = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.head_attr  = nn.Linear(self.embed_dim, num_attr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)      # [B, L, D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x)          # [B, L, D]\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.head_class(x), self.head_attr(x)\n",
    "\n",
    "# Reload MAE weights into fresh encoder\n",
    "mae_for_ft = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75, \n",
    ").to(DEVICE)\n",
    "\n",
    "# Load ONLY encoder weights\n",
    "mae_for_ft.encoder.load_state_dict(\n",
    "    torch.load(\n",
    "        \"checkpoints/mae_encoder_pretrained.pt\",\n",
    "        map_location=DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ MAE encoder weights loaded (decoder ignored)\")\n",
    "\n",
    "for p in mae_for_ft.patch_embed.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in mae_for_ft.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "mae_for_ft.pos_embed.requires_grad = False\n",
    "model = MAEViTClassifierWithAttributes(\n",
    "    patch_embed=mae_for_ft.patch_embed,\n",
    "    pos_embed=mae_for_ft.pos_embed,\n",
    "    encoder=mae_for_ft.encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(\"MAE classifier ready. Trainable params:\",\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = next(iter(mae_for_ft.encoder.parameters()))\n",
    "print(\"Mean abs encoder weight:\", torch.mean(torch.abs(w)).item())\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"checkpoints/mae_encoder_pretrained.pt\"))\n",
    "encoder.to(DEVICE)\n",
    "print(\"✓ Pretrained MAE encoder loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training & evaluation loops (no Bayesian optimisation)\n",
    "\n",
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_cls  += loss_cls.item() * bs\n",
    "        total_attr += loss_attr.item() * bs\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += bs\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            batch_acc = (preds == labels).float().mean().item()\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"- Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Epoch {epoch_idx}] Train Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Train Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_cls  += loss_cls.item() * bs\n",
    "            total_attr += loss_attr.item() * bs\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += bs\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    print(f\"[Validation] Loss: {avg_loss:.4f} \"\n",
    "          f\"(cls: {avg_cls:.4f}, attr: {avg_attr:.4f}) | \"\n",
    "          f\"Val Acc: {acc:.4f}\")\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Train MAE-backed classifier + checkpoints\n",
    "\n",
    "import os\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "CHECKPOINT_EVERY = 5   # save every epoch (change to 5 if you want)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[CLS + MAE] Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # Train + Validate\n",
    "    train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "\n",
    "    print(f\"→ Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save BEST model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"mae_nuclear_classifier.pt\")\n",
    "        print(\"✓ Saved new best MAE classifier\")\n",
    "\n",
    "    # Save checkpoint (resume-safe)\n",
    "    if epoch % CHECKPOINT_EVERY == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "        }\n",
    "\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            f\"checkpoints/checkpoint_epoch_{epoch}.pt\"\n",
    ")\n",
    "\n",
    "        print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "print(\"Best validation accuracy (MAE-backed):\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Final validation eval with best-saved MAE classifier\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_attr = nn.MSELoss()\n",
    "\n",
    "# 1. Rebuild MAE skeleton\n",
    "mae_eval = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,\n",
    ")\n",
    "\n",
    "# 2. Load encoder weights ON CPU\n",
    "mae_eval.encoder.load_state_dict(\n",
    "    torch.load(\n",
    "        \"checkpoints/mae_encoder_pretrained.pt\",\n",
    "        map_location=\"cpu\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Move MAE backbone to GPU AFTER loading\n",
    "mae_eval = mae_eval.to(DEVICE)\n",
    "\n",
    "print(\"✓ MAE encoder loaded for evaluation\")\n",
    "\n",
    "# 3. Rebuild classifier\n",
    "best_mae = MAEViTClassifierWithAttributes(\n",
    "    patch_embed=mae_eval.patch_embed,\n",
    "    pos_embed=mae_eval.pos_embed,\n",
    "    encoder=mae_eval.encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    drop=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Load classifier weights ON CPU\n",
    "state_dict = torch.load(\n",
    "    \"mae_nuclear_classifier.pt\",\n",
    "    map_location=\"cpu\"\n",
    ")\n",
    "\n",
    "best_mae.load_state_dict(state_dict)\n",
    "\n",
    "# Move model to GPU AFTER loading\n",
    "best_mae = best_mae.to(DEVICE)\n",
    "best_mae.eval()\n",
    "\n",
    "print(\"✓ Best MAE classifier loaded\")\n",
    "\n",
    "# Final validation loop\n",
    "val_correct = 0\n",
    "val_samples = 0\n",
    "val_total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, attr_targets, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        logits, attr_pred = best_mae(imgs)\n",
    "\n",
    "        loss_cls = criterion_cls(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_samples += imgs.size(0)\n",
    "        val_total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "val_acc = val_correct / val_samples\n",
    "val_loss_avg = val_total_loss / val_samples\n",
    "\n",
    "print(\"\\n=== Final Validation Evaluation (MAE-pretrained ViT) ===\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss_avg:.4f}\")\n",
    "print(f\"Correct Predictions: {val_correct}/{val_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Test predictions + submission \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "best_mae.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in tqdm(test_loader, desc=\"Running test inference\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        logits, _ = best_mae(imgs)   # attr_pred not needed\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "print(f\"\\nTotal test samples processed: {len(all_preds)}\")\n",
    "\n",
    "# Inspect prediction distribution\n",
    "all_preds_np = np.array(all_preds)\n",
    "unique, counts = np.unique(all_preds_np, return_counts=True)\n",
    "\n",
    "print(\"\\nPredicted class distribution (0-based):\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  class {u}: {c} samples\")\n",
    "\n",
    "# Create submission (1-based labels if required)\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels,\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head())\n",
    "\n",
    "submission.to_csv(\"mae_nuclear_submission.csv\", index=False)\n",
    "print(\"\\n✓ Saved mae_nuclear_submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
