{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b553b33c-dc5f-4b53-acd5-fac282e346e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Searching for weights in old_hyperparam_model_checkpoints...\n",
      "Selected checkpoint: old_hyperparam_model_checkpoints\\checkpoint-1785\n",
      "Loading test data form processed_bird_test_data...\n",
      "Found 4000 IDs in dataset.\n",
      "Initializing model coatnet_0_rw_224...\n",
      "Loading SafeTensors...\n",
      "Running prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|████████████████████████████████████████████████████████████████████| 125/125 [05:55<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_old_model_fixed.csv\n",
      "   id  label\n",
      "0   1     69\n",
      "1   2     60\n",
      "2   3     52\n",
      "3   4     12\n",
      "4   5     74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import timm\n",
    "import glob\n",
    "import numpy as np\n",
    "import safetensors.torch\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_DATA_PATH = \"processed_bird_test_data\"\n",
    "BASE_CHECKPOINT_DIR = \"old_hyperparam_model_checkpoints\"\n",
    "MODEL_NAME = \"coatnet_0_rw_224\"\n",
    "OUTPUT_FILENAME = \"submission_old_model_fixed.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if not os.path.exists(BASE_CHECKPOINT_DIR):\n",
    "    if os.path.exists(f\"../{BASE_CHECKPOINT_DIR}\"):\n",
    "        BASE_CHECKPOINT_DIR = f\"../{BASE_CHECKPOINT_DIR}\"\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Folder {BASE_CHECKPOINT_DIR} not found\")\n",
    "\n",
    "checkpoints = glob.glob(os.path.join(BASE_CHECKPOINT_DIR, \"checkpoint-*\"))\n",
    "valid_checkpoints = []\n",
    "\n",
    "print(f\"Searching for weights in {BASE_CHECKPOINT_DIR}...\")\n",
    "for ckpt in checkpoints:\n",
    "    if os.path.exists(os.path.join(ckpt, \"model.safetensors\")) or os.path.exists(os.path.join(ckpt, \"pytorch_model.bin\")):\n",
    "        try:\n",
    "            step_num = int(ckpt.split(\"-\")[-1])\n",
    "            valid_checkpoints.append((step_num, ckpt))\n",
    "        except ValueError: continue\n",
    "\n",
    "if not valid_checkpoints:\n",
    "    raise FileNotFoundError(\"No valid checkpoints found.\")\n",
    "\n",
    "valid_checkpoints.sort(key=lambda x: x[0], reverse=True)\n",
    "best_ckpt_step, WEIGHTS_PATH = valid_checkpoints[0]\n",
    "print(f\"Selected checkpoint: {WEIGHTS_PATH}\")\n",
    "\n",
    "print(f\"Loading test data form {TEST_DATA_PATH}...\")\n",
    "dataset_raw = load_from_disk(TEST_DATA_PATH)\n",
    "if isinstance(dataset_raw, dict) and \"test\" in dataset_raw:\n",
    "    test_ds = dataset_raw[\"test\"]\n",
    "else:\n",
    "    test_ds = dataset_raw\n",
    "\n",
    "if \"id\" in test_ds.column_names:\n",
    "    submission_ids = list(test_ds[\"id\"])\n",
    "    print(f\"Found {len(submission_ids)} IDs in dataset.\")\n",
    "else:\n",
    "    print(\"Warning: 'id' column not found. Creating sequential IDs.\")\n",
    "    submission_ids = list(range(len(test_ds)))\n",
    "\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "val_transform = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.hf_dataset[idx][\"image\"]\n",
    "        img = img.convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    SimpleTestDataset(test_ds, transform=val_transform),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Initializing model {MODEL_NAME}...\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=200)\n",
    "\n",
    "path_safe = os.path.join(WEIGHTS_PATH, \"model.safetensors\")\n",
    "path_bin = os.path.join(WEIGHTS_PATH, \"pytorch_model.bin\")\n",
    "\n",
    "if os.path.exists(path_safe):\n",
    "    print(\"Loading SafeTensors...\")\n",
    "    state_dict = safetensors.torch.load_file(path_safe, device=DEVICE)\n",
    "else:\n",
    "    print(\"Loading PyTorch Bin...\")\n",
    "    state_dict = torch.load(path_bin, map_location=DEVICE)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "print(\"Running prediction...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        \n",
    "        logits = model(imgs)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "final_preds = np.array(all_preds) + 1 \n",
    "\n",
    "if len(final_preds) != len(submission_ids):\n",
    "    print(f\"ERROR: Mismatch! IDs: {len(submission_ids)} vs Preds: {len(final_preds)}\")\n",
    "else:\n",
    "    df = pd.DataFrame({\n",
    "        \"id\": submission_ids,\n",
    "        \"label\": final_preds\n",
    "    })\n",
    "    \n",
    "    df = df.sort_values(by=\"id\")\n",
    "    \n",
    "    df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "    print(f\"Saved {OUTPUT_FILENAME}\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13257e-d257-4fdb-a5cc-00cb9acd308e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
