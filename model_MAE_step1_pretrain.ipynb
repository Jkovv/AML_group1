{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c5aecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (2.0.45)\n",
      "Requirement already satisfied: tqdm in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: optuna-integration in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: pytorch_lightning in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: optuna in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna-integration) (4.6.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (2.9.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.10.0)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (4.15.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (0.15.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.13.2)\n",
      "Requirement already satisfied: optuna-integration in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: pytorch_lightning in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: optuna in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna-integration) (4.6.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (2.9.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.10.0)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (4.15.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from pytorch_lightning) (0.15.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.13.2)\n",
      "Requirement already satisfied: setuptools in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.6.0)\n",
      "Requirement already satisfied: filelock in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torchmetrics>0.7.0->pytorch_lightning) (2.3.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (2.0.45)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.22.0)\n",
      "Requirement already satisfied: Mako in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.6.0)\n",
      "Requirement already satisfied: filelock in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from torchmetrics>0.7.0->pytorch_lightning) (2.3.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from optuna->optuna-integration) (2.0.45)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.22.0)\n",
      "Requirement already satisfied: Mako in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/VanshitaS/miniforge3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "!pip install optuna-integration 'pytorch_lightning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac745e5-0ac5-4f90-ac57-0543c89e695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback  # optional, not required\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8fa109-6a5f-45e9-9050-e498d77dd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "TUNE_EPOCHS = 5      # for Bayesian optimisation\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "TUNE_EPOCHS = 5\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58da542",
   "metadata": {},
   "source": [
    "Right now, your functions use global state (model, optimizer, LAMBDA_ATTR, train_loader, val_loader, etc.).\n",
    "We can exploit that and just re-assign the globals inside the Optuna objective to avoid refactoring everything.\n",
    "\n",
    "Idea: for each trial we:\n",
    "* create a new model with its own drop\n",
    "* create a new optimizer (lr, weight_decay)\n",
    "* set LAMBDA_ATTR\n",
    "* train for TUNE_EPOCHS\n",
    "* return best validation accuracy seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d09249b9-254e-44ef-b415-40729cf1048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/val dataset from: processed_bird_data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory processed_bird_data not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m TEST_PATH = \u001b[33m\"\u001b[39m\u001b[33mprocessed_bird_test_data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading train/val dataset from:\u001b[39m\u001b[33m\"\u001b[39m, TRAIN_VAL_PATH)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m full_ds = \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_VAL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m train_hf = full_ds[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m val_hf = full_ds[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/datasets/load.py:1471\u001b[39m, in \u001b[36mload_from_disk\u001b[39m\u001b[34m(dataset_path, keep_in_memory, storage_options)\u001b[39m\n\u001b[32m   1469\u001b[39m fs, *_ = url_to_fs(dataset_path, **(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs.exists(dataset_path):\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fs.isfile(posixpath.join(dataset_path, config.DATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs.isfile(\n\u001b[32m   1473\u001b[39m     posixpath.join(dataset_path, config.DATASET_STATE_JSON_FILENAME)\n\u001b[32m   1474\u001b[39m ):\n\u001b[32m   1475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Directory processed_bird_data not found"
     ]
    }
   ],
   "source": [
    "TRAIN_VAL_PATH = \"processed_bird_data\"\n",
    "TEST_PATH = \"processed_bird_test_data\"\n",
    "\n",
    "print(\"Loading train/val dataset from:\", TRAIN_VAL_PATH)\n",
    "full_ds = load_from_disk(TRAIN_VAL_PATH)\n",
    "train_hf = full_ds[\"train\"]\n",
    "val_hf = full_ds[\"validation\"]\n",
    "\n",
    "print(\"Train size:\", len(train_hf))\n",
    "print(\"Val size:\", len(val_hf))\n",
    "\n",
    "print(\"\\nLoading test dataset from:\", TEST_PATH)\n",
    "test_hf = load_from_disk(TEST_PATH)\n",
    "print(\"Test size:\", len(test_hf))\n",
    "\n",
    "# attributes\n",
    "ATTR_PATH = \"data/attributes.npy\"\n",
    "attributes = np.load(ATTR_PATH)\n",
    "NUM_CLASSES = attributes.shape[0]\n",
    "NUM_ATTR = attributes.shape[1]\n",
    "\n",
    "print(\"\\nAttributes shape:\", attributes.shape)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| NUM_ATTR:\", NUM_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f443e22-bcdc-4d53-b2a8-75260db502da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bb57d-7ff9-4497-ab63-386a1f0dd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdTrainDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, attributes, transform=None):\n",
    "        self.ds = hf_dataset\n",
    "        self.attributes = attributes.astype(\"float32\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "        label = int(item[\"label\"]) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        attr_vec = self.attributes[label]\n",
    "        attr_vec = torch.from_numpy(attr_vec)\n",
    "\n",
    "        return img, attr_vec, label\n",
    "\n",
    "\n",
    "class BirdTestDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.ds = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_id = int(item[\"id\"])\n",
    "        return img, img_id\n",
    "\n",
    "\n",
    "train_dataset = BirdTrainDataset(train_hf, attributes, transform=train_transform)\n",
    "val_dataset   = BirdTrainDataset(val_hf,   attributes, transform=eval_transform)\n",
    "test_dataset  = BirdTestDataset(test_hf,   transform=eval_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f4495-261b-4151-875a-9b8911a741cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleViTWithAttributes(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderBlock(\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (head_class): Linear(in_features=192, out_features=200, bias=True)\n",
       "  (head_attr): Linear(in_features=192, out_features=312, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size//patch_size\n",
    "        self.num_patches = self.grid_size**2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [B,3,H,W]\n",
    "        x = self.proj(x) #[B,embed_dim,H/P,W/P]\n",
    "        x = x.flatten(2) #[B,embed_dim,num_patches]\n",
    "        x = x.transpose(1, 2) #[B,num_patches,embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x [B,N,D]\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleViTWithAttributes(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 num_classes=200, num_attr=312, embed_dim=192, depth=6,\n",
    "                 num_heads=3, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, drop)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head_class = nn.Linear(embed_dim, num_classes)\n",
    "        # gÅ‚owa atrybutowa\n",
    "        self.head_attr  = nn.Linear(embed_dim, num_attr)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head_class.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head_attr.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:[B,3,224,224]\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x) #[B,N,D]\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1) #[B,1,D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1) #[B,1+N,D]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0] #[B,D]\n",
    "\n",
    "        logits = self.head_class(cls)\n",
    "        attr_pred = self.head_attr(cls)\n",
    "        return logits, attr_pred\n",
    "    \n",
    "model = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ca56e-54cd-4681-acae-4a924b24c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_attr  = nn.MSELoss()\n",
    "LAMBDA_ATTR = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Option B: Self-Supervised MAE (\"Nuclear Option\") =====\n",
    "# This uses the same IMG_SIZE, PATCH_SIZE, DEVICE, and train_loader already defined above.\n",
    "\n",
    "class MAEViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoencoder with ViT backbone.\n",
    "    - Encoder: patch embedding + TransformerEncoder\n",
    "    - Decoder: tiny transformer to reconstruct masked patches\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_chans=3,\n",
    "        embed_dim=192,\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        decoder_embed_dim=128,\n",
    "        decoder_depth=4,\n",
    "        decoder_num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Encoder ---\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, decoder_embed_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=decoder_embed_dim,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=decoder_depth)\n",
    "\n",
    "        patch_dim = patch_size * patch_size * in_chans\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.num_patches = num_patches\n",
    "        self.norm_pix_loss = True\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    # ------- patchify / unpatchify helpers -------\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: [B, 3, H, W] -> [B, L, patch_dim]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = imgs.shape\n",
    "        assert H == self.img_size and W == self.img_size\n",
    "\n",
    "        h = H // p\n",
    "        w = W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)      # [B, h, w, p, p, C]\n",
    "        x = x.reshape(B, h * w, p * p * C)   # [B, L, patch_dim]\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, L, patch_dim] -> [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        B, L, patch_dim = x.shape\n",
    "        C = self.in_chans\n",
    "        h = w = int(L ** 0.5)\n",
    "        assert h * w == L\n",
    "\n",
    "        x = x.reshape(B, h, w, p, p, C)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        imgs = x.reshape(B, C, h * p, w * p)\n",
    "        return imgs\n",
    "\n",
    "    # ------- random masking -------\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        x: [B, L, D]\n",
    "        Returns:\n",
    "          x_masked, mask, ids_restore\n",
    "        mask: 1 = masked, 0 = visible\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(B, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(\n",
    "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "        )\n",
    "\n",
    "        mask = torch.ones(B, L, device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    # ------- encoder / decoder forward -------\n",
    "\n",
    "    def forward_encoder(self, imgs):\n",
    "        x = self.patch_embed(imgs)              # [B, L, D]\n",
    "        x = x + self.pos_embed                  # [B, L, D]\n",
    "        x_masked, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "        x_encoded = self.encoder(x_masked)      # [B, L_visible, D]\n",
    "        return x_encoded, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x_encoded, ids_restore):\n",
    "        B, L_vis, D = x_encoded.shape\n",
    "        x = self.decoder_embed(x_encoded)       # [B, L_vis, D_dec]\n",
    "\n",
    "        L = self.num_patches\n",
    "        L_mask = L - L_vis\n",
    "        mask_tokens = self.mask_token.repeat(B, L_mask, 1)\n",
    "\n",
    "        x_ = torch.cat([x, mask_tokens], dim=1)       # [B, L, D_dec]\n",
    "        index = ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n",
    "        x_full = torch.gather(x_, dim=1, index=index) # [B, L, D_dec]\n",
    "\n",
    "        x_full = x_full + self.decoder_pos_embed\n",
    "        x_full = self.decoder(x_full)                 # [B, L, D_dec]\n",
    "\n",
    "        pred = self.decoder_pred(x_full)              # [B, L, patch_dim]\n",
    "        return pred\n",
    "\n",
    "    def loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [B, 3, H, W]\n",
    "        pred: [B, L, patch_dim]\n",
    "        mask: [B, L]\n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1e-6).sqrt()\n",
    "\n",
    "        loss_per_patch = (pred - target) ** 2\n",
    "        loss_per_patch = loss_per_patch.mean(dim=-1)      # [B, L]\n",
    "        loss = (loss_per_patch * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x_encoded, mask, ids_restore = self.forward_encoder(imgs)\n",
    "        pred = self.forward_decoder(x_encoded, ids_restore)\n",
    "        loss = self.loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAE] Epoch 1/20 - loss: 0.9152 | Recon MSE: 0.4048 | Proxy Acc: 0.7118\n",
      "[MAE] Epoch 2/20 - loss: 0.7871 | Recon MSE: 0.3894 | Proxy Acc: 0.7197\n",
      "[MAE] Epoch 2/20 - loss: 0.7871 | Recon MSE: 0.3894 | Proxy Acc: 0.7197\n",
      "[MAE] Epoch 3/20 - loss: 0.7655 | Recon MSE: 0.4013 | Proxy Acc: 0.7136\n",
      "[MAE] Epoch 3/20 - loss: 0.7655 | Recon MSE: 0.4013 | Proxy Acc: 0.7136\n",
      "[MAE] Epoch 4/20 - loss: 0.7570 | Recon MSE: 0.4044 | Proxy Acc: 0.7120\n",
      "[MAE] Epoch 4/20 - loss: 0.7570 | Recon MSE: 0.4044 | Proxy Acc: 0.7120\n",
      "[MAE] Epoch 5/20 - loss: 0.7527 | Recon MSE: 0.4035 | Proxy Acc: 0.7125\n",
      "[MAE] Epoch 5/20 - loss: 0.7527 | Recon MSE: 0.4035 | Proxy Acc: 0.7125\n",
      "[MAE] Epoch 6/20 - loss: 0.7387 | Recon MSE: 0.3947 | Proxy Acc: 0.7170\n",
      "[MAE] Epoch 6/20 - loss: 0.7387 | Recon MSE: 0.3947 | Proxy Acc: 0.7170\n",
      "[MAE] Epoch 7/20 - loss: 0.7381 | Recon MSE: 0.3937 | Proxy Acc: 0.7175\n",
      "[MAE] Epoch 7/20 - loss: 0.7381 | Recon MSE: 0.3937 | Proxy Acc: 0.7175\n",
      "[MAE] Epoch 8/20 - loss: 0.7352 | Recon MSE: 0.3951 | Proxy Acc: 0.7168\n",
      "[MAE] Epoch 8/20 - loss: 0.7352 | Recon MSE: 0.3951 | Proxy Acc: 0.7168\n",
      "[MAE] Epoch 9/20 - loss: 0.7323 | Recon MSE: 0.3940 | Proxy Acc: 0.7173\n",
      "[MAE] Epoch 9/20 - loss: 0.7323 | Recon MSE: 0.3940 | Proxy Acc: 0.7173\n",
      "[MAE] Epoch 10/20 - loss: 0.7289 | Recon MSE: 0.3969 | Proxy Acc: 0.7159\n",
      "[MAE] Epoch 10/20 - loss: 0.7289 | Recon MSE: 0.3969 | Proxy Acc: 0.7159\n",
      "[MAE] Epoch 11/20 - loss: 0.7218 | Recon MSE: 0.4027 | Proxy Acc: 0.7129\n",
      "[MAE] Epoch 11/20 - loss: 0.7218 | Recon MSE: 0.4027 | Proxy Acc: 0.7129\n",
      "[MAE] Epoch 12/20 - loss: 0.7140 | Recon MSE: 0.4041 | Proxy Acc: 0.7122\n",
      "[MAE] Epoch 12/20 - loss: 0.7140 | Recon MSE: 0.4041 | Proxy Acc: 0.7122\n",
      "[MAE] Epoch 13/20 - loss: 0.7112 | Recon MSE: 0.4039 | Proxy Acc: 0.7123\n",
      "[MAE] Epoch 13/20 - loss: 0.7112 | Recon MSE: 0.4039 | Proxy Acc: 0.7123\n",
      "[MAE] Epoch 14/20 - loss: 0.7119 | Recon MSE: 0.4034 | Proxy Acc: 0.7126\n",
      "[MAE] Epoch 14/20 - loss: 0.7119 | Recon MSE: 0.4034 | Proxy Acc: 0.7126\n",
      "[MAE] Epoch 15/20 - loss: 0.7077 | Recon MSE: 0.4047 | Proxy Acc: 0.7119\n",
      "[MAE] Epoch 15/20 - loss: 0.7077 | Recon MSE: 0.4047 | Proxy Acc: 0.7119\n",
      "[MAE] Epoch 16/20 - loss: 0.7052 | Recon MSE: 0.4083 | Proxy Acc: 0.7101\n",
      "[MAE] Epoch 16/20 - loss: 0.7052 | Recon MSE: 0.4083 | Proxy Acc: 0.7101\n",
      "[MAE] Epoch 17/20 - loss: 0.7050 | Recon MSE: 0.4043 | Proxy Acc: 0.7121\n",
      "[MAE] Epoch 17/20 - loss: 0.7050 | Recon MSE: 0.4043 | Proxy Acc: 0.7121\n",
      "[MAE] Epoch 18/20 - loss: 0.7054 | Recon MSE: 0.4058 | Proxy Acc: 0.7113\n",
      "[MAE] Epoch 18/20 - loss: 0.7054 | Recon MSE: 0.4058 | Proxy Acc: 0.7113\n",
      "[MAE] Epoch 19/20 - loss: 0.7039 | Recon MSE: 0.4056 | Proxy Acc: 0.7114\n",
      "[MAE] Epoch 19/20 - loss: 0.7039 | Recon MSE: 0.4056 | Proxy Acc: 0.7114\n",
      "[MAE] Epoch 20/20 - loss: 0.7048 | Recon MSE: 0.4051 | Proxy Acc: 0.7117\n",
      "Saved MAE nuclear weights to mae_pretrained_nuclear.pth\n",
      "[MAE] Epoch 20/20 - loss: 0.7048 | Recon MSE: 0.4051 | Proxy Acc: 0.7117\n",
      "Saved MAE nuclear weights to mae_pretrained_nuclear.pth\n"
     ]
    }
   ],
   "source": [
    "def pretrain_mae(mae_model, loader, epochs=20, lr=1e-4, weight_decay=0.05):\n",
    "    \"\"\"\n",
    "    Self-supervised MAE pretraining on the train_loader.\n",
    "    We ignore attributes and labels here.\n",
    "    Tracks reconstruction loss and patch-level MSE as a proxy metric.\n",
    "    \"\"\"\n",
    "    optimizer = optim.AdamW(mae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    mae_model.train()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        samples = 0\n",
    "\n",
    "        for imgs, _, _ in loader:   # ignore attr_vec, label\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, pred, mask = mae_model(imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = imgs.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            \n",
    "            # Compute patch-level MSE as reconstruction quality metric\n",
    "            # (lower is better, inverse of \"accuracy\")\n",
    "            patch_mse = ((pred - mae_model.patchify(imgs)) ** 2).mean().item()\n",
    "            total_mse += patch_mse * bs\n",
    "            \n",
    "            samples += bs\n",
    "\n",
    "        avg_loss = total_loss / samples\n",
    "        avg_mse = total_mse / samples\n",
    "        \n",
    "        # MSE is inverted: lower MSE = better reconstruction = \"higher accuracy\"\n",
    "        # Compute a proxy accuracy: 1 / (1 + MSE) to show improvement trend\n",
    "        proxy_acc = 1.0 / (1.0 + avg_mse)\n",
    "        \n",
    "        print(f\"[MAE] Epoch {epoch}/{epochs} - loss: {avg_loss:.4f} | Recon MSE: {avg_mse:.4f} | Proxy Acc: {proxy_acc:.4f}\")\n",
    "\n",
    "    return mae_model\n",
    "\n",
    "# ---- actually run MAE pretraining (once) ----\n",
    "mae = MAEViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    decoder_embed_dim=128,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=4,\n",
    "    mask_ratio=0.75,   # 75% masked = nuclear\n",
    ").to(DEVICE)\n",
    "\n",
    "mae = pretrain_mae(mae, train_loader, epochs=20, lr=1e-4, weight_decay=0.05)\n",
    "torch.save(mae.state_dict(), \"mae_pretrained_nuclear.pth\")\n",
    "print(\"Saved MAE nuclear weights to mae_pretrained_nuclear.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6714d61-1bb1-485a-9f19-f3bd241ce9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, attr_targets, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, attr_pred = model(imgs)\n",
    "\n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_cls  += loss_cls.item() * imgs.size(0)\n",
    "        total_attr += loss_attr.item() * imgs.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        samples += imgs.size(0)\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"[Epoch {epoch_idx}] Batch {batch_idx}/{len(train_loader)} \"\n",
    "                  f\"loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    return avg_loss, avg_cls, avg_attr, acc\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls  = 0.0\n",
    "    total_attr = 0.0\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, attr_targets, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "            logits, attr_pred = model(imgs)\n",
    "\n",
    "            loss_cls = criterion_class(logits, labels)\n",
    "            loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "            loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total_cls  += loss_cls.item() * imgs.size(0)\n",
    "            total_attr += loss_attr.item() * imgs.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            samples += imgs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / samples\n",
    "    avg_cls  = total_cls  / samples\n",
    "    avg_attr = total_attr / samples\n",
    "    acc = correct / samples\n",
    "    return avg_loss, avg_cls, avg_attr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b4a5034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NUCLEAR MAE] Epoch 1/25\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[NUCLEAR MAE] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     train_loss, train_cls, train_attr, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     val_loss, val_cls, val_attr, val_acc = evaluate()\n\u001b[32m      7\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch_idx)\u001b[39m\n\u001b[32m     11\u001b[39m labels = labels.to(DEVICE)\n\u001b[32m     12\u001b[39m attr_targets = attr_targets.to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43moptimizer\u001b[49m.zero_grad()\n\u001b[32m     16\u001b[39m logits, attr_pred = model(imgs)\n\u001b[32m     18\u001b[39m loss_cls = criterion_class(logits, labels)\n",
      "\u001b[31mNameError\u001b[39m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# ---- Training loop reusing your existing functions ----\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[NUCLEAR MAE] Epoch {epoch}/{EPOCHS}\")\n",
    "    train_loss, train_cls, train_attr, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_cls, val_attr, val_acc = evaluate()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"  Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"mae_nuclear_classifier.pth\")\n",
    "        print(\"Saved new best nuclear MAE classifier\")\n",
    "\n",
    "print(\"Best val acc (nuclear / no Bayes):\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5803f8f-ede0-479c-ac77-27c98afe70f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_params = \u001b[43mstudy\u001b[49m.best_trial.params\n\u001b[32m      2\u001b[39m best_lr           = best_params[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m best_weight_decay = best_params[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "best_lr           = best_params[\"lr\"]\n",
    "best_weight_decay = best_params[\"weight_decay\"]\n",
    "best_drop         = best_params[\"drop\"]\n",
    "best_lambda_attr  = best_params[\"lambda_attr\"]\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "best_vit = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=best_drop,   \n",
    ").to(DEVICE)\n",
    "\n",
    "LAMBDA_ATTR = best_lambda_attr\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n[FINAL TRAIN] Epoch {epoch}/{EPOCHS}\")\n",
    "    train_one_epoch(epoch)\n",
    "    _, _, _, val_acc = evaluate()\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"vit_best_model_optimized.pth\")\n",
    "        print(\"Saved new best optimized model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 25\n",
      "Best trial value (val_acc): 0.023769100169779286\n",
      "Best hyperparameters:\n",
      "  lr: 0.0004273669312062059\n",
      "  weight_decay: 2.3739423955292538e-05\n",
      "  drop: 0.10912464381815776\n",
      "  lambda_attr: 0.010368427651183114\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial value (val_acc):\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4305c16-e782-4882-bc9b-6df3eaea2342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label\n",
      "0   1     33\n",
      "1   2     55\n",
      "2   3     28\n",
      "3   4     34\n",
      "4   5     16\n",
      "\n",
      "Saved vit_submission.csv\n"
     ]
    }
   ],
   "source": [
    "best_vit = SimpleViTWithAttributes(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_attr=NUM_ATTR,\n",
    "    embed_dim=192,\n",
    "    depth=6,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    drop=best_drop,  \n",
    ").to(DEVICE)\n",
    "\n",
    "best_vit.load_state_dict(torch.load(\"vit_best_model.pth\", map_location=DEVICE))\n",
    "best_vit.eval()\n",
    "\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "\n",
    "# ---- Evaluate on test set ----\n",
    "print(\"\\n=== Evaluating on Test Set ===\")\n",
    "test_correct = 0\n",
    "test_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits, attr_pred = best_vit(imgs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(img_ids.numpy().tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "        \n",
    "        # Note: test_loader doesn't have labels, so we can't compute true accuracy\n",
    "        # But we can track predictions\n",
    "        test_samples += len(img_ids)\n",
    "\n",
    "print(f\"Total test samples processed: {test_samples}\")\n",
    "print(f\"Predictions made: {len(all_preds)}\")\n",
    "\n",
    "kaggle_labels = [p + 1 for p in all_preds]\n",
    "\n",
    "submission_vit = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": kaggle_labels\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission_vit.head())\n",
    "\n",
    "submission_vit.to_csv(\"vit_submission.csv\", index=False)\n",
    "print(\"\\nâœ“ Saved vit_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f85a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Validation Set Evaluation ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_vit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---- Final Evaluation on Validation Set (for reference accuracy) ----\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Final Validation Set Evaluation ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mbest_vit\u001b[49m.eval()\n\u001b[32m      4\u001b[39m val_correct = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m val_samples = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'best_vit' is not defined"
     ]
    }
   ],
   "source": [
    "# ---- Final Evaluation on Validation Set (for reference accuracy) ----\n",
    "print(\"\\n=== Final Validation Set Evaluation ===\")\n",
    "best_vit.eval()\n",
    "val_correct = 0\n",
    "val_samples = 0\n",
    "val_total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, attr_targets, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        attr_targets = attr_targets.to(DEVICE)\n",
    "\n",
    "        logits, attr_pred = best_vit(imgs)\n",
    "        \n",
    "        loss_cls = criterion_class(logits, labels)\n",
    "        loss_attr = criterion_attr(attr_pred, attr_targets)\n",
    "        loss = loss_cls + LAMBDA_ATTR * loss_attr\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_samples += imgs.size(0)\n",
    "        val_total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "val_acc = val_correct / val_samples\n",
    "val_loss_avg = val_total_loss / val_samples\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss_avg:.4f}\")\n",
    "print(f\"Correct Predictions: {val_correct}/{val_samples}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
