{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453ab9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jkowa\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc302f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a872b2e6-501a-4469-b4db-84a80dc2ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"processed_bird_data\"\n",
    "TEST_DATA_PATH = \"processed_bird_test_data\"\n",
    "MODEL_NAME = \"google/mobilenet_v2_1.0_224\"\n",
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfcee223-b046-4569-989b-0b45f57abc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "try:\n",
    "    dataset = load_from_disk(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {DATA_PATH} not found.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e845a9c-6c39-4c9d-98f0-1cd977400a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkowa\\anaconda3\\Lib\\site-packages\\transformers\\models\\mobilenet_v2\\feature_extraction_mobilenet_v2.py:30: FutureWarning: The class MobileNetV2FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use MobileNetV2ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2890d50-d6de-4745-a3a3-a76e7481584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 105 | Val batches: 19\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=feature_extractor.image_mean, \n",
    "    std=feature_extractor.image_std\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), \n",
    "    transforms.RandomHorizontalFlip(),                 \n",
    "    transforms.RandomRotation(15),                     \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), \n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def train_transform_func(batch):\n",
    "    inputs = {}\n",
    "    inputs[\"pixel_values\"] = [train_transforms(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "    inputs[\"label\"] = batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "def val_transform_func(batch):\n",
    "    inputs = {}\n",
    "    inputs[\"pixel_values\"] = [val_transforms(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "    inputs[\"label\"] = batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_func)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].with_transform(val_transform_func)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset[\"validation\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c91cadb-93f6-4662-8675-5d058905e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Baseline Model (MobileNetV2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileNetV2ForImageClassification were not initialized from the model checkpoint at google/mobilenet_v2_1.0_224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1001]) in the checkpoint and torch.Size([200]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1001, 1280]) in the checkpoint and torch.Size([200, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Baseline Model (MobileNetV2)...\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=200,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99beb7c3-ba4a-48b9-9f39-2dfcc39bf400",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "save_path = \"baseline_best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35991d96-f2d4-4820-8914-2c398371aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop (with early stopping)...\n",
      "\n",
      "[Epoch 1] Batch 0/105 | loss=5.3042\n",
      "[Epoch 1] Batch 20/105 | loss=5.1404\n",
      "[Epoch 1] Batch 40/105 | loss=5.0564\n",
      "[Epoch 1] Batch 60/105 | loss=4.3242\n",
      "[Epoch 1] Batch 80/105 | loss=3.9000\n",
      "[Epoch 1] Batch 100/105 | loss=3.6477\n",
      "Train: loss=4.4982, acc=0.1250\n",
      "Val:   loss=3.6576, acc=0.2207\n",
      "Validation loss improved. Model saved to baseline_best_model.pth\n",
      "------------------------------\n",
      "[Epoch 2] Batch 0/105 | loss=3.1088\n",
      "[Epoch 2] Batch 20/105 | loss=3.0372\n",
      "[Epoch 2] Batch 40/105 | loss=2.4082\n",
      "[Epoch 2] Batch 60/105 | loss=2.6759\n",
      "[Epoch 2] Batch 80/105 | loss=2.3149\n",
      "[Epoch 2] Batch 100/105 | loss=2.3968\n",
      "Train: loss=2.6193, acc=0.4402\n",
      "Val:   loss=2.7232, acc=0.3599\n",
      "Validation loss improved. Model saved to baseline_best_model.pth\n",
      "------------------------------\n",
      "[Epoch 3] Batch 0/105 | loss=1.9641\n",
      "[Epoch 3] Batch 20/105 | loss=1.7902\n",
      "[Epoch 3] Batch 40/105 | loss=1.6156\n",
      "[Epoch 3] Batch 60/105 | loss=2.1417\n",
      "[Epoch 3] Batch 80/105 | loss=1.7953\n",
      "[Epoch 3] Batch 100/105 | loss=1.4796\n",
      "Train: loss=1.6749, acc=0.6203\n",
      "Val:   loss=2.1782, acc=0.4601\n",
      "Validation loss improved. Model saved to baseline_best_model.pth\n",
      "------------------------------\n",
      "[Epoch 4] Batch 0/105 | loss=0.8984\n",
      "[Epoch 4] Batch 20/105 | loss=1.0845\n",
      "[Epoch 4] Batch 40/105 | loss=0.9578\n",
      "[Epoch 4] Batch 60/105 | loss=1.3055\n",
      "[Epoch 4] Batch 80/105 | loss=1.1195\n",
      "[Epoch 4] Batch 100/105 | loss=1.0390\n",
      "Train: loss=1.1649, acc=0.7387\n",
      "Val:   loss=2.2708, acc=0.4499\n",
      "No improvement in validation loss for 1/5 epochs.\n",
      "------------------------------\n",
      "[Epoch 5] Batch 0/105 | loss=0.6874\n"
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training loop (with early stopping)...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass \n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # stats\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(f\"[Epoch {epoch+1}] Batch {i}/{len(train_loader)} | loss={loss.item():.4f}\")\n",
    "\n",
    "    train_epoch_loss = running_loss / len(train_loader)\n",
    "    train_epoch_acc = correct_train / total_train\n",
    "    \n",
    "    # validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            \n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            val_loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            val_running_loss += val_loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    \n",
    "    # status report\n",
    "    print(f\"Train: loss={train_epoch_loss:.4f}, acc={train_epoch_acc:.4f}\")\n",
    "    print(f\"Val:   loss={val_epoch_loss:.4f}, acc={val_epoch_acc:.4f}\")\n",
    "    \n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        best_val_acc = val_epoch_acc\n",
    "        counter = 0 # reset bc improvement was found\n",
    "        \n",
    "        # saving the best model\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Validation loss improved. Model saved to {save_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss for {counter}/{patience} epochs.\")\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(f\"\\nTraining finished. Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4497876f-e27d-440e-9fe7-a666230b0f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from processed_bird_test_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|████████████████████████████████████████████████████████████████████| 125/125 [01:55<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to baseline_submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "TEST_DATA_PATH = \"processed_bird_test_data\"\n",
    "WEIGHTS_PATH = \"baseline_best_model.pth\"\n",
    "OUTPUT_FILENAME = \"baseline_submission.csv\"\n",
    "\n",
    "print(f\"Loading test data from {TEST_DATA_PATH}...\")\n",
    "try:\n",
    "    dataset_raw = load_from_disk(TEST_DATA_PATH)\n",
    "    if isinstance(dataset_raw, dict) and \"test\" in dataset_raw:\n",
    "        test_ds = dataset_raw[\"test\"]\n",
    "    else:\n",
    "        test_ds = dataset_raw\n",
    "    \n",
    "    submission_ids = list(test_ds[\"id\"]) if \"id\" in test_ds.column_names else list(range(len(test_ds)))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise e\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=feature_extractor.image_mean, \n",
    "    std=feature_extractor.image_std\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "class BaselineTestDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.hf_dataset[idx][\"image\"].convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    BaselineTestDataset(test_ds, transform=test_transforms),\n",
    "    batch_size=32,\n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "if os.path.exists(WEIGHTS_PATH):\n",
    "    model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            outputs = model(pixel_values=imgs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    df = pd.DataFrame({\"id\": submission_ids, \"label\": all_preds})\n",
    "    df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "    print(f\"Saved to {OUTPUT_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cee3a18-fa4a-496b-a0b4-991c663ac28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? blind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f69744-1899-4ec0-a86a-2eaf668c9484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
